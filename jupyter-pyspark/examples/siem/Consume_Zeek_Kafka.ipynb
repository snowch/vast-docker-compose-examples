{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a523f92d-6eb5-43a6-92bc-80b33ecabbe1",
   "metadata": {},
   "source": [
    "Fraud_Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cba218b-786c-4787-ad23-569d0044fe1f",
   "metadata": {},
   "source": [
    "# Trade Settlement (Spark Streaming app that consumes stock settlement data from Kafka and stores them into the  VAST Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8311525d-91b5-440b-986a-cf708b915c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "DOCKER_HOST_OR_IP=10.143.11.241\n",
      "---\n",
      "VASTDB_ENDPOINT=http://172.200.204.2:80\n",
      "VASTDB_ACCESS_KEY==****QXN5\n",
      "VASTDB_SECRET_KEY=****oLGr\n",
      "VASTDB_SIEM_BUCKET=csnow-db\n",
      "VASTDB_SIEM_SCHEMA=zeek-live-logs\n",
      "VASTDB_SIEM_TABLE_PREFIX=zeek_\n",
      "---\n",
      "VAST_KAFKA_BROKER=172.200.204.1:9092\n",
      "topic=zeek-live-logs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Load environment variables for Kafka and VastDB connectivity\n",
    "DOCKER_HOST_OR_IP = os.getenv(\"DOCKER_HOST_OR_IP\", \"localhost\")\n",
    "VASTDB_ENDPOINT = os.getenv(\"VASTDB_ENDPOINT\")\n",
    "VASTDB_ACCESS_KEY = os.getenv(\"VASTDB_ACCESS_KEY\")\n",
    "VASTDB_SECRET_KEY = os.getenv(\"VASTDB_SECRET_KEY\")\n",
    "\n",
    "VASTDB_SIEM_BUCKET = os.getenv(\"VASTDB_SIEM_BUCKET\", 'csnow-db')\n",
    "VASTDB_SIEM_SCHEMA = os.getenv(\"VASTDB_SIEM_SCHEMA\", 'zeek-live-logs')\n",
    "VASTDB_SIEM_TABLE_PREFIX = 'zeek_'\n",
    "\n",
    "use_vastkafka = True\n",
    "if use_vastkafka:\n",
    "    VAST_KAFKA_BROKER = os.getenv(\"VAST_KAFKA_BROKER\")\n",
    "else:\n",
    "    VAST_KAFKA_BROKER = f\"{DOCKER_HOST_OR_IP}:19092\"\n",
    "\n",
    "kafka_brokers = VAST_KAFKA_BROKER\n",
    "topic = 'zeek-live-logs'\n",
    "\n",
    "# Print configurations\n",
    "print(f\"\"\"\n",
    "---\n",
    "DOCKER_HOST_OR_IP={DOCKER_HOST_OR_IP}\n",
    "---\n",
    "VASTDB_ENDPOINT={VASTDB_ENDPOINT}\n",
    "VASTDB_ACCESS_KEY==****{VASTDB_ACCESS_KEY[-4:]}\n",
    "VASTDB_SECRET_KEY=****{VASTDB_SECRET_KEY[-4:]}\n",
    "VASTDB_SIEM_BUCKET={VASTDB_SIEM_BUCKET}\n",
    "VASTDB_SIEM_SCHEMA={VASTDB_SIEM_SCHEMA}\n",
    "VASTDB_SIEM_TABLE_PREFIX={VASTDB_SIEM_TABLE_PREFIX}\n",
    "---\n",
    "VAST_KAFKA_BROKER={VAST_KAFKA_BROKER}\n",
    "topic={topic}\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7269aa1b-8af2-4200-a15b-7d3f7ac5c43d",
   "metadata": {},
   "source": [
    "Create Vast DB schema if it doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93bdf80c-b2c8-4135-8948-4f47d395ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U vastdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "926f7c72-dabd-4a80-a039-084c0e4bf976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vastdb\n",
    "\n",
    "session = vastdb.connect(endpoint=VASTDB_ENDPOINT, access=VASTDB_ACCESS_KEY, secret=VASTDB_SECRET_KEY)\n",
    "with session.transaction() as tx:\n",
    "    bucket = tx.bucket(VASTDB_SIEM_BUCKET)\n",
    "    bucket.schema(VASTDB_SIEM_SCHEMA, fail_if_missing=False) or bucket.create_schema(VASTDB_SIEM_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9488ff35-56ab-4f59-8a1f-4de356a5701b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark successfully loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import socket\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, count, get_json_object\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, BooleanType\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Spark Configuration\n",
    "conf = SparkConf()\n",
    "conf.setAll([\n",
    "    (\"spark.driver.host\", socket.gethostbyname(socket.gethostname())),\n",
    "    (\"spark.sql.execution.arrow.pyspark.enabled\", \"false\"),\n",
    "    # VASTDB\n",
    "    (\"spark.sql.catalog.ndb\", 'spark.sql.catalog.ndb.VastCatalog'),\n",
    "    (\"spark.ndb.endpoint\", VASTDB_ENDPOINT),\n",
    "    (\"spark.ndb.data_endpoints\", VASTDB_ENDPOINT),\n",
    "    (\"spark.ndb.access_key_id\", VASTDB_ACCESS_KEY),\n",
    "    (\"spark.ndb.secret_access_key\", VASTDB_SECRET_KEY),\n",
    "    (\"spark.driver.extraClassPath\", '/usr/local/spark/jars/spark3-vast-3.4.1-f93839bfa38a/*'),\n",
    "    (\"spark.executor.extraClassPath\", '/usr/local/spark/jars/spark3-vast-3.4.1-f93839bfa38a/*'),\n",
    "    (\"spark.sql.extensions\", 'ndb.NDBSparkSessionExtension'),\n",
    "    # Kafka\n",
    "    (\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.4.3,\" \n",
    "                            \"org.apache.logging.log4j:log4j-slf4j2-impl:2.19.0,\" \n",
    "                            \"org.apache.logging.log4j:log4j-api:2.19.0,\" \n",
    "                            \"org.apache.logging.log4j:log4j-core:2.19.0\"),\n",
    "    (\"spark.jars.excludes\", \"org.slf4j:slf4j-api,org.slf4j:slf4j-log4j12\"),\n",
    "    (\"spark.hadoop.fs.file.impl\", \"org.apache.hadoop.fs.RawLocalFileSystem\"),\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"KafkaStreamingToVastDB\") \\\n",
    "    .config(conf=conf) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"DEBUG\")\n",
    "\n",
    "print(\"Spark successfully loaded\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39e771cf-14c2-45e8-8222-6153e5ec3be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'`ndb`.`csnow-db`.`zeek-live-logs`.`zeek_`'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destination_table_name_prefix = f\"`ndb`.`{VASTDB_SIEM_BUCKET}`.`{VASTDB_SIEM_SCHEMA}`.`{VASTDB_SIEM_TABLE_PREFIX}`\"\n",
    "destination_table_name_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5642f2ec-902d-4665-864a-606a3606a51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Zeek log streaming job to VastDB...\n",
      "This will dynamically create VastDB tables for each Zeek log type (conn, analyzer, weird, etc.)\n",
      "Tables will be created in: ndb.csnow-db.zeek-live-logs\n",
      "Last update: 17:54:42 | Batch 0: 0 records | Total messages: 0 | Total VastDB rows: 0 | Log types: 0 () | Tables: [No tables yet]     \n",
      "Error parsing JSON: name 'json' is not defined\n",
      "Last update: 17:54:57 | Batch 7: 1 records | Total messages: 1 | Total VastDB rows: 0 | Log types: 0 () | Tables: [No tables yet]     \n",
      "Error parsing JSON: name 'json' is not defined\n",
      "Last update: 17:54:58 | Batch 8: 1 records | Total messages: 2 | Total VastDB rows: 0 | Log types: 0 () | Tables: [No tables yet]     \n",
      "Error parsing JSON: name 'json' is not defined\n",
      "\n",
      "Error parsing JSON: name 'json' is not defined\n",
      "Last update: 17:55:00 | Batch 9: 2 records | Total messages: 4 | Total VastDB rows: 0 | Log types: 0 () | Tables: [No tables yet]     \n",
      "Error parsing JSON: name 'json' is not defined\n",
      "Last update: 17:55:02 | Batch 10: 1 records | Total messages: 5 | Total VastDB rows: 0 | Log types: 0 () | Tables: [No tables yet]     \n",
      "Error parsing JSON: name 'json' is not defined\n",
      "Last update: 17:55:04 | Batch 11: 1 records | Total messages: 6 | Total VastDB rows: 0 | Log types: 0 () | Tables: [No tables yet]     \n",
      "Error parsing JSON: name 'json' is not defined\n",
      "Last update: 17:55:06 | Batch 12: 1 records | Total messages: 7 | Total VastDB rows: 0 | Log types: 0 () | Tables: [No tables yet]     \n",
      "Error parsing JSON: name 'json' is not defined\n",
      "\n",
      "Error parsing JSON: name 'json' is not defined\n",
      "Last update: 17:55:08 | Batch 13: 2 records | Total messages: 9 | Total VastDB rows: 0 | Log types: 0 () | Tables: [No tables yet]     \n",
      "Error parsing JSON: name 'json' is not defined\n",
      "Last update: 17:55:10 | Batch 14: 1 records | Total messages: 10 | Total VastDB rows: 0 | Log types: 0 () | Tables: [No tables yet]     \n",
      "Error parsing JSON: name 'json' is not defined\n",
      "Last update: 17:55:12 | Batch 15: 1 records | Total messages: 11 | Total VastDB rows: 0 | Log types: 0 () | Tables: [No tables yet]     \n",
      "Error parsing JSON: name 'json' is not defined\n",
      "Last update: 17:55:14 | Batch 16: 1 records | Total messages: 12 | Total VastDB rows: 0 | Log types: 0 () | Tables: [No tables yet]     \n",
      "Graceful shutdown initiated...\n",
      "\n",
      "Final status:\n",
      "VastDB Zeek streaming completed. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import signal\n",
    "import time\n",
    "import threading\n",
    "import pyspark\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create checkpoint directory with absolute path\n",
    "checkpoint_dir = os.path.abspath(\"/tmp/spark_checkpoint\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Global variables for tracking\n",
    "total_message_count = 0\n",
    "table_row_counts = {}  # Track row counts per table\n",
    "last_batch_id = 0\n",
    "last_batch_size = 0\n",
    "processed_log_types = set()  # Track which log types we've seen\n",
    "created_tables = set()  # Track which tables we've already created\n",
    "\n",
    "should_shutdown = False\n",
    "\n",
    "# Print a comprehensive status update\n",
    "def print_status(source=\"\"):\n",
    "    global total_message_count, table_row_counts, last_batch_id, last_batch_size, processed_log_types\n",
    "    if not should_shutdown:\n",
    "        current_time = time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "        total_db_rows = sum(table_row_counts.values())\n",
    "        \n",
    "        # Create summary of table counts\n",
    "        table_summary = \", \".join([f\"{log_type}: {count}\" for log_type, count in table_row_counts.items()])\n",
    "        if not table_summary:\n",
    "            table_summary = \"No tables yet\"\n",
    "            \n",
    "        print(f\"\\rLast update: {current_time} | Batch {last_batch_id}: {last_batch_size} records | \"\n",
    "              f\"Total messages: {total_message_count} | Total VastDB rows: {total_db_rows} | \"\n",
    "              f\"Log types: {len(processed_log_types)} ({', '.join(sorted(processed_log_types))}) | \"\n",
    "              f\"Tables: [{table_summary}]     \", end=\"\")\n",
    "        \n",
    "        import sys\n",
    "        sys.stdout.flush()\n",
    "\n",
    "# Helper function to create safe VastDB table names\n",
    "def create_vastdb_table_name(log_type):\n",
    "    \"\"\"Create a VastDB table name for the log type\"\"\"\n",
    "    # Clean up the log type for SQL compatibility\n",
    "    clean_log_type = log_type.replace(\"-\", \"_\").replace(\".\", \"_\")\n",
    "    return f\"`ndb`.`{VASTDB_SIEM_BUCKET}`.`{VASTDB_SIEM_SCHEMA}`.`{clean_log_type}`\"\n",
    "\n",
    "# Helper function to create table schema in VastDB if it doesn't exist\n",
    "def ensure_table_exists(log_type, sample_data):\n",
    "    \"\"\"Ensure the VastDB table exists for this log type\"\"\"\n",
    "    global created_tables\n",
    "    \n",
    "    table_name = create_vastdb_table_name(log_type)\n",
    "    table_key = f\"{VASTDB_SIEM_BUCKET}.{VASTDB_SIEM_SCHEMA}.{log_type}\"\n",
    "    \n",
    "    if table_key in created_tables:\n",
    "        return table_name\n",
    "    \n",
    "    try:\n",
    "        # Try to query the table to see if it exists\n",
    "        spark.sql(f\"SELECT 1 FROM {table_name} LIMIT 1\")\n",
    "        created_tables.add(table_key)\n",
    "        return table_name\n",
    "    except:\n",
    "        # Table doesn't exist, we'll let Spark create it dynamically\n",
    "        created_tables.add(table_key)\n",
    "        return table_name\n",
    "\n",
    "# Process each microbatch with dynamic table routing\n",
    "def process_microbatch(raw_df, epoch_id):\n",
    "    global total_message_count, last_batch_id, last_batch_size, processed_log_types\n",
    "    if not should_shutdown:\n",
    "        try:\n",
    "            batch_size = raw_df.count()\n",
    "            if batch_size == 0:\n",
    "                return\n",
    "                \n",
    "            total_message_count += batch_size\n",
    "            last_batch_id = epoch_id\n",
    "            last_batch_size = batch_size\n",
    "            \n",
    "            # Collect all JSON strings to determine log types\n",
    "            json_strings = [row.json for row in raw_df.collect()]\n",
    "            \n",
    "            # Group messages by log type\n",
    "            log_type_groups = {}\n",
    "            for json_str in json_strings:\n",
    "                try:\n",
    "                    parsed = json.loads(json_str)\n",
    "                    # Get the top-level key (log type)\n",
    "                    log_type = list(parsed.keys())[0]\n",
    "                    processed_log_types.add(log_type)\n",
    "                    \n",
    "                    if log_type not in log_type_groups:\n",
    "                        log_type_groups[log_type] = []\n",
    "                    log_type_groups[log_type].append(json_str)\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError parsing JSON: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Process each log type group\n",
    "            for log_type, json_list in log_type_groups.items():\n",
    "                try:\n",
    "                    # Create DataFrame for this log type\n",
    "                    log_type_rdd = spark.sparkContext.parallelize([(json_str,) for json_str in json_list])\n",
    "                    log_type_df = spark.createDataFrame(log_type_rdd, [\"json\"])\n",
    "                    \n",
    "                    # Extract the nested object for this log type using get_json_object\n",
    "                    extracted_df = log_type_df.select(\n",
    "                        get_json_object(col(\"json\"), f\"$.{log_type}\").alias(\"log_data\")\n",
    "                    ).filter(col(\"log_data\").isNotNull())\n",
    "                    \n",
    "                    if extracted_df.count() > 0:\n",
    "                        # Use from_json with schema inference\n",
    "                        sample_json = extracted_df.select(\"log_data\").first()\n",
    "                        if sample_json and sample_json.log_data:\n",
    "                            try:\n",
    "                                # Parse sample to create a basic schema\n",
    "                                sample_dict = json.loads(sample_json.log_data)\n",
    "                                \n",
    "                                # Create a flexible schema that accommodates common types\n",
    "                                fields = []\n",
    "                                for key, value in sample_dict.items():\n",
    "                                    # Keep original field names but handle special characters\n",
    "                                    if isinstance(value, str):\n",
    "                                        fields.append(StructField(key, StringType(), True))\n",
    "                                    elif isinstance(value, int):\n",
    "                                        fields.append(StructField(key, LongType(), True))\n",
    "                                    elif isinstance(value, float):\n",
    "                                        fields.append(StructField(key, DoubleType(), True))\n",
    "                                    elif isinstance(value, bool):\n",
    "                                        fields.append(StructField(key, BooleanType(), True))\n",
    "                                    else:\n",
    "                                        # Default to string for complex types\n",
    "                                        fields.append(StructField(key, StringType(), True))\n",
    "                                \n",
    "                                inferred_schema = StructType(fields)\n",
    "                                \n",
    "                                # Parse with inferred schema\n",
    "                                parsed_df = extracted_df.select(\n",
    "                                    from_json(col(\"log_data\"), inferred_schema).alias(\"parsed\")\n",
    "                                ).select(\"parsed.*\")\n",
    "                                \n",
    "                                # Ensure table exists and get table name\n",
    "                                table_name = ensure_table_exists(log_type, sample_dict)\n",
    "                                \n",
    "                                # Write to VastDB table specific to this log type\n",
    "                                parsed_df.write.mode(\"append\").saveAsTable(table_name)\n",
    "                                \n",
    "                            except Exception as schema_error:\n",
    "                                print(f\"\\nSchema inference error for {log_type}: {schema_error}\")\n",
    "                                # Fallback: store as raw JSON string\n",
    "                                try:\n",
    "                                    fallback_df = extracted_df.select(col(\"log_data\").alias(\"raw_json\"))\n",
    "                                    table_name = f\"`ndb`.`{VASTDB_SIEM_BUCKET}`.`{VASTDB_SIEM_SCHEMA}`.`{log_type}_raw`\"\n",
    "                                    fallback_df.write.mode(\"append\").saveAsTable(table_name)\n",
    "                                except Exception as fallback_error:\n",
    "                                    print(f\"\\nFallback failed for {log_type}: {fallback_error}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError processing log type {log_type}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            print_status(\"Batch\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nException in process_microbatch: {e}\")\n",
    "\n",
    "# Function to periodically check and update row counts for all VastDB tables\n",
    "def check_row_counts():\n",
    "    global table_row_counts\n",
    "    while not should_shutdown:\n",
    "        time.sleep(3)  # Check every 3 seconds\n",
    "        try:\n",
    "            for log_type in processed_log_types:\n",
    "                try:\n",
    "                    table_name = create_vastdb_table_name(log_type)\n",
    "                    new_count = spark.sql(f\"SELECT count(*) FROM {table_name}\").collect()[0][0]\n",
    "                    if table_row_counts.get(log_type, 0) != new_count:\n",
    "                        table_row_counts[log_type] = new_count\n",
    "                        print_status(\"DB Count\")\n",
    "                except Exception:\n",
    "                    # Table might not exist yet or be accessible\n",
    "                    pass\n",
    "        except Exception:\n",
    "            # Ignore errors in checking\n",
    "            pass\n",
    "\n",
    "# Read data from Kafka stream\n",
    "raw_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_brokers) \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"failOnDataLoss\", \"true\") \\\n",
    "    .load()\n",
    "\n",
    "# Decode Kafka messages as JSON strings\n",
    "decoded_stream = raw_stream.selectExpr(\"CAST(value AS STRING) as json\")\n",
    "\n",
    "# Main processing query - using the dynamic approach\n",
    "zeek_query = decoded_stream.writeStream \\\n",
    "    .foreachBatch(process_microbatch) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime='2 seconds') \\\n",
    "    .option(\"maxFilesPerTrigger\", 1000) \\\n",
    "    .option(\"checkpointLocation\", checkpoint_dir) \\\n",
    "    .start()\n",
    "\n",
    "# Print initial status message\n",
    "print(\"\\nStarting Zeek log streaming job to VastDB...\")\n",
    "print(\"This will dynamically create VastDB tables for each Zeek log type (conn, analyzer, weird, etc.)\")\n",
    "print(f\"Tables will be created in: ndb.{VASTDB_SIEM_BUCKET}.{VASTDB_SIEM_SCHEMA}\")\n",
    "print_status(\"Init\")\n",
    "\n",
    "# Start thread for checking row counts\n",
    "row_count_thread = threading.Thread(target=check_row_counts)\n",
    "row_count_thread.daemon = True\n",
    "row_count_thread.start()\n",
    "\n",
    "shutdown_flag = threading.Event()\n",
    "\n",
    "def signal_handler(sig, frame):\n",
    "    global should_shutdown\n",
    "    print(\"\\nGraceful shutdown initiated...\")\n",
    "    should_shutdown = True\n",
    "    shutdown_flag.set()\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "signal.signal(signal.SIGTERM, signal_handler)\n",
    "\n",
    "# Main loop\n",
    "try:\n",
    "    while zeek_query.isActive and not shutdown_flag.is_set():\n",
    "        time.sleep(1)\n",
    "    if zeek_query.isActive:\n",
    "        zeek_query.stop()\n",
    "    zeek_query.awaitTermination()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during awaitTermination: {e}\")\n",
    "\n",
    "print(\"\\nFinal status:\")\n",
    "for log_type, count in table_row_counts.items():\n",
    "    print(f\"  {VASTDB_SIEM_BUCKET}.{VASTDB_SIEM_SCHEMA}.{log_type}: {count} rows\")\n",
    "print(\"VastDB Zeek streaming completed. Goodbye!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4875e87b-c076-4ec8-a7b3-02806cd05a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb58f143-6f99-4a50-9782-aed41cbe1830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
