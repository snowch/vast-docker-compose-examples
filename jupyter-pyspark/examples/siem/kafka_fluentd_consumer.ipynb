{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7a2d7a-a3b3-46b7-aa3a-a0fcfcfb4fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "DOCKER_HOST_OR_IP=10.143.11.241\n",
      "---\n",
      "VASTDB_ENDPOINT=http://172.200.204.2:80\n",
      "VASTDB_ACCESS_KEY==****QXN5\n",
      "VASTDB_SECRET_KEY=****oLGr\n",
      "VASTDB_SIEM_BUCKET=csnow-db\n",
      "VASTDB_SIEM_SCHEMA=fluentd-ocsf-logs\n",
      "VASTDB_SIEM_TABLE_PREFIX=ocsf_\n",
      "---\n",
      "VAST_KAFKA_BROKER=172.200.204.1:9092\n",
      "topic=fluentd-events\n",
      "\n",
      "Spark successfully loaded\n",
      "\n",
      "Table prefix: `ndb`.`csnow-db`.`fluentd-ocsf-logs`.`ocsf_`\n",
      "\n",
      "Starting FluentD OCSF log streaming job to VastDB...\n",
      "This will dynamically create VastDB tables for each OCSF event class (security_finding, authentication, etc.)\n",
      "Tables will be created in: ndb.csnow-db.fluentd-ocsf-logs\n",
      "All timestamp fields will be properly converted to TimestampType for optimal query performance.\n",
      "OCSF event classes will be automatically detected and routed to appropriate tables.\n",
      "Last update: 19:40:32 | Batch 0: 0 records | Total messages: 0 | Total VastDB rows: 0 | OCSF Classes: 0 () | Tables: [No tables yet]     \n",
      "Created new OCSF table `ndb`.`csnow-db`.`fluentd-ocsf-logs`.`ocsf_2004_security_finding` for class_uid 2004 (79 columns)\n",
      "Last update: 19:40:39 | Batch 0: 1 records | Total messages: 1 | Total VastDB rows: 1 | OCSF Classes: 1 (2004_security_finding) | Tables: [2004_security_finding: 1]     "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "\n",
    "# Load environment variables for Kafka and VastDB connectivity\n",
    "DOCKER_HOST_OR_IP = os.getenv(\"DOCKER_HOST_OR_IP\", \"localhost\")\n",
    "VASTDB_ENDPOINT = os.getenv(\"VASTDB_ENDPOINT\")\n",
    "VASTDB_ACCESS_KEY = os.getenv(\"VASTDB_ACCESS_KEY\")\n",
    "VASTDB_SECRET_KEY = os.getenv(\"VASTDB_SECRET_KEY\")\n",
    "\n",
    "VASTDB_SIEM_BUCKET = os.getenv(\"VASTDB_SIEM_BUCKET\", 'csnow-db')\n",
    "VASTDB_SIEM_SCHEMA = os.getenv(\"VASTDB_SIEM_SCHEMA\", 'fluentd-ocsf-logs')\n",
    "VASTDB_SIEM_TABLE_PREFIX = 'ocsf_'\n",
    "\n",
    "use_vastkafka = True\n",
    "if use_vastkafka:\n",
    "    VAST_KAFKA_BROKER = os.getenv(\"VAST_KAFKA_BROKER\")\n",
    "else:\n",
    "    VAST_KAFKA_BROKER = f\"{DOCKER_HOST_OR_IP}:19092\"\n",
    "\n",
    "kafka_brokers = VAST_KAFKA_BROKER\n",
    "topic = 'fluentd-events'\n",
    "\n",
    "# Print configurations\n",
    "print(f\"\"\"\n",
    "---\n",
    "DOCKER_HOST_OR_IP={DOCKER_HOST_OR_IP}\n",
    "---\n",
    "VASTDB_ENDPOINT={VASTDB_ENDPOINT}\n",
    "VASTDB_ACCESS_KEY==****{VASTDB_ACCESS_KEY[-4:]}\n",
    "VASTDB_SECRET_KEY=****{VASTDB_SECRET_KEY[-4:]}\n",
    "VASTDB_SIEM_BUCKET={VASTDB_SIEM_BUCKET}\n",
    "VASTDB_SIEM_SCHEMA={VASTDB_SIEM_SCHEMA}\n",
    "VASTDB_SIEM_TABLE_PREFIX={VASTDB_SIEM_TABLE_PREFIX}\n",
    "---\n",
    "VAST_KAFKA_BROKER={VAST_KAFKA_BROKER}\n",
    "topic={topic}\n",
    "\"\"\")\n",
    "\n",
    "# Create Vast DB schema if it doesn't exist.\n",
    "get_ipython().run_cell_magic('capture', '--no-stderr', '%pip install --quiet -U vastdb\\n')\n",
    "\n",
    "import vastdb\n",
    "\n",
    "session = vastdb.connect(endpoint=VASTDB_ENDPOINT, access=VASTDB_ACCESS_KEY, secret=VASTDB_SECRET_KEY)\n",
    "with session.transaction() as tx:\n",
    "    bucket = tx.bucket(VASTDB_SIEM_BUCKET)\n",
    "    bucket.schema(VASTDB_SIEM_SCHEMA, fail_if_missing=False) or bucket.create_schema(VASTDB_SIEM_SCHEMA)\n",
    "\n",
    "import socket\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, count, get_json_object, to_timestamp, lit, when, isnan, isnull\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, BooleanType, TimestampType, IntegerType\n",
    "import threading\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Spark Configuration\n",
    "conf = SparkConf()\n",
    "conf.setAll([\n",
    "    (\"spark.driver.host\", socket.gethostbyname(socket.gethostname())),\n",
    "    (\"spark.sql.execution.arrow.pyspark.enabled\", \"false\"),\n",
    "    # VASTDB\n",
    "    (\"spark.sql.catalog.ndb\", 'spark.sql.catalog.ndb.VastCatalog'),\n",
    "    (\"spark.ndb.endpoint\", VASTDB_ENDPOINT),\n",
    "    (\"spark.ndb.data_endpoints\", VASTDB_ENDPOINT),\n",
    "    (\"spark.ndb.access_key_id\", VASTDB_ACCESS_KEY),\n",
    "    (\"spark.ndb.secret_access_key\", VASTDB_SECRET_KEY),\n",
    "    (\"spark.driver.extraClassPath\", '/usr/local/spark/jars/spark3-vast-3.4.1-f93839bfa38a/*'),\n",
    "    (\"spark.executor.extraClassPath\", '/usr/local/spark/jars/spark3-vast-3.4.1-f93839bfa38a/*'),\n",
    "    (\"spark.sql.extensions\", 'ndb.NDBSparkSessionExtension'),\n",
    "    # Kafka\n",
    "    (\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.4.3,\" \n",
    "                            \"org.apache.logging.log4j:log4j-slf4j2-impl:2.19.0,\" \n",
    "                            \"org.apache.logging.log4j:log4j-api:2.19.0,\" \n",
    "                            \"org.apache.logging.log4j:log4j-core:2.19.0\"),\n",
    "    (\"spark.jars.excludes\", \"org.slf4j:slf4j-api,org.slf4j:slf4j-log4j12\"),\n",
    "    (\"spark.hadoop.fs.file.impl\", \"org.apache.hadoop.fs.RawLocalFileSystem\"),\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"FluentDOCSFStreamingToVastDB\") \\\n",
    "    .config(conf=conf) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"DEBUG\")\n",
    "\n",
    "print(\"Spark successfully loaded\\n\")\n",
    "\n",
    "destination_table_name_prefix = f\"`ndb`.`{VASTDB_SIEM_BUCKET}`.`{VASTDB_SIEM_SCHEMA}`.`{VASTDB_SIEM_TABLE_PREFIX}`\"\n",
    "print(f\"Table prefix: {destination_table_name_prefix}\")\n",
    "\n",
    "import os\n",
    "import signal\n",
    "import time\n",
    "import threading\n",
    "import pyspark\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create checkpoint directory with absolute path\n",
    "checkpoint_dir = os.path.abspath(\"/tmp/spark_fluentd_checkpoint\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Global variables for tracking\n",
    "total_message_count = 0\n",
    "table_row_counts = {}  # Track row counts per table\n",
    "last_batch_id = 0\n",
    "last_batch_size = 0\n",
    "processed_event_classes = set()  # Track which OCSF event classes we've seen\n",
    "created_tables = set()  # Track which tables we've already created\n",
    "\n",
    "should_shutdown = False\n",
    "\n",
    "# OCSF Class UID to Name mapping (major classes)\n",
    "OCSF_CLASS_MAPPING = {\n",
    "    \"1001\": \"file_activity\",\n",
    "    \"1002\": \"kernel_extension\",  \n",
    "    \"1003\": \"kernel_activity\",\n",
    "    \"1004\": \"memory_activity\",\n",
    "    \"1005\": \"module_activity\",\n",
    "    \"1006\": \"scheduled_job_activity\",\n",
    "    \"1007\": \"process_activity\",\n",
    "    \"2001\": \"authentication\",\n",
    "    \"2002\": \"authorization\", \n",
    "    \"2003\": \"account_change\",\n",
    "    \"2004\": \"security_finding\",\n",
    "    \"3001\": \"network_activity\",\n",
    "    \"3002\": \"http_activity\",\n",
    "    \"3003\": \"dns_activity\", \n",
    "    \"3004\": \"dhcp_activity\",\n",
    "    \"3005\": \"rdp_activity\",\n",
    "    \"3006\": \"smb_activity\",\n",
    "    \"3007\": \"ssh_activity\",\n",
    "    \"3008\": \"ftp_activity\",\n",
    "    \"3009\": \"email_activity\",\n",
    "    \"4001\": \"network_file_activity\",\n",
    "    \"4002\": \"email_file_activity\",\n",
    "    \"4003\": \"email_url_activity\",\n",
    "    \"4004\": \"web_resources_activity\",\n",
    "    \"5001\": \"inventory_info\",\n",
    "    \"5002\": \"device_config_state\",\n",
    "    \"5003\": \"user_access\",\n",
    "    \"6001\": \"compliance_finding\",\n",
    "    \"6002\": \"detection_finding\",\n",
    "    \"6003\": \"incident_finding\",\n",
    "    \"6004\": \"security_finding\",\n",
    "    \"6005\": \"vulnerability_finding\"\n",
    "}\n",
    "\n",
    "# Print a comprehensive status update\n",
    "def print_status(source=\"\"):\n",
    "    global total_message_count, table_row_counts, last_batch_id, last_batch_size, processed_event_classes\n",
    "    if not should_shutdown:\n",
    "        current_time = time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "        total_db_rows = sum(table_row_counts.values())\n",
    "        \n",
    "        # Create summary of table counts\n",
    "        table_summary = \", \".join([f\"{event_class}: {count}\" for event_class, count in table_row_counts.items()])\n",
    "        if not table_summary:\n",
    "            table_summary = \"No tables yet\"\n",
    "            \n",
    "        print(f\"\\rLast update: {current_time} | Batch {last_batch_id}: {last_batch_size} records | \"\n",
    "              f\"Total messages: {total_message_count} | Total VastDB rows: {total_db_rows} | \"\n",
    "              f\"OCSF Classes: {len(processed_event_classes)} ({', '.join(sorted(processed_event_classes))}) | \"\n",
    "              f\"Tables: [{table_summary}]     \", end=\"\")\n",
    "        \n",
    "        import sys\n",
    "        sys.stdout.flush()\n",
    "\n",
    "# Helper function to create safe VastDB table names\n",
    "def create_vastdb_table_name(event_class):\n",
    "    \"\"\"Create a VastDB table name for the OCSF event class\"\"\"\n",
    "    # Clean up the event class for SQL compatibility\n",
    "    clean_event_class = event_class.replace(\"-\", \"_\").replace(\".\", \"_\")\n",
    "    return f\"`ndb`.`{VASTDB_SIEM_BUCKET}`.`{VASTDB_SIEM_SCHEMA}`.`{VASTDB_SIEM_TABLE_PREFIX}{clean_event_class}`\"\n",
    "\n",
    "# Helper function to create comprehensive OCSF schemas\n",
    "def create_ocsf_schema(class_uid, class_name=None):\n",
    "    \"\"\"Create a comprehensive schema for OCSF event classes\"\"\"\n",
    "    \n",
    "    # Base OCSF fields that are common across all event classes\n",
    "    base_fields = [\n",
    "        # Core OCSF fields\n",
    "        StructField(\"activity_id\", StringType(), True),\n",
    "        StructField(\"activity_name\", StringType(), True),\n",
    "        StructField(\"category_name\", StringType(), True),\n",
    "        StructField(\"category_uid\", StringType(), True),\n",
    "        StructField(\"class_name\", StringType(), True),\n",
    "        StructField(\"class_uid\", StringType(), True),\n",
    "        StructField(\"confidence\", IntegerType(), True),\n",
    "        StructField(\"count\", IntegerType(), True),\n",
    "        StructField(\"duration\", IntegerType(), True),\n",
    "        StructField(\"end_time\", TimestampType(), True),\n",
    "        StructField(\"end_time_dt\", TimestampType(), True),\n",
    "        StructField(\"message\", StringType(), True),\n",
    "        StructField(\"metadata\", StringType(), True),  # JSON string for complex metadata\n",
    "        StructField(\"observables\", StringType(), True),  # Array of observables as JSON\n",
    "        StructField(\"raw_data\", StringType(), True),\n",
    "        StructField(\"severity\", StringType(), True),\n",
    "        StructField(\"severity_id\", StringType(), True),\n",
    "        StructField(\"start_time\", TimestampType(), True),\n",
    "        StructField(\"start_time_dt\", TimestampType(), True),\n",
    "        StructField(\"status\", StringType(), True),\n",
    "        StructField(\"status_code\", StringType(), True),\n",
    "        StructField(\"status_detail\", StringType(), True),\n",
    "        StructField(\"status_id\", StringType(), True),\n",
    "        StructField(\"time\", TimestampType(), True),\n",
    "        StructField(\"time_dt\", TimestampType(), True),\n",
    "        StructField(\"timezone_offset\", IntegerType(), True),\n",
    "        StructField(\"type_name\", StringType(), True),\n",
    "        StructField(\"type_uid\", StringType(), True),\n",
    "        StructField(\"unmapped\", StringType(), True),  # JSON string for unmapped fields\n",
    "        \n",
    "        # Actor fields\n",
    "        StructField(\"actor_user_name\", StringType(), True),\n",
    "        StructField(\"actor_user_type\", StringType(), True),\n",
    "        StructField(\"actor_user_uid\", StringType(), True),\n",
    "        StructField(\"actor_user_email_addr\", StringType(), True),\n",
    "        StructField(\"actor_user_domain\", StringType(), True),\n",
    "        StructField(\"actor_session_uid\", StringType(), True),\n",
    "        StructField(\"actor_session_created_time\", TimestampType(), True),\n",
    "        StructField(\"actor_session_is_remote\", BooleanType(), True),\n",
    "        \n",
    "        # Device fields  \n",
    "        StructField(\"device_name\", StringType(), True),\n",
    "        StructField(\"device_type\", StringType(), True),\n",
    "        StructField(\"device_type_id\", StringType(), True),\n",
    "        StructField(\"device_uid\", StringType(), True),\n",
    "        StructField(\"device_hostname\", StringType(), True),\n",
    "        StructField(\"device_domain\", StringType(), True),\n",
    "        StructField(\"device_ip\", StringType(), True),\n",
    "        StructField(\"device_mac\", StringType(), True),\n",
    "        StructField(\"device_os_name\", StringType(), True),\n",
    "        StructField(\"device_os_version\", StringType(), True),\n",
    "        \n",
    "        # Network endpoint fields\n",
    "        StructField(\"src_endpoint_ip\", StringType(), True),\n",
    "        StructField(\"src_endpoint_port\", IntegerType(), True),\n",
    "        StructField(\"src_endpoint_hostname\", StringType(), True),\n",
    "        StructField(\"src_endpoint_domain\", StringType(), True),\n",
    "        StructField(\"src_endpoint_mac\", StringType(), True),\n",
    "        StructField(\"src_ip\", StringType(), True),\n",
    "        StructField(\"src_port\", IntegerType(), True),\n",
    "        StructField(\"dst_endpoint_ip\", StringType(), True),\n",
    "        StructField(\"dst_endpoint_port\", IntegerType(), True),\n",
    "        StructField(\"dst_endpoint_hostname\", StringType(), True),\n",
    "        StructField(\"dst_endpoint_domain\", StringType(), True),\n",
    "        StructField(\"dst_endpoint_mac\", StringType(), True),\n",
    "        StructField(\"dst_ip\", StringType(), True),\n",
    "        StructField(\"dst_port\", IntegerType(), True),\n",
    "        \n",
    "        # Common user fields\n",
    "        StructField(\"user\", StringType(), True),\n",
    "        StructField(\"user_name\", StringType(), True),\n",
    "        StructField(\"user_uid\", StringType(), True),\n",
    "        StructField(\"user_email_addr\", StringType(), True),\n",
    "        StructField(\"user_domain\", StringType(), True),\n",
    "        \n",
    "        # Event specific fields based on your example\n",
    "        StructField(\"event\", StringType(), True),\n",
    "        StructField(\"original_event_type\", StringType(), True),\n",
    "        \n",
    "        # Product/Vendor fields\n",
    "        StructField(\"ocsf_product_name\", StringType(), True),\n",
    "        StructField(\"ocsf_vendor_name\", StringType(), True),\n",
    "        StructField(\"ocsf_version\", StringType(), True),\n",
    "    ]\n",
    "    \n",
    "    # Add class-specific fields based on class_uid\n",
    "    class_specific_fields = []\n",
    "    \n",
    "    if class_uid == \"2004\":  # Security Finding\n",
    "        class_specific_fields.extend([\n",
    "            StructField(\"finding_uid\", StringType(), True),\n",
    "            StructField(\"finding_title\", StringType(), True),\n",
    "            StructField(\"finding_desc\", StringType(), True),\n",
    "            StructField(\"finding_types\", StringType(), True),  # Array as JSON string\n",
    "            StructField(\"risk_level\", StringType(), True),\n",
    "            StructField(\"risk_level_id\", StringType(), True),\n",
    "            StructField(\"risk_score\", DoubleType(), True),\n",
    "            StructField(\"compliance_standards\", StringType(), True), # Array as JSON string\n",
    "        ])\n",
    "    elif class_uid == \"2001\":  # Authentication\n",
    "        class_specific_fields.extend([\n",
    "            StructField(\"auth_protocol\", StringType(), True),\n",
    "            StructField(\"auth_protocol_id\", StringType(), True),\n",
    "            StructField(\"logon_type\", StringType(), True),\n",
    "            StructField(\"logon_type_id\", StringType(), True),\n",
    "            StructField(\"is_cleartext\", BooleanType(), True),\n",
    "            StructField(\"is_mfa\", BooleanType(), True),\n",
    "            StructField(\"is_new_logon\", BooleanType(), True),\n",
    "            StructField(\"is_remote\", BooleanType(), True),\n",
    "        ])\n",
    "    elif class_uid == \"3001\":  # Network Activity\n",
    "        class_specific_fields.extend([\n",
    "            StructField(\"connection_uid\", StringType(), True),\n",
    "            StructField(\"direction\", StringType(), True),\n",
    "            StructField(\"direction_id\", StringType(), True),\n",
    "            StructField(\"protocol_name\", StringType(), True),\n",
    "            StructField(\"protocol_num\", IntegerType(), True),\n",
    "            StructField(\"session_uid\", StringType(), True),\n",
    "            StructField(\"bytes_in\", LongType(), True),\n",
    "            StructField(\"bytes_out\", LongType(), True),\n",
    "            StructField(\"packets_in\", LongType(), True),\n",
    "            StructField(\"packets_out\", LongType(), True),\n",
    "        ])\n",
    "    elif class_uid == \"3002\":  # HTTP Activity  \n",
    "        class_specific_fields.extend([\n",
    "            StructField(\"http_method\", StringType(), True),\n",
    "            StructField(\"http_status\", IntegerType(), True),\n",
    "            StructField(\"http_response_code\", IntegerType(), True),\n",
    "            StructField(\"url_hostname\", StringType(), True),\n",
    "            StructField(\"url_path\", StringType(), True),\n",
    "            StructField(\"url_port\", IntegerType(), True),\n",
    "            StructField(\"url_query_string\", StringType(), True),\n",
    "            StructField(\"url_scheme\", StringType(), True),\n",
    "            StructField(\"user_agent\", StringType(), True),\n",
    "            StructField(\"referrer\", StringType(), True),\n",
    "            StructField(\"request_uid\", StringType(), True),\n",
    "            StructField(\"response_uid\", StringType(), True),\n",
    "        ])\n",
    "    elif class_uid == \"1007\":  # Process Activity\n",
    "        class_specific_fields.extend([\n",
    "            StructField(\"process_name\", StringType(), True),\n",
    "            StructField(\"process_pid\", IntegerType(), True),\n",
    "            StructField(\"process_uid\", StringType(), True),\n",
    "            StructField(\"process_cmd_line\", StringType(), True),\n",
    "            StructField(\"process_file_name\", StringType(), True),\n",
    "            StructField(\"process_file_path\", StringType(), True),\n",
    "            StructField(\"parent_process_name\", StringType(), True),\n",
    "            StructField(\"parent_process_pid\", IntegerType(), True),\n",
    "            StructField(\"parent_process_uid\", StringType(), True),\n",
    "        ])\n",
    "    \n",
    "    # Combine base fields with class-specific fields\n",
    "    all_fields = base_fields + class_specific_fields\n",
    "    \n",
    "    return StructType(all_fields)\n",
    "\n",
    "# Helper function to determine event class from OCSF data\n",
    "def determine_event_class(ocsf_data):\n",
    "    \"\"\"Determine the event class from OCSF data\"\"\"\n",
    "    if \"class_uid\" in ocsf_data:\n",
    "        class_uid = str(ocsf_data[\"class_uid\"])\n",
    "        class_name = OCSF_CLASS_MAPPING.get(class_uid)\n",
    "        if class_name:\n",
    "            return f\"{class_uid}_{class_name}\"\n",
    "        else:\n",
    "            return f\"class_{class_uid}\"\n",
    "    elif \"type_name\" in ocsf_data:\n",
    "        # Use type_name as fallback, clean it up\n",
    "        type_name = ocsf_data[\"type_name\"].lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "        return f\"type_{type_name}\"\n",
    "    else:\n",
    "        return \"unknown_event\"\n",
    "\n",
    "# Helper function to convert timestamp columns to proper format\n",
    "def convert_timestamp_columns(df, event_class):\n",
    "    \"\"\"Convert timestamp columns to proper TimestampType\"\"\"\n",
    "    timestamp_columns = [\"time\", \"time_dt\", \"start_time\", \"start_time_dt\", \"end_time\", \"end_time_dt\", \n",
    "                        \"actor_session_created_time\"]\n",
    "    \n",
    "    for ts_col in timestamp_columns:\n",
    "        if ts_col in df.columns:\n",
    "            try:\n",
    "                # Handle different timestamp formats\n",
    "                df = df.withColumn(ts_col, \n",
    "                    when(col(ts_col).isNotNull() & (col(ts_col) != \"\"), \n",
    "                         to_timestamp(col(ts_col))\n",
    "                    ).otherwise(None)\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not convert timestamp column {ts_col} for {event_class}: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Helper function to create table schema in VastDB if it doesn't exist\n",
    "def ensure_table_exists(event_class, sample_data):\n",
    "    \"\"\"Ensure the VastDB table exists for this OCSF event class with proper schema\"\"\"\n",
    "    global created_tables\n",
    "    \n",
    "    table_name = create_vastdb_table_name(event_class)\n",
    "    table_key = f\"{VASTDB_SIEM_BUCKET}.{VASTDB_SIEM_SCHEMA}.{event_class}\"\n",
    "    \n",
    "    if table_key not in created_tables:\n",
    "        try:\n",
    "            # Extract class_uid from the sample data to get appropriate schema\n",
    "            sample_dict = json.loads(sample_data) if isinstance(sample_data, str) else sample_data\n",
    "            class_uid = str(sample_dict.get(\"class_uid\", \"unknown\"))\n",
    "            \n",
    "            # Get comprehensive schema for this class\n",
    "            comprehensive_schema = create_ocsf_schema(class_uid)\n",
    "            \n",
    "            # Check if table exists by trying to query it\n",
    "            try:\n",
    "                existing_columns = spark.sql(f\"DESCRIBE {table_name}\").collect()\n",
    "                existing_col_names = [row.col_name for row in existing_columns]\n",
    "                expected_col_names = [field.name for field in comprehensive_schema.fields]\n",
    "                \n",
    "                # If the existing table doesn't have all the expected columns, recreate it\n",
    "                missing_columns = set(expected_col_names) - set(existing_col_names)\n",
    "                if missing_columns:\n",
    "                    print(f\"\\nTable {table_name} exists but missing columns: {len(missing_columns)} columns\")\n",
    "                    print(f\"Dropping and recreating table with comprehensive OCSF schema...\")\n",
    "                    \n",
    "                    # Drop the existing table\n",
    "                    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "                    \n",
    "                    # Create a sample DataFrame with the comprehensive schema\n",
    "                    sample_data_list = [(None,) * len(comprehensive_schema.fields)]\n",
    "                    sample_df = spark.createDataFrame(sample_data_list, comprehensive_schema)\n",
    "                    sample_df.limit(0).write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "                    print(f\"Recreated table {table_name} with {len(comprehensive_schema.fields)} columns\")\n",
    "                \n",
    "                created_tables.add(table_key)\n",
    "                return table_name\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Table doesn't exist, create it with comprehensive schema\n",
    "                try:\n",
    "                    sample_data_list = [(None,) * len(comprehensive_schema.fields)]\n",
    "                    sample_df = spark.createDataFrame(sample_data_list, comprehensive_schema)\n",
    "                    sample_df.limit(0).write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "                    print(f\"\\nCreated new OCSF table {table_name} for class_uid {class_uid} ({len(comprehensive_schema.fields)} columns)\")\n",
    "                    created_tables.add(table_key)\n",
    "                    return table_name\n",
    "                except Exception as create_error:\n",
    "                    print(f\"\\nError creating table with comprehensive schema: {create_error}\")\n",
    "                    # Fall back to dynamic creation\n",
    "                    created_tables.add(table_key)\n",
    "                    return table_name\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in ensure_table_exists for {event_class}: {e}\")\n",
    "            created_tables.add(table_key)\n",
    "            return table_name\n",
    "    else:\n",
    "        return create_vastdb_table_name(event_class)\n",
    "\n",
    "# Process each microbatch with dynamic table routing based on OCSF event classes\n",
    "def process_microbatch(raw_df, epoch_id):\n",
    "    global total_message_count, last_batch_id, last_batch_size, processed_event_classes\n",
    "    if not should_shutdown:\n",
    "        try:\n",
    "            batch_size = raw_df.count()\n",
    "            if batch_size == 0:\n",
    "                return\n",
    "                \n",
    "            total_message_count += batch_size\n",
    "            last_batch_id = epoch_id\n",
    "            last_batch_size = batch_size\n",
    "            \n",
    "            # Collect all JSON strings to determine event classes\n",
    "            json_strings = [row.json for row in raw_df.collect()]\n",
    "            \n",
    "            # Group messages by event class\n",
    "            event_class_groups = {}\n",
    "            for json_str in json_strings:\n",
    "                try:\n",
    "                    parsed = json.loads(json_str)\n",
    "                    event_class = determine_event_class(parsed)\n",
    "                    processed_event_classes.add(event_class)\n",
    "                    \n",
    "                    if event_class not in event_class_groups:\n",
    "                        event_class_groups[event_class] = []\n",
    "                    event_class_groups[event_class].append(json_str)\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError parsing OCSF JSON: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Process each event class group\n",
    "            for event_class, json_list in event_class_groups.items():\n",
    "                try:\n",
    "                    # Create DataFrame for this event class\n",
    "                    event_class_rdd = spark.sparkContext.parallelize([(json_str,) for json_str in json_list])\n",
    "                    event_class_df = spark.createDataFrame(event_class_rdd, [\"json\"])\n",
    "                    \n",
    "                    # Parse the JSON directly (FluentD logs are flat JSON, not nested like Zeek)\n",
    "                    sample_json = event_class_df.select(\"json\").first()\n",
    "                    if sample_json and sample_json.json:\n",
    "                        try:\n",
    "                            sample_dict = json.loads(sample_json.json)\n",
    "                            class_uid = str(sample_dict.get(\"class_uid\", \"unknown\"))\n",
    "                            \n",
    "                            # Get comprehensive OCSF schema\n",
    "                            comprehensive_schema = create_ocsf_schema(class_uid)\n",
    "                            \n",
    "                            # Parse with comprehensive schema\n",
    "                            parsed_df = event_class_df.select(\n",
    "                                from_json(col(\"json\"), comprehensive_schema).alias(\"parsed\")\n",
    "                            ).select(\"parsed.*\")\n",
    "                            \n",
    "                            # Convert timestamp columns\n",
    "                            parsed_df = convert_timestamp_columns(parsed_df, event_class)\n",
    "                            \n",
    "                            # Ensure table exists and get table name\n",
    "                            table_name = ensure_table_exists(event_class, sample_json.json)\n",
    "                            \n",
    "                            # Write to VastDB table specific to this event class with error handling\n",
    "                            try:\n",
    "                                parsed_df.write.mode(\"append\").saveAsTable(table_name)\n",
    "                            except Exception as write_error:\n",
    "                                if \"Py4JNetworkError\" in str(write_error) or \"Answer from Java side is empty\" in str(write_error):\n",
    "                                    print(f\"\\nSpark connection error writing to {table_name}: {write_error}\")\n",
    "                                    time.sleep(2)\n",
    "                                else:\n",
    "                                    raise write_error\n",
    "                                \n",
    "                        except Exception as schema_error:\n",
    "                            print(f\"\\nSchema processing error for {event_class}: {schema_error}\")\n",
    "                            # Fallback: store as raw JSON string\n",
    "                            try:\n",
    "                                fallback_df = event_class_df.select(col(\"json\").alias(\"raw_json\"))\n",
    "                                table_name = f\"`ndb`.`{VASTDB_SIEM_BUCKET}`.`{VASTDB_SIEM_SCHEMA}`.`{event_class}_raw`\"\n",
    "                                fallback_df.write.mode(\"append\").saveAsTable(table_name)\n",
    "                            except Exception as fallback_error:\n",
    "                                print(f\"\\nFallback failed for {event_class}: {fallback_error}\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError processing event class {event_class}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            print_status(\"Batch\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nException in process_microbatch: {e}\")\n",
    "\n",
    "# Function to periodically check and update row counts for all VastDB tables\n",
    "def check_row_counts():\n",
    "    global table_row_counts, should_shutdown, processed_event_classes\n",
    "    while not should_shutdown:\n",
    "        time.sleep(3)  # Check every 3 seconds\n",
    "        try:\n",
    "            if should_shutdown:\n",
    "                break\n",
    "            # Create a copy of processed_event_classes to avoid modification during iteration\n",
    "            event_classes_to_check = list(processed_event_classes)\n",
    "            for event_class in event_classes_to_check:\n",
    "                if should_shutdown:\n",
    "                    break\n",
    "                try:\n",
    "                    table_name = create_vastdb_table_name(event_class)\n",
    "                    # Add timeout and error handling for Spark SQL calls\n",
    "                    new_count = spark.sql(f\"SELECT count(*) FROM {table_name}\").collect()[0][0]\n",
    "                    if table_row_counts.get(event_class, 0) != new_count:\n",
    "                        table_row_counts[event_class] = new_count\n",
    "                        print_status(\"DB Count\")\n",
    "                except Exception as e:\n",
    "                    # Table might not exist yet or be accessible, or Spark connection issues\n",
    "                    if \"Py4JNetworkError\" in str(e) or \"Answer from Java side is empty\" in str(e):\n",
    "                        print(f\"\\nSpark connection issue in row count check: {e}\")\n",
    "                        time.sleep(5)  # Wait longer before next attempt\n",
    "                    pass\n",
    "        except Exception as e:\n",
    "            if should_shutdown:\n",
    "                break\n",
    "            # Ignore most errors in checking, but log network issues\n",
    "            if \"Py4JNetworkError\" in str(e):\n",
    "                print(f\"\\nNetwork error in check_row_counts: {e}\")\n",
    "                time.sleep(5)\n",
    "\n",
    "# Read data from Kafka stream\n",
    "raw_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_brokers) \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"failOnDataLoss\", \"true\") \\\n",
    "    .load()\n",
    "\n",
    "# Decode Kafka messages as JSON strings\n",
    "decoded_stream = raw_stream.selectExpr(\"CAST(value AS STRING) as json\")\n",
    "\n",
    "# Main processing query - using the dynamic approach for OCSF events\n",
    "ocsf_query = decoded_stream.writeStream \\\n",
    "    .foreachBatch(process_microbatch) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime='2 seconds') \\\n",
    "    .option(\"maxFilesPerTrigger\", 1000) \\\n",
    "    .option(\"checkpointLocation\", checkpoint_dir) \\\n",
    "    .start()\n",
    "\n",
    "# Print initial status message\n",
    "print(\"\\nStarting FluentD OCSF log streaming job to VastDB...\")\n",
    "print(\"This will dynamically create VastDB tables for each OCSF event class (security_finding, authentication, etc.)\")\n",
    "print(f\"Tables will be created in: ndb.{VASTDB_SIEM_BUCKET}.{VASTDB_SIEM_SCHEMA}\")\n",
    "print(\"All timestamp fields will be properly converted to TimestampType for optimal query performance.\")\n",
    "print(\"OCSF event classes will be automatically detected and routed to appropriate tables.\")\n",
    "print_status(\"Init\")\n",
    "\n",
    "# Start thread for checking row counts\n",
    "row_count_thread = threading.Thread(target=check_row_counts)\n",
    "row_count_thread.daemon = True\n",
    "row_count_thread.start()\n",
    "\n",
    "shutdown_flag = threading.Event()\n",
    "\n",
    "def signal_handler(sig, frame):\n",
    "    global should_shutdown\n",
    "    print(\"\\nGraceful shutdown initiated...\")\n",
    "    should_shutdown = True\n",
    "    shutdown_flag.set()\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "signal.signal(signal.SIGTERM, signal_handler)\n",
    "\n",
    "# Main loop\n",
    "try:\n",
    "    while ocsf_query.isActive and not shutdown_flag.is_set():\n",
    "        time.sleep(1)\n",
    "    if ocsf_query.isActive:\n",
    "        ocsf_query.stop()\n",
    "    ocsf_query.awaitTermination()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during awaitTermination: {e}\")\n",
    "\n",
    "print(\"\\nFinal status:\")\n",
    "for event_class, count in table_row_counts.items():\n",
    "    print(f\"  {VASTDB_SIEM_BUCKET}.{VASTDB_SIEM_SCHEMA}.{VASTDB_SIEM_TABLE_PREFIX}{event_class}: {count} rows\")\n",
    "print(\"VastDB FluentD OCSF streaming completed. Goodbye!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d090cd21-ead2-4b17-bc99-3ab2473f6356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
