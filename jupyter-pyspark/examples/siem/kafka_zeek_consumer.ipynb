{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fa2523-9d41-4bc1-810b-1ccc85a9aec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "DOCKER_HOST_OR_IP=10.143.11.241\n",
      "---\n",
      "VASTDB_ENDPOINT=http://172.200.204.2:80\n",
      "VASTDB_ACCESS_KEY==****QXN5\n",
      "VASTDB_SECRET_KEY=****oLGr\n",
      "VASTDB_SIEM_BUCKET=csnow-db\n",
      "VASTDB_SIEM_SCHEMA=zeek-live-logs\n",
      "VASTDB_SIEM_TABLE_PREFIX=zeek_\n",
      "---\n",
      "VAST_KAFKA_BROKER=172.200.204.1:9092\n",
      "topic=zeek-live-logs\n",
      "\n",
      "Spark successfully loaded\n",
      "\n",
      "\n",
      "Starting Zeek log streaming job to VastDB...\n",
      "This will dynamically create VastDB tables for each Zeek log type (conn, analyzer, weird, etc.)\n",
      "Tables will be created in: ndb.csnow-db.zeek-live-logs\n",
      "All timestamp fields will be properly converted to TimestampType for optimal query performance.\n",
      "Last update: 09:36:10 | Batch 26331: 162 records | Total messages: 824131 | Total VastDB rows: 2361103 | Log types: 7 (analyzer, conn, dns, ftp, http, known_services, weird) | Tables: [conn: 1186888, known_services: 88, http: 213403, ftp: 24717, analyzer: 22692, dns: 913045, weird: 270]     "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Fraud_Detector\n",
    "\n",
    "# # Trade Settlement (Spark Streaming app that consumes stock settlement data from Kafka and stores them into the  VAST Database\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# Load environment variables for Kafka and VastDB connectivity\n",
    "DOCKER_HOST_OR_IP = os.getenv(\"DOCKER_HOST_OR_IP\", \"localhost\")\n",
    "VASTDB_ENDPOINT = os.getenv(\"VASTDB_ENDPOINT\")\n",
    "VASTDB_ACCESS_KEY = os.getenv(\"VASTDB_ACCESS_KEY\")\n",
    "VASTDB_SECRET_KEY = os.getenv(\"VASTDB_SECRET_KEY\")\n",
    "\n",
    "VASTDB_SIEM_BUCKET = os.getenv(\"VASTDB_SIEM_BUCKET\", 'csnow-db')\n",
    "VASTDB_SIEM_SCHEMA = os.getenv(\"VASTDB_SIEM_SCHEMA\", 'zeek-live-logs')\n",
    "VASTDB_SIEM_TABLE_PREFIX = 'zeek_'\n",
    "\n",
    "use_vastkafka = True\n",
    "if use_vastkafka:\n",
    "    VAST_KAFKA_BROKER = os.getenv(\"VAST_KAFKA_BROKER\")\n",
    "else:\n",
    "    VAST_KAFKA_BROKER = f\"{DOCKER_HOST_OR_IP}:19092\"\n",
    "\n",
    "kafka_brokers = VAST_KAFKA_BROKER\n",
    "topic = 'zeek-live-logs'\n",
    "\n",
    "# Print configurations\n",
    "print(f\"\"\"\n",
    "---\n",
    "DOCKER_HOST_OR_IP={DOCKER_HOST_OR_IP}\n",
    "---\n",
    "VASTDB_ENDPOINT={VASTDB_ENDPOINT}\n",
    "VASTDB_ACCESS_KEY==****{VASTDB_ACCESS_KEY[-4:]}\n",
    "VASTDB_SECRET_KEY=****{VASTDB_SECRET_KEY[-4:]}\n",
    "VASTDB_SIEM_BUCKET={VASTDB_SIEM_BUCKET}\n",
    "VASTDB_SIEM_SCHEMA={VASTDB_SIEM_SCHEMA}\n",
    "VASTDB_SIEM_TABLE_PREFIX={VASTDB_SIEM_TABLE_PREFIX}\n",
    "---\n",
    "VAST_KAFKA_BROKER={VAST_KAFKA_BROKER}\n",
    "topic={topic}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "# Create Vast DB schema if it doesn't exist.\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "get_ipython().run_cell_magic('capture', '--no-stderr', '%pip install --quiet -U vastdb\\n')\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "import vastdb\n",
    "\n",
    "session = vastdb.connect(endpoint=VASTDB_ENDPOINT, access=VASTDB_ACCESS_KEY, secret=VASTDB_SECRET_KEY)\n",
    "with session.transaction() as tx:\n",
    "    bucket = tx.bucket(VASTDB_SIEM_BUCKET)\n",
    "    bucket.schema(VASTDB_SIEM_SCHEMA, fail_if_missing=False) or bucket.create_schema(VASTDB_SIEM_SCHEMA)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "import socket\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, count, get_json_object, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, BooleanType, TimestampType\n",
    "import threading\n",
    "import time\n",
    "import json  # <-- MISSING IMPORT ADDED HERE\n",
    "\n",
    "# Spark Configuration\n",
    "conf = SparkConf()\n",
    "conf.setAll([\n",
    "    (\"spark.driver.host\", socket.gethostbyname(socket.gethostname())),\n",
    "    (\"spark.sql.execution.arrow.pyspark.enabled\", \"false\"),\n",
    "    # VASTDB\n",
    "    (\"spark.sql.catalog.ndb\", 'spark.sql.catalog.ndb.VastCatalog'),\n",
    "    (\"spark.ndb.endpoint\", VASTDB_ENDPOINT),\n",
    "    (\"spark.ndb.data_endpoints\", VASTDB_ENDPOINT),\n",
    "    (\"spark.ndb.access_key_id\", VASTDB_ACCESS_KEY),\n",
    "    (\"spark.ndb.secret_access_key\", VASTDB_SECRET_KEY),\n",
    "    (\"spark.driver.extraClassPath\", '/usr/local/spark/jars/spark3-vast-3.4.1-f93839bfa38a/*'),\n",
    "    (\"spark.executor.extraClassPath\", '/usr/local/spark/jars/spark3-vast-3.4.1-f93839bfa38a/*'),\n",
    "    (\"spark.sql.extensions\", 'ndb.NDBSparkSessionExtension'),\n",
    "    # Kafka\n",
    "    (\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.4.3,\" \n",
    "                            \"org.apache.logging.log4j:log4j-slf4j2-impl:2.19.0,\" \n",
    "                            \"org.apache.logging.log4j:log4j-api:2.19.0,\" \n",
    "                            \"org.apache.logging.log4j:log4j-core:2.19.0\"),\n",
    "    (\"spark.jars.excludes\", \"org.slf4j:slf4j-api,org.slf4j:slf4j-log4j12\"),\n",
    "    (\"spark.hadoop.fs.file.impl\", \"org.apache.hadoop.fs.RawLocalFileSystem\"),\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"KafkaStreamingToVastDB\") \\\n",
    "    .config(conf=conf) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"DEBUG\")\n",
    "\n",
    "print(\"Spark successfully loaded\\n\")\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "destination_table_name_prefix = f\"`ndb`.`{VASTDB_SIEM_BUCKET}`.`{VASTDB_SIEM_SCHEMA}`.`{VASTDB_SIEM_TABLE_PREFIX}`\"\n",
    "destination_table_name_prefix\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "import os\n",
    "import signal\n",
    "import time\n",
    "import threading\n",
    "import pyspark\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create checkpoint directory with absolute path\n",
    "checkpoint_dir = os.path.abspath(\"/tmp/spark_checkpoint\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Global variables for tracking\n",
    "total_message_count = 0\n",
    "table_row_counts = {}  # Track row counts per table\n",
    "last_batch_id = 0\n",
    "last_batch_size = 0\n",
    "processed_log_types = set()  # Track which log types we've seen\n",
    "created_tables = set()  # Track which tables we've already created\n",
    "\n",
    "should_shutdown = False\n",
    "\n",
    "# Print a comprehensive status update\n",
    "def print_status(source=\"\"):\n",
    "    global total_message_count, table_row_counts, last_batch_id, last_batch_size, processed_log_types\n",
    "    if not should_shutdown:\n",
    "        current_time = time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "        total_db_rows = sum(table_row_counts.values())\n",
    "        \n",
    "        # Create summary of table counts\n",
    "        table_summary = \", \".join([f\"{log_type}: {count}\" for log_type, count in table_row_counts.items()])\n",
    "        if not table_summary:\n",
    "            table_summary = \"No tables yet\"\n",
    "            \n",
    "        print(f\"\\rLast update: {current_time} | Batch {last_batch_id}: {last_batch_size} records | \"\n",
    "              f\"Total messages: {total_message_count} | Total VastDB rows: {total_db_rows} | \"\n",
    "              f\"Log types: {len(processed_log_types)} ({', '.join(sorted(processed_log_types))}) | \"\n",
    "              f\"Tables: [{table_summary}]     \", end=\"\")\n",
    "        \n",
    "        import sys\n",
    "        sys.stdout.flush()\n",
    "\n",
    "# Helper function to create safe VastDB table names\n",
    "def create_vastdb_table_name(log_type):\n",
    "    \"\"\"Create a VastDB table name for the log type\"\"\"\n",
    "    # Clean up the log type for SQL compatibility\n",
    "    clean_log_type = log_type.replace(\"-\", \"_\").replace(\".\", \"_\")\n",
    "    return f\"`ndb`.`{VASTDB_SIEM_BUCKET}`.`{VASTDB_SIEM_SCHEMA}`.`{clean_log_type}`\"\n",
    "\n",
    "# Helper function to create comprehensive schema for Zeek log types\n",
    "def create_comprehensive_schema(log_type):\n",
    "    \"\"\"Create a comprehensive schema that includes all possible fields for a log type\"\"\"\n",
    "    \n",
    "    # Define comprehensive schemas for common Zeek log types with proper TimestampType\n",
    "    zeek_schemas = {\n",
    "        'analyzer': StructType([\n",
    "            StructField(\"ts\", TimestampType(), True),\n",
    "            StructField(\"cause\", StringType(), True),\n",
    "            StructField(\"analyzer_kind\", StringType(), True),\n",
    "            StructField(\"analyzer_name\", StringType(), True),\n",
    "            StructField(\"uid\", StringType(), True),  # Optional field\n",
    "            StructField(\"fuid\", StringType(), True),  # Optional field\n",
    "            StructField(\"id.orig_h\", StringType(), True),  # Optional field from conn_id\n",
    "            StructField(\"id.orig_p\", LongType(), True),  # Optional field from conn_id\n",
    "            StructField(\"id.resp_h\", StringType(), True),  # Optional field from conn_id\n",
    "            StructField(\"id.resp_p\", LongType(), True),  # Optional field from conn_id\n",
    "            StructField(\"failure_reason\", StringType(), True),  # Optional field\n",
    "            StructField(\"failure_data\", StringType(), True),  # Optional field\n",
    "        ]),\n",
    "        'conn': StructType([\n",
    "            StructField(\"ts\", TimestampType(), True),\n",
    "            StructField(\"uid\", StringType(), True),\n",
    "            StructField(\"id.orig_h\", StringType(), True),\n",
    "            StructField(\"id.orig_p\", LongType(), True),\n",
    "            StructField(\"id.resp_h\", StringType(), True),\n",
    "            StructField(\"id.resp_p\", LongType(), True),\n",
    "            StructField(\"proto\", StringType(), True),\n",
    "            StructField(\"service\", StringType(), True),  # Optional field\n",
    "            StructField(\"duration\", DoubleType(), True),  # Optional field\n",
    "            StructField(\"orig_bytes\", LongType(), True),  # Optional field\n",
    "            StructField(\"resp_bytes\", LongType(), True),  # Optional field\n",
    "            StructField(\"conn_state\", StringType(), True),\n",
    "            StructField(\"local_orig\", BooleanType(), True),\n",
    "            StructField(\"local_resp\", BooleanType(), True),\n",
    "            StructField(\"missed_bytes\", LongType(), True),\n",
    "            StructField(\"history\", StringType(), True),\n",
    "            StructField(\"orig_pkts\", LongType(), True),\n",
    "            StructField(\"orig_ip_bytes\", LongType(), True),\n",
    "            StructField(\"resp_pkts\", LongType(), True),\n",
    "            StructField(\"resp_ip_bytes\", LongType(), True),\n",
    "            StructField(\"ip_proto\", StringType(), True),\n",
    "            StructField(\"tunnel_parents\", StringType(), True),  # Optional field\n",
    "            # Additional common Zeek conn.log fields\n",
    "            StructField(\"vlan\", LongType(), True),  # VLAN tag\n",
    "            StructField(\"inner_vlan\", LongType(), True),  # Inner VLAN tag\n",
    "            StructField(\"orig_l2_addr\", StringType(), True),  # Original L2 address\n",
    "            StructField(\"resp_l2_addr\", StringType(), True),  # Responder L2 address\n",
    "            StructField(\"community_id\", StringType(), True),  # Community ID hash\n",
    "            StructField(\"orig_shim\", StringType(), True),  # Original shim header\n",
    "            StructField(\"resp_shim\", StringType(), True),  # Responder shim header\n",
    "        ]),\n",
    "        'http': StructType([\n",
    "            StructField(\"ts\", TimestampType(), True),\n",
    "            StructField(\"uid\", StringType(), True),\n",
    "            StructField(\"id.orig_h\", StringType(), True),\n",
    "            StructField(\"id.orig_p\", LongType(), True),\n",
    "            StructField(\"id.resp_h\", StringType(), True),\n",
    "            StructField(\"id.resp_p\", LongType(), True),\n",
    "            StructField(\"trans_depth\", LongType(), True),\n",
    "            StructField(\"method\", StringType(), True),\n",
    "            StructField(\"host\", StringType(), True),\n",
    "            StructField(\"uri\", StringType(), True),\n",
    "            StructField(\"referrer\", StringType(), True),\n",
    "            StructField(\"version\", StringType(), True),\n",
    "            StructField(\"user_agent\", StringType(), True),\n",
    "            StructField(\"origin\", StringType(), True),\n",
    "            StructField(\"request_body_len\", LongType(), True),\n",
    "            StructField(\"response_body_len\", LongType(), True),\n",
    "            StructField(\"status_code\", LongType(), True),\n",
    "            StructField(\"status_msg\", StringType(), True),\n",
    "            StructField(\"info_code\", LongType(), True),\n",
    "            StructField(\"info_msg\", StringType(), True),\n",
    "            StructField(\"tags\", StringType(), True),\n",
    "            StructField(\"username\", StringType(), True),\n",
    "            StructField(\"password\", StringType(), True),\n",
    "            StructField(\"proxied\", StringType(), True),\n",
    "            StructField(\"orig_fuids\", StringType(), True),\n",
    "            StructField(\"orig_filenames\", StringType(), True),\n",
    "            StructField(\"orig_mime_types\", StringType(), True),\n",
    "            StructField(\"resp_fuids\", StringType(), True),\n",
    "            StructField(\"resp_filenames\", StringType(), True),\n",
    "            StructField(\"resp_mime_types\", StringType(), True),\n",
    "        ]),\n",
    "        'dns': StructType([\n",
    "            StructField(\"ts\", TimestampType(), True),\n",
    "            StructField(\"uid\", StringType(), True),\n",
    "            StructField(\"id.orig_h\", StringType(), True),\n",
    "            StructField(\"id.orig_p\", LongType(), True),\n",
    "            StructField(\"id.resp_h\", StringType(), True),\n",
    "            StructField(\"id.resp_p\", LongType(), True),\n",
    "            StructField(\"proto\", StringType(), True),\n",
    "            StructField(\"trans_id\", LongType(), True),\n",
    "            StructField(\"rtt\", DoubleType(), True),\n",
    "            StructField(\"query\", StringType(), True),\n",
    "            StructField(\"qclass\", LongType(), True),\n",
    "            StructField(\"qclass_name\", StringType(), True),\n",
    "            StructField(\"qtype\", LongType(), True),\n",
    "            StructField(\"qtype_name\", StringType(), True),\n",
    "            StructField(\"rcode\", LongType(), True),\n",
    "            StructField(\"rcode_name\", StringType(), True),\n",
    "            StructField(\"AA\", BooleanType(), True),\n",
    "            StructField(\"TC\", BooleanType(), True),\n",
    "            StructField(\"RD\", BooleanType(), True),\n",
    "            StructField(\"RA\", BooleanType(), True),\n",
    "            StructField(\"Z\", LongType(), True),\n",
    "            StructField(\"answers\", StringType(), True),\n",
    "            StructField(\"TTLs\", StringType(), True),\n",
    "            StructField(\"rejected\", BooleanType(), True),\n",
    "        ]),\n",
    "        'ssl': StructType([\n",
    "            StructField(\"ts\", TimestampType(), True),\n",
    "            StructField(\"uid\", StringType(), True),\n",
    "            StructField(\"id.orig_h\", StringType(), True),\n",
    "            StructField(\"id.orig_p\", LongType(), True),\n",
    "            StructField(\"id.resp_h\", StringType(), True),\n",
    "            StructField(\"id.resp_p\", LongType(), True),\n",
    "            StructField(\"version\", StringType(), True),\n",
    "            StructField(\"cipher\", StringType(), True),\n",
    "            StructField(\"curve\", StringType(), True),\n",
    "            StructField(\"server_name\", StringType(), True),\n",
    "            StructField(\"resumed\", BooleanType(), True),\n",
    "            StructField(\"last_alert\", StringType(), True),\n",
    "            StructField(\"next_protocol\", StringType(), True),\n",
    "            StructField(\"established\", BooleanType(), True),\n",
    "            StructField(\"cert_chain_fuids\", StringType(), True),\n",
    "            StructField(\"client_cert_chain_fuids\", StringType(), True),\n",
    "            StructField(\"subject\", StringType(), True),\n",
    "            StructField(\"issuer\", StringType(), True),\n",
    "            StructField(\"client_subject\", StringType(), True),\n",
    "            StructField(\"client_issuer\", StringType(), True),\n",
    "            StructField(\"validation_status\", StringType(), True),\n",
    "        ]),\n",
    "        'weird': StructType([\n",
    "            StructField(\"ts\", TimestampType(), True),\n",
    "            StructField(\"uid\", StringType(), True),\n",
    "            StructField(\"id.orig_h\", StringType(), True),\n",
    "            StructField(\"id.orig_p\", LongType(), True),\n",
    "            StructField(\"id.resp_h\", StringType(), True),\n",
    "            StructField(\"id.resp_p\", LongType(), True),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"addl\", StringType(), True),\n",
    "            StructField(\"notice\", BooleanType(), True),\n",
    "            StructField(\"peer\", StringType(), True),\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    # Return predefined schema if available, otherwise return None for dynamic inference\n",
    "    return zeek_schemas.get(log_type, None)\n",
    "\n",
    "# Helper function to convert timestamp string to proper format\n",
    "def convert_timestamp_column(df, log_type):\n",
    "    \"\"\"Convert timestamp string to proper TimestampType\"\"\"\n",
    "    if \"ts\" in df.columns:\n",
    "        # Handle different timestamp formats that might be in Zeek logs\n",
    "        try:\n",
    "            # Try ISO format first: \"2025-05-31T20:53:25.978455Z\"\n",
    "            df = df.withColumn(\"ts\", to_timestamp(col(\"ts\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSSSS'Z'\"))\n",
    "        except:\n",
    "            try:\n",
    "                # Try without microseconds: \"2025-05-31T20:53:25.978Z\"\n",
    "                df = df.withColumn(\"ts\", to_timestamp(col(\"ts\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"))\n",
    "            except:\n",
    "                try:\n",
    "                    # Try basic ISO format\n",
    "                    df = df.withColumn(\"ts\", to_timestamp(col(\"ts\")))\n",
    "                except:\n",
    "                    print(f\"Warning: Could not convert timestamp for {log_type}, keeping as string\")\n",
    "    return df\n",
    "\n",
    "# Helper function to create table schema in VastDB if it doesn't exist\n",
    "def ensure_table_exists(log_type, sample_data):\n",
    "    \"\"\"Ensure the VastDB table exists for this log type with proper schema\"\"\"\n",
    "    global created_tables\n",
    "    \n",
    "    table_name = create_vastdb_table_name(log_type)\n",
    "    table_key = f\"{VASTDB_SIEM_BUCKET}.{VASTDB_SIEM_SCHEMA}.{log_type}\"\n",
    "    \n",
    "    # Check if we have a comprehensive schema for this log type\n",
    "    comprehensive_schema = create_comprehensive_schema(log_type)\n",
    "    \n",
    "    if comprehensive_schema and table_key not in created_tables:\n",
    "        try:\n",
    "            # Check if table exists by trying to query it\n",
    "            existing_columns = spark.sql(f\"DESCRIBE {table_name}\").collect()\n",
    "            existing_col_names = [row.col_name for row in existing_columns]\n",
    "            expected_col_names = [field.name for field in comprehensive_schema.fields]\n",
    "            \n",
    "            # If the existing table doesn't have all the expected columns, we need to recreate it\n",
    "            missing_columns = set(expected_col_names) - set(existing_col_names)\n",
    "            if missing_columns:\n",
    "                print(f\"\\nTable {table_name} exists but missing columns: {missing_columns}\")\n",
    "                print(f\"Dropping and recreating table with comprehensive schema...\")\n",
    "                \n",
    "                # Drop the existing table\n",
    "                spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "                \n",
    "                # Create a sample DataFrame with the comprehensive schema to create the table\n",
    "                sample_data_list = [(None,) * len(comprehensive_schema.fields)]\n",
    "                sample_df = spark.createDataFrame(sample_data_list, comprehensive_schema)\n",
    "                sample_df.limit(0).write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "                print(f\"Recreated table {table_name} with {len(comprehensive_schema.fields)} columns\")\n",
    "            \n",
    "            created_tables.add(table_key)\n",
    "            return table_name\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Table doesn't exist, create it with comprehensive schema\n",
    "            try:\n",
    "                sample_data_list = [(None,) * len(comprehensive_schema.fields)]\n",
    "                sample_df = spark.createDataFrame(sample_data_list, comprehensive_schema)\n",
    "                sample_df.limit(0).write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "                print(f\"\\nCreated new table {table_name} with comprehensive schema ({len(comprehensive_schema.fields)} columns)\")\n",
    "                created_tables.add(table_key)\n",
    "                return table_name\n",
    "            except Exception as create_error:\n",
    "                print(f\"\\nError creating table with comprehensive schema: {create_error}\")\n",
    "                # Fall back to dynamic creation\n",
    "                created_tables.add(table_key)\n",
    "                return table_name\n",
    "    else:\n",
    "        # No comprehensive schema available or already created, use existing logic\n",
    "        if table_key in created_tables:\n",
    "            return table_name\n",
    "        \n",
    "        try:\n",
    "            # Try to query the table to see if it exists\n",
    "            spark.sql(f\"SELECT 1 FROM {table_name} LIMIT 1\")\n",
    "            created_tables.add(table_key)\n",
    "            return table_name\n",
    "        except:\n",
    "            # Table doesn't exist, we'll let Spark create it dynamically\n",
    "            created_tables.add(table_key)\n",
    "            return table_name\n",
    "\n",
    "# Process each microbatch with dynamic table routing and timestamp conversion\n",
    "def process_microbatch(raw_df, epoch_id):\n",
    "    global total_message_count, last_batch_id, last_batch_size, processed_log_types\n",
    "    if not should_shutdown:\n",
    "        try:\n",
    "            batch_size = raw_df.count()\n",
    "            if batch_size == 0:\n",
    "                return\n",
    "                \n",
    "            total_message_count += batch_size\n",
    "            last_batch_id = epoch_id\n",
    "            last_batch_size = batch_size\n",
    "            \n",
    "            # Collect all JSON strings to determine log types\n",
    "            json_strings = [row.json for row in raw_df.collect()]\n",
    "            \n",
    "            # Group messages by log type\n",
    "            log_type_groups = {}\n",
    "            for json_str in json_strings:\n",
    "                try:\n",
    "                    parsed = json.loads(json_str)\n",
    "                    # Get the top-level key (log type)\n",
    "                    log_type = list(parsed.keys())[0]\n",
    "                    processed_log_types.add(log_type)\n",
    "                    \n",
    "                    if log_type not in log_type_groups:\n",
    "                        log_type_groups[log_type] = []\n",
    "                    log_type_groups[log_type].append(json_str)\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError parsing JSON: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Process each log type group\n",
    "            for log_type, json_list in log_type_groups.items():\n",
    "                try:\n",
    "                    # Create DataFrame for this log type\n",
    "                    log_type_rdd = spark.sparkContext.parallelize([(json_str,) for json_str in json_list])\n",
    "                    log_type_df = spark.createDataFrame(log_type_rdd, [\"json\"])\n",
    "                    \n",
    "                    # Extract the nested object for this log type using get_json_object\n",
    "                    extracted_df = log_type_df.select(\n",
    "                        get_json_object(col(\"json\"), f\"$.{log_type}\").alias(\"log_data\")\n",
    "                    ).filter(col(\"log_data\").isNotNull())\n",
    "                    \n",
    "                    if extracted_df.count() > 0:\n",
    "                        # Use from_json with comprehensive or inferred schema\n",
    "                        sample_json = extracted_df.select(\"log_data\").first()\n",
    "                        if sample_json and sample_json.log_data:\n",
    "                            try:\n",
    "                                # Try to use comprehensive schema first\n",
    "                                comprehensive_schema = create_comprehensive_schema(log_type)\n",
    "                                \n",
    "                                if comprehensive_schema:\n",
    "                                    # Use predefined comprehensive schema\n",
    "                                    parsed_df = extracted_df.select(\n",
    "                                        from_json(col(\"log_data\"), comprehensive_schema).alias(\"parsed\")\n",
    "                                    ).select(\"parsed.*\")\n",
    "                                    \n",
    "                                    # Convert timestamp column for predefined schemas\n",
    "                                    parsed_df = convert_timestamp_column(parsed_df, log_type)\n",
    "                                else:\n",
    "                                    # Fall back to dynamic schema inference\n",
    "                                    sample_dict = json.loads(sample_json.log_data)\n",
    "                                    \n",
    "                                    # Create a flexible schema that accommodates common types\n",
    "                                    fields = []\n",
    "                                    for key, value in sample_dict.items():\n",
    "                                        if key == \"ts\" and isinstance(value, str):\n",
    "                                            # For timestamp fields, start with StringType and convert later\n",
    "                                            fields.append(StructField(key, StringType(), True))\n",
    "                                        elif isinstance(value, str):\n",
    "                                            fields.append(StructField(key, StringType(), True))\n",
    "                                        elif isinstance(value, int):\n",
    "                                            fields.append(StructField(key, LongType(), True))\n",
    "                                        elif isinstance(value, float):\n",
    "                                            fields.append(StructField(key, DoubleType(), True))\n",
    "                                        elif isinstance(value, bool):\n",
    "                                            fields.append(StructField(key, BooleanType(), True))\n",
    "                                        else:\n",
    "                                            # Default to string for complex types\n",
    "                                            fields.append(StructField(key, StringType(), True))\n",
    "                                    \n",
    "                                    inferred_schema = StructType(fields)\n",
    "                                    \n",
    "                                    # Parse with inferred schema\n",
    "                                    parsed_df = extracted_df.select(\n",
    "                                        from_json(col(\"log_data\"), inferred_schema).alias(\"parsed\")\n",
    "                                    ).select(\"parsed.*\")\n",
    "                                    \n",
    "                                    # Convert timestamp column for dynamic schemas\n",
    "                                    parsed_df = convert_timestamp_column(parsed_df, log_type)\n",
    "                                \n",
    "                                # Ensure table exists and get table name\n",
    "                                table_name = ensure_table_exists(log_type, sample_json.log_data)\n",
    "                                \n",
    "                                # Write to VastDB table specific to this log type with error handling\n",
    "                                try:\n",
    "                                    parsed_df.write.mode(\"append\").saveAsTable(table_name)\n",
    "                                except Exception as write_error:\n",
    "                                    if \"Py4JNetworkError\" in str(write_error) or \"Answer from Java side is empty\" in str(write_error):\n",
    "                                        print(f\"\\nSpark connection error writing to {table_name}: {write_error}\")\n",
    "                                        # Try to reconnect or handle gracefully\n",
    "                                        time.sleep(2)\n",
    "                                    else:\n",
    "                                        raise write_error\n",
    "                                \n",
    "                            except Exception as schema_error:\n",
    "                                print(f\"\\nSchema inference error for {log_type}: {schema_error}\")\n",
    "                                # Fallback: store as raw JSON string\n",
    "                                try:\n",
    "                                    fallback_df = extracted_df.select(col(\"log_data\").alias(\"raw_json\"))\n",
    "                                    table_name = f\"`ndb`.`{VASTDB_SIEM_BUCKET}`.`{VASTDB_SIEM_SCHEMA}`.`{log_type}_raw`\"\n",
    "                                    fallback_df.write.mode(\"append\").saveAsTable(table_name)\n",
    "                                except Exception as fallback_error:\n",
    "                                    print(f\"\\nFallback failed for {log_type}: {fallback_error}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError processing log type {log_type}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            print_status(\"Batch\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nException in process_microbatch: {e}\")\n",
    "\n",
    "# Function to periodically check and update row counts for all VastDB tables\n",
    "def check_row_counts():\n",
    "    global table_row_counts, should_shutdown, processed_log_types\n",
    "    while not should_shutdown:\n",
    "        time.sleep(3)  # Check every 3 seconds\n",
    "        try:\n",
    "            if should_shutdown:\n",
    "                break\n",
    "            # Create a copy of processed_log_types to avoid modification during iteration\n",
    "            log_types_to_check = list(processed_log_types)\n",
    "            for log_type in log_types_to_check:\n",
    "                if should_shutdown:\n",
    "                    break\n",
    "                try:\n",
    "                    table_name = create_vastdb_table_name(log_type)\n",
    "                    # Add timeout and error handling for Spark SQL calls\n",
    "                    new_count = spark.sql(f\"SELECT count(*) FROM {table_name}\").collect()[0][0]\n",
    "                    if table_row_counts.get(log_type, 0) != new_count:\n",
    "                        table_row_counts[log_type] = new_count\n",
    "                        print_status(\"DB Count\")\n",
    "                except Exception as e:\n",
    "                    # Table might not exist yet or be accessible, or Spark connection issues\n",
    "                    if \"Py4JNetworkError\" in str(e) or \"Answer from Java side is empty\" in str(e):\n",
    "                        print(f\"\\nSpark connection issue in row count check: {e}\")\n",
    "                        time.sleep(5)  # Wait longer before next attempt\n",
    "                    pass\n",
    "        except Exception as e:\n",
    "            if should_shutdown:\n",
    "                break\n",
    "            # Ignore most errors in checking, but log network issues\n",
    "            if \"Py4JNetworkError\" in str(e):\n",
    "                print(f\"\\nNetwork error in check_row_counts: {e}\")\n",
    "                time.sleep(5)\n",
    "\n",
    "# Read data from Kafka stream\n",
    "raw_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_brokers) \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"failOnDataLoss\", \"true\") \\\n",
    "    .load()\n",
    "\n",
    "# Decode Kafka messages as JSON strings\n",
    "decoded_stream = raw_stream.selectExpr(\"CAST(value AS STRING) as json\")\n",
    "\n",
    "# Main processing query - using the dynamic approach\n",
    "zeek_query = decoded_stream.writeStream \\\n",
    "    .foreachBatch(process_microbatch) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime='2 seconds') \\\n",
    "    .option(\"maxFilesPerTrigger\", 1000) \\\n",
    "    .option(\"checkpointLocation\", checkpoint_dir) \\\n",
    "    .start()\n",
    "\n",
    "# Print initial status message\n",
    "print(\"\\nStarting Zeek log streaming job to VastDB...\")\n",
    "print(\"This will dynamically create VastDB tables for each Zeek log type (conn, analyzer, weird, etc.)\")\n",
    "print(f\"Tables will be created in: ndb.{VASTDB_SIEM_BUCKET}.{VASTDB_SIEM_SCHEMA}\")\n",
    "print(\"All timestamp fields will be properly converted to TimestampType for optimal query performance.\")\n",
    "print_status(\"Init\")\n",
    "\n",
    "# Start thread for checking row counts\n",
    "row_count_thread = threading.Thread(target=check_row_counts)\n",
    "row_count_thread.daemon = True\n",
    "row_count_thread.start()\n",
    "\n",
    "shutdown_flag = threading.Event()\n",
    "\n",
    "def signal_handler(sig, frame):\n",
    "    global should_shutdown\n",
    "    print(\"\\nGraceful shutdown initiated...\")\n",
    "    should_shutdown = True\n",
    "    shutdown_flag.set()\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "signal.signal(signal.SIGTERM, signal_handler)\n",
    "\n",
    "# Main loop\n",
    "try:\n",
    "    while zeek_query.isActive and not shutdown_flag.is_set():\n",
    "        time.sleep(1)\n",
    "    if zeek_query.isActive:\n",
    "        zeek_query.stop()\n",
    "    zeek_query.awaitTermination()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during awaitTermination: {e}\")\n",
    "\n",
    "print(\"\\nFinal status:\")\n",
    "for log_type, count in table_row_counts.items():\n",
    "    print(f\"  {VASTDB_SIEM_BUCKET}.{VASTDB_SIEM_SCHEMA}.{log_type}: {count} rows\")\n",
    "print(\"VastDB Zeek streaming completed. Goodbye!\")\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcb2837-823a-4261-803c-f30bd5b60520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
