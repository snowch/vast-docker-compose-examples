{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11fc660f-dff9-45e0-919a-a9460d42fd47",
   "metadata": {},
   "source": [
    "# Python SDK - import netcdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8923de48",
   "metadata": {},
   "source": [
    "## Introduction: Streamlining NetCDF Data with VAST Database\n",
    "\n",
    "NetCDF (Network Common Data Form) files are a standard for storing scientific data, particularly large, multi-dimensional datasets like climate and weather information. While effective, managing and querying data directly from NetCDF files can be cumbersome, slow, and resource-intensive. This is where VAST Database offers a significant advantage.\n",
    "\n",
    "**Benefits of using VAST Database over direct NetCDF file handling:**\n",
    "\n",
    "* **Scalability and Performance:** VAST is designed for high-performance analytics on massive datasets. It allows for efficient querying and analysis of NetCDF data at scale, significantly outperforming traditional file-based approaches.\n",
    "* **Efficient Data Management:** VAST provides a structured, database-driven approach to data management. This simplifies data organization, versioning, and access control, reducing the complexity associated with managing numerous NetCDF files.\n",
    "* **Optimized Querying:** VAST enables complex queries using SQL-like syntax, allowing for fast and flexible data retrieval. This is particularly beneficial for tasks like finding minimum/maximum values, filtering data based on conditions, and performing aggregations.\n",
    "* **Parallel Processing:** VAST leverages parallel processing to accelerate data ingestion and querying. This is crucial for handling the large volumes of data typically found in NetCDF files.\n",
    "* **Simplified Data Access:** Instead of dealing with individual NetCDF files and their specific structures, VAST provides a unified interface for accessing and analyzing the data.\n",
    "* **Integration with Python and other tools:** VastDB python SDK allows easy integration with python, and tools like pandas and pyarrow, for data manipulation and analysis.\n",
    "* **Reduced I/O Overhead:** By storing data in a columnar format, VAST minimizes I/O operations, leading to faster query execution compared to row-based file access.\n",
    "\n",
    "The provided Python script demonstrates how to:\n",
    "\n",
    "1.  **Ingest NetCDF data into a VAST Database:** It reads NetCDF files, converts them to Pandas DataFrames, and then loads them into a VAST table using PyArrow for efficient data transfer.\n",
    "2.  **Query and Analyze data within VAST:** It utilizes the VAST Python SDK to perform analytical queries, such as finding minimum and maximum values for a specified column and retrieving rows containing the maximum value.\n",
    "\n",
    "By leveraging VAST Database, users can unlock the full potential of their NetCDF data, enabling faster, more efficient, and scalable analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e77b2dd",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Install Vast DB sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdb21a6-64b3-4e6d-ba33-51c4c7d37aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vastdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75ee834-6664-4006-ab5f-38fcc450173a",
   "metadata": {},
   "source": [
    "### Install NETCDF and Xarray Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b0c58-348f-4d44-8ccb-5973fa77a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y netcdf4 h5netcdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91027b9-fb36-422e-bb4a-afb2e466c849",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install xarray netCDF4 --yes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d7916a-18bb-4c60-bd69-6ddea89695d2",
   "metadata": {},
   "source": [
    "## Import NETCDF Files into VAST Database\n",
    "\n",
    "Adjust File Location, Database Connection details before executing the script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c8f1550-5be5-4525-9cef-404110938664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 11:14:43,927 - 4090262 - INFO - Using VAST Endpoint: http://172.200.203.1:80\n",
      "2025-03-26 11:14:43,928 - 4090262 - INFO - Using VAST Bucket: christian-db\n",
      "2025-03-26 11:14:43,929 - 4090262 - INFO - Using Target Schema: netcdfdemo\n",
      "2025-03-26 11:14:43,929 - 4090262 - INFO - Using Target Table: weather\n",
      "2025-03-26 11:14:43,933 - 4090262 - INFO - Starting NetCDF to VAST Data ingestion process using vastdb_sdk...\n",
      "2025-03-26 11:14:43,933 - 4090262 - INFO - Searching for NetCDF files (*ncdd*.nc) in: /home/jovyan/datastore/weather_data/netcdf\n",
      "2025-03-26 11:14:43,938 - 4090262 - INFO - Found 1 NetCDF files.\n",
      "2025-03-26 11:14:43,944 - 4090262 - INFO - Successfully connected VAST session to endpoint: http://172.200.203.1:80\n",
      "2025-03-26 11:14:43,945 - 4090262 - INFO - Reading first file to determine schema: /home/jovyan/datastore/weather_data/netcdf/netcdf_files/ncdd-202211-grd-scaled.nc\n",
      "2025-03-26 11:14:58,435 - 4090262 - INFO - NetCDF file 'ncdd-202211-grd-scaled.nc' converted to DataFrame in 14.49s. Shape: (24763800, 7)\n",
      "2025-03-26 11:14:58,453 - 4090262 - INFO - Inferred PyArrow schema from DataFrame.\n",
      "2025-03-26 11:14:58,456 - 4090262 - INFO - Derived PyArrow Schema: time: timestamp[ns]\n",
      "lat: float\n",
      "lon: float\n",
      "tmax: double\n",
      "tmin: double\n",
      "prcp: double\n",
      "tavg: double\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [], \"column_indexes\": [], \"columns\": [{\"name\":' + 842\n",
      "2025-03-26 11:14:58,501 - 4090262 - INFO - Schema 'netcdfdemo' not found. Creating...\n",
      "2025-03-26 11:14:58,512 - 4090262 - INFO - Created schema: netcdfdemo\n",
      "2025-03-26 11:14:58,522 - 4090262 - INFO - Schema 'netcdfdemo' created successfully.\n",
      "2025-03-26 11:14:58,529 - 4090262 - INFO - Table 'netcdfdemo.weather' not found. Creating...\n",
      "2025-03-26 11:14:58,578 - 4090262 - INFO - Created table: weather\n",
      "2025-03-26 11:14:58,626 - 4090262 - INFO - Table 'netcdfdemo.weather' created successfully.\n",
      "2025-03-26 11:14:58,638 - 4090262 - INFO - Distributing 1 files across 1 worker processes.\n",
      "2025-03-26 11:14:58,653 - 4095577 - INFO - Worker started processing batch of 1 files.\n",
      "2025-03-26 11:14:58,698 - 4095577 - INFO - Successfully connected VAST session to endpoint: http://172.200.203.1:80\n",
      "2025-03-26 11:14:58,701 - 4095577 - INFO - Processing file: ncdd-202211-grd-scaled.nc\n",
      "2025-03-26 11:15:11,279 - 4095577 - INFO - NetCDF file 'ncdd-202211-grd-scaled.nc' converted to DataFrame in 12.58s. Shape: (24763800, 7)\n",
      "2025-03-26 11:15:11,549 - 4095577 - INFO - Converted DataFrame to Arrow Table for ncdd-202211-grd-scaled.nc. Shape: (24763800, 7)\n",
      "2025-03-26 11:15:11,551 - 4095577 - INFO - Inserting 24763800 rows into table 'netcdfdemo.weather'...\n",
      "2025-03-26 11:15:28,721 - 4095577 - INFO - 24763800 rows inserted into table 'netcdfdemo.weather' in 17.17s.\n",
      "2025-03-26 11:15:28,745 - 4095577 - INFO - Worker finished. Processed: 1, Failed: 0\n",
      "2025-03-26 11:15:28,769 - 4090262 - INFO - --- Ingestion Summary ---\n",
      "2025-03-26 11:15:28,770 - 4090262 - INFO - Target Bucket: christian-db\n",
      "2025-03-26 11:15:28,770 - 4090262 - INFO - Target Schema.Table: netcdfdemo.weather\n",
      "2025-03-26 11:15:28,771 - 4090262 - INFO - Total NetCDF files found: 1\n",
      "2025-03-26 11:15:28,772 - 4090262 - INFO - Successfully processed files: 1\n",
      "2025-03-26 11:15:28,772 - 4090262 - INFO - Failed files: 0\n",
      "2025-03-26 11:15:28,773 - 4090262 - INFO - Total script execution time: 44.84 seconds.\n",
      "2025-03-26 11:15:28,773 - 4090262 - INFO - -------------------------\n"
     ]
    }
   ],
   "source": [
    "import os # Ensure os is imported\n",
    "import time\n",
    "import glob\n",
    "import multiprocessing\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import math\n",
    "import logging\n",
    "\n",
    "# Import VAST SDK and PyArrow\n",
    "import vastdb\n",
    "import pyarrow as pa\n",
    "from vastdb import errors as vast_errors # Import specific errors module\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# VAST Cluster Connection Details from Environment Variables\n",
    "VAST_ENDPOINT = os.getenv(\"VASTDB_ENDPOINT\")\n",
    "ACCESS_KEY = os.getenv(\"VASTDB_ACCESS_KEY\")\n",
    "SECRET_KEY = os.getenv(\"VASTDB_SECRET_KEY\")\n",
    "\n",
    "# Data and Table Details (Updated as per request)\n",
    "SOURCE_DIRECTORY = '/home/jovyan/datastore/weather_data/netcdf' # Directory containing .nc files (keep or adjust as needed)\n",
    "TARGET_BUCKET = 'christian-db' # Updated Bucket Name\n",
    "TARGET_SCHEMA = 'netcdfdemo'    # Updated Schema Name\n",
    "TARGET_TABLE_NAME = 'weather'     # Updated Table Name\n",
    "\n",
    "# Performance Tuning\n",
    "MAX_PROCESSES = 18 # Adjust based on your client machine's resources\n",
    "\n",
    "# --- Logging Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(process)d - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Validation for Environment Variables ---\n",
    "if not VAST_ENDPOINT:\n",
    "    logging.error(\"Environment variable VASTDB_ENDPOINT is not set.\")\n",
    "    exit(\"Error: VASTDB_ENDPOINT environment variable missing.\") # Exit if essential config is missing\n",
    "if not ACCESS_KEY:\n",
    "    logging.error(\"Environment variable VASTDB_ACCESS_KEY is not set.\")\n",
    "    exit(\"Error: VASTDB_ACCESS_KEY environment variable missing.\") # Exit if essential config is missing\n",
    "if not SECRET_KEY:\n",
    "    logging.error(\"Environment variable VASTDB_SECRET_KEY is not set.\")\n",
    "    exit(\"Error: VASTDB_SECRET_KEY environment variable missing.\") # Exit if essential config is missing\n",
    "\n",
    "logging.info(f\"Using VAST Endpoint: {VAST_ENDPOINT}\")\n",
    "logging.info(f\"Using VAST Bucket: {TARGET_BUCKET}\")\n",
    "logging.info(f\"Using Target Schema: {TARGET_SCHEMA}\")\n",
    "logging.info(f\"Using Target Table: {TARGET_TABLE_NAME}\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def create_vast_session(endpoint, access_key, secret_key):\n",
    "    \"\"\"Creates a VAST SDK session instance.\"\"\"\n",
    "    # Function remains the same, uses provided arguments\n",
    "    try:\n",
    "        session = vastdb.connect(\n",
    "            endpoint=endpoint,\n",
    "            access=access_key,\n",
    "            secret=secret_key,\n",
    "            # verify_ssl=False # Add if needed\n",
    "        )\n",
    "        logging.info(f\"Successfully connected VAST session to endpoint: {endpoint}\")\n",
    "        return session\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create VAST session: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_pyarrow_schema_from_dataframe(df):\n",
    "    \"\"\"Generates a pyarrow.Schema from a Pandas DataFrame.\"\"\"\n",
    "    # Function remains the same\n",
    "    try:\n",
    "        arrow_schema = pa.Schema.from_pandas(df, preserve_index=False)\n",
    "        logging.info(\"Inferred PyArrow schema from DataFrame.\")\n",
    "        return arrow_schema\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create PyArrow schema from DataFrame: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_vast_schema_and_table(session, bucket_name, schema_name, table_name, pa_schema):\n",
    "    \"\"\"Creates schema and table if they don't exist within a transaction.\"\"\"\n",
    "    # Function remains the same, uses provided arguments\n",
    "    try:\n",
    "        with session.transaction() as tx:\n",
    "            bucket = tx.bucket(bucket_name)\n",
    "            try:\n",
    "                schema = bucket.schema(schema_name)\n",
    "                logging.info(f\"Schema '{schema_name}' already exists in bucket '{bucket_name}'.\")\n",
    "            except vast_errors.MissingSchema:\n",
    "                logging.info(f\"Schema '{schema_name}' not found. Creating...\")\n",
    "                schema = bucket.create_schema(schema_name)\n",
    "                logging.info(f\"Schema '{schema_name}' created successfully.\")\n",
    "\n",
    "            try:\n",
    "                table = schema.table(table_name)\n",
    "                logging.info(f\"Table '{schema_name}.{table_name}' already exists.\")\n",
    "            except vast_errors.MissingTable:\n",
    "                logging.info(f\"Table '{schema_name}.{table_name}' not found. Creating...\")\n",
    "                table = schema.create_table(table_name, columns=pa_schema)\n",
    "                logging.info(f\"Table '{schema_name}.{table_name}' created successfully.\")\n",
    "\n",
    "    except (vast_errors.SchemaExists, vast_errors.TableExists) as e:\n",
    "         logging.warning(f\"Schema or Table creation conflict (likely already exists): {e}\")\n",
    "    except vast_errors.HttpError as e:\n",
    "        logging.error(f\"VAST HTTP Error during schema/table setup: {e.status} - {e.message}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error during schema/table setup: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def netcdf_to_dataframe(file_path):\n",
    "    \"\"\"Converts a NetCDF file to a Pandas DataFrame.\"\"\"\n",
    "    # Function remains the same\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        with xr.open_dataset(file_path) as ds:\n",
    "            df = ds.to_dataframe().reset_index()\n",
    "        elapsed = time.time() - start_time\n",
    "        logging.info(f\"NetCDF file '{os.path.basename(file_path)}' converted to DataFrame in {elapsed:.2f}s. Shape: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error converting NetCDF file '{file_path}': {e}\")\n",
    "        return None\n",
    "\n",
    "def insert_pyarrow_table_to_vast(session, bucket_name, schema_name, table_name, arrow_table):\n",
    "    \"\"\"Inserts a PyArrow Table into a VAST Data table within a transaction.\"\"\"\n",
    "    # Function remains the same, uses provided arguments\n",
    "    if arrow_table is None or arrow_table.num_rows == 0:\n",
    "        logging.warning(f\"Skipping insertion into '{schema_name}.{table_name}' due to empty or invalid Arrow Table.\")\n",
    "        return False\n",
    "\n",
    "    logging.info(f\"Inserting {arrow_table.num_rows} rows into table '{schema_name}.{table_name}'...\")\n",
    "    start_time = time.time()\n",
    "    inserted = False\n",
    "    try:\n",
    "        with session.transaction() as tx:\n",
    "            table = tx.bucket(bucket_name).schema(schema_name).table(table_name)\n",
    "            table.insert(arrow_table)\n",
    "        elapsed = time.time() - start_time\n",
    "        logging.info(f\"{arrow_table.num_rows} rows inserted into table '{schema_name}.{table_name}' in {elapsed:.2f}s.\")\n",
    "        inserted = True\n",
    "    except vast_errors.HttpError as e:\n",
    "        logging.error(f\"VAST HTTP Error inserting data into '{schema_name}.{table_name}': {e.status} - {e.message}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error inserting data into '{schema_name}.{table_name}': {e}\")\n",
    "\n",
    "    return inserted\n",
    "\n",
    "# --- Multiprocessing Worker Function ---\n",
    "\n",
    "def process_file_batch(file_batch, bucket, schema_name, table_name, vast_endpoint, access_key, secret_key, pa_schema_for_conversion):\n",
    "    \"\"\"\n",
    "    Worker function for multiprocessing.\n",
    "    Connects to VAST, processes a batch of files, converts to PyArrow Table, and inserts data.\n",
    "    \"\"\"\n",
    "    # Function remains the same, uses provided arguments\n",
    "    worker_session = None\n",
    "    files_processed = 0\n",
    "    files_failed = 0\n",
    "    process_id = os.getpid()\n",
    "    logging.info(f\"Worker started processing batch of {len(file_batch)} files.\")\n",
    "\n",
    "    try:\n",
    "        worker_session = create_vast_session(vast_endpoint, access_key, secret_key)\n",
    "\n",
    "        for file_path in file_batch:\n",
    "            logging.info(f\"Processing file: {os.path.basename(file_path)}\")\n",
    "            df = None\n",
    "            arrow_table = None\n",
    "            try:\n",
    "                df = netcdf_to_dataframe(file_path)\n",
    "                if df is not None and not df.empty:\n",
    "                    arrow_table = pa.Table.from_pandas(df, schema=pa_schema_for_conversion, preserve_index=False)\n",
    "                    logging.info(f\"Converted DataFrame to Arrow Table for {os.path.basename(file_path)}. Shape: {arrow_table.shape}\")\n",
    "                    success = insert_pyarrow_table_to_vast(worker_session, bucket, schema_name, table_name, arrow_table)\n",
    "                    if success:\n",
    "                        files_processed += 1\n",
    "                    else:\n",
    "                        files_failed += 1\n",
    "                elif df is not None and df.empty:\n",
    "                     logging.warning(f\"Skipping empty DataFrame from file {file_path}\")\n",
    "                     files_processed += 1\n",
    "                else:\n",
    "                    files_failed += 1 # Failure during NetCDF conversion\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing/inserting file {file_path}: {e}\")\n",
    "                files_failed += 1\n",
    "            finally:\n",
    "                del df\n",
    "                del arrow_table\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Worker process failed unexpectedly: {e}\")\n",
    "        files_failed = len(file_batch) - files_processed\n",
    "    finally:\n",
    "        logging.info(f\"Worker finished. Processed: {files_processed}, Failed: {files_failed}\")\n",
    "        return files_processed, files_failed\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the NetCDF to VAST Data process.\"\"\"\n",
    "    # Ensure global config variables are accessible if needed, or pass explicitly\n",
    "    global VAST_ENDPOINT, ACCESS_KEY, SECRET_KEY, TARGET_BUCKET, TARGET_SCHEMA, TARGET_TABLE_NAME\n",
    "\n",
    "    logging.info(\"Starting NetCDF to VAST Data ingestion process using vastdb_sdk...\")\n",
    "    script_start_time = time.time()\n",
    "\n",
    "    # 1. Find NetCDF files\n",
    "    logging.info(f\"Searching for NetCDF files (*ncdd*.nc) in: {SOURCE_DIRECTORY}\")\n",
    "    nc_files = glob.glob(os.path.join(SOURCE_DIRECTORY, '**', '*ncdd*.nc'), recursive=True)\n",
    "    num_files = len(nc_files)\n",
    "    logging.info(f\"Found {num_files} NetCDF files.\")\n",
    "\n",
    "    if num_files == 0:\n",
    "        logging.warning(\"No NetCDF files found matching the pattern. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # 2. Create main VAST session (using config from env vars)\n",
    "    try:\n",
    "        # Pass the globally defined config values\n",
    "        main_session = create_vast_session(VAST_ENDPOINT, ACCESS_KEY, SECRET_KEY)\n",
    "    except Exception:\n",
    "        logging.error(\"Could not establish initial VAST session. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # 3. Determine PyArrow Schema and Create Table/Schema (if needed)\n",
    "    pa_schema = None\n",
    "    try:\n",
    "        logging.info(f\"Reading first file to determine schema: {nc_files[0]}\")\n",
    "        first_df = netcdf_to_dataframe(nc_files[0])\n",
    "        if first_df is None:\n",
    "            logging.error(\"Failed to read the first NetCDF file to determine schema. Exiting.\")\n",
    "            return\n",
    "\n",
    "        pa_schema = get_pyarrow_schema_from_dataframe(first_df)\n",
    "        if not pa_schema:\n",
    "            logging.error(\"Could not determine PyArrow schema from the first DataFrame. Exiting.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"Derived PyArrow Schema: {pa_schema}\")\n",
    "        del first_df\n",
    "\n",
    "        # Create schema and table using the derived PyArrow schema and updated names\n",
    "        create_vast_schema_and_table(main_session, TARGET_BUCKET, TARGET_SCHEMA, TARGET_TABLE_NAME, pa_schema)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed during schema derivation or table creation: {e}\")\n",
    "        return\n",
    "    finally:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # 4. Prepare for Multiprocessing\n",
    "    num_processes = min(num_files, MAX_PROCESSES)\n",
    "    if num_processes <= 0 :\n",
    "       logging.error(\"Invalid number of processes calculated. Exiting.\")\n",
    "       return\n",
    "\n",
    "    files_per_process = math.ceil(num_files / num_processes)\n",
    "    file_batches = [nc_files[i:i + files_per_process] for i in range(0, num_files, files_per_process)]\n",
    "    logging.info(f\"Distributing {num_files} files across {num_processes} worker processes.\")\n",
    "\n",
    "    # 5. Run Multiprocessing Pool\n",
    "    total_processed = 0\n",
    "    total_failed = 0\n",
    "    # Pass updated config values to the worker processes\n",
    "    pool_args = [(batch, TARGET_BUCKET, TARGET_SCHEMA, TARGET_TABLE_NAME, VAST_ENDPOINT, ACCESS_KEY, SECRET_KEY, pa_schema) for batch in file_batches]\n",
    "\n",
    "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "        results_async = pool.starmap_async(process_file_batch, pool_args)\n",
    "        try:\n",
    "            worker_outputs = results_async.get()\n",
    "            for processed_count, failed_count in worker_outputs:\n",
    "                total_processed += processed_count\n",
    "                total_failed += failed_count\n",
    "        except KeyboardInterrupt:\n",
    "            logging.warning(\"Process interrupted by user. Terminating pool.\")\n",
    "            pool.terminate()\n",
    "            pool.join()\n",
    "            total_failed = num_files - total_processed\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error occurred during multiprocessing execution: {e}\")\n",
    "            total_failed = num_files - total_processed\n",
    "\n",
    "    # 6. Log Summary\n",
    "    script_elapsed_time = time.time() - script_start_time\n",
    "    logging.info(\"--- Ingestion Summary ---\")\n",
    "    logging.info(f\"Target Bucket: {TARGET_BUCKET}\")\n",
    "    logging.info(f\"Target Schema.Table: {TARGET_SCHEMA}.{TARGET_TABLE_NAME}\")\n",
    "    logging.info(f\"Total NetCDF files found: {num_files}\")\n",
    "    logging.info(f\"Successfully processed files: {total_processed}\")\n",
    "    logging.info(f\"Failed files: {total_failed}\")\n",
    "    logging.info(f\"Total script execution time: {script_elapsed_time:.2f} seconds.\")\n",
    "    logging.info(\"-------------------------\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing.set_start_method('spawn') # Consider uncommenting for macOS/Windows\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24eec02-fd90-4282-82c7-aebdb9562295",
   "metadata": {},
   "source": [
    "## Read data from the VAST Database using python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f554be8-e69d-4759-ab45-9cfd34910ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 11:48:49,679 - 4090262 - INFO - Using VAST Endpoint: http://172.200.203.1:80\n",
      "2025-03-26 11:48:49,681 - 4090262 - INFO - Querying Bucket: christian-db\n",
      "2025-03-26 11:48:49,682 - 4090262 - INFO - Querying Schema: netcdfdemo\n",
      "2025-03-26 11:48:49,683 - 4090262 - INFO - Querying Table: weather\n",
      "2025-03-26 11:48:49,684 - 4090262 - INFO - Analyzing Column: prcp\n",
      "2025-03-26 11:48:49,686 - 4090262 - INFO - Script started at 2025-03-26 11:48:49\n",
      "2025-03-26 11:48:49,695 - 4090262 - INFO - Successfully connected VAST session.\n",
      "2025-03-26 11:48:49,697 - 4090262 - INFO - Starting analysis for 'prcp' in netcdfdemo.weather...\n",
      "2025-03-26 11:48:49,727 - 4090262 - INFO - Accessed table netcdfdemo.weather\n",
      "2025-03-26 11:48:49,728 - 4090262 - INFO - Executing select for column: prcp to find min/max\n",
      "2025-03-26 11:48:49,738 - 4090262 - INFO - Reading data into PyArrow table...\n",
      "2025-03-26 11:48:50,560 - 4090262 - INFO - Read 24763800 records for min/max calculation.\n",
      "2025-03-26 11:48:50,561 - 4090262 - INFO - Converting to Pandas DataFrame for min/max...\n",
      "2025-03-26 11:48:50,942 - 4090262 - INFO - Calculated MIN=0.0, MAX=302.3125.\n",
      "2025-03-26 11:48:50,944 - 4090262 - INFO - Executing select for full rows where prcp == 302.3125\n",
      "2025-03-26 11:48:50,957 - 4090262 - INFO - Reading max value rows into PyArrow table...\n",
      "2025-03-26 11:48:51,046 - 4090262 - INFO - Found 1 row(s) with the max value.\n",
      "2025-03-26 11:48:51,047 - 4090262 - INFO - Converting max rows to Pandas DataFrame...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " VAST Data Query Summary - 2025-03-26 11:48:51\n",
      "============================================================\n",
      " Target:         christian-db.netcdfdemo.weather\n",
      " Analyzed Column: prcp\n",
      " Query Duration: 0:00:01.368626\n",
      "------------------------------------------------------------\n",
      " Statistics for 'prcp':\n",
      "   Total Records Read (for prcp): 24763800\n",
      "   Minimum Value:  0.0\n",
      "   Maximum Value:  302.3125\n",
      "------------------------------------------------------------\n",
      " Full Row(s) Containing Maximum 'prcp' Value (302.3125):\n",
      "      time       lat         lon      tmax      tmin     prcp     tavg\n",
      "2022-11-05 46.854168 -121.770836 -4.828125 -9.726562 302.3125 -7.28125\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import vastdb\n",
    "import pyarrow as pa\n",
    "from vastdb import errors as vast_errors\n",
    "import logging\n",
    "import pandas as pd\n",
    "import sys\n",
    "from datetime import datetime # To get current time for summary\n",
    "\n",
    "# ibis is needed for creating the predicate (_.prcp == max_val)\n",
    "try:\n",
    "    from ibis import _\n",
    "    IBIS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    logging.warning(\"`ibis-framework` not found. Install it (`pip install ibis-framework`) to filter for max value rows.\")\n",
    "    IBIS_AVAILABLE = False\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# VAST Cluster Connection Details from Environment Variables\n",
    "VAST_ENDPOINT = os.getenv(\"VASTDB_ENDPOINT\")\n",
    "ACCESS_KEY = os.getenv(\"VASTDB_ACCESS_KEY\")\n",
    "SECRET_KEY = os.getenv(\"VASTDB_SECRET_KEY\")\n",
    "\n",
    "# Data and Table Details\n",
    "TARGET_BUCKET = 'christian-db'\n",
    "TARGET_SCHEMA = 'netcdfdemo'\n",
    "TARGET_TABLE_NAME = 'weather'\n",
    "TARGET_COLUMN = 'prcp' # Column to query for min/max\n",
    "\n",
    "# --- Logging Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Validation for Environment Variables ---\n",
    "# (Keep the validation block from the previous script)\n",
    "if not VAST_ENDPOINT:\n",
    "    logging.error(\"Environment variable VASTDB_ENDPOINT is not set.\")\n",
    "    sys.exit(\"Error: VASTDB_ENDPOINT environment variable missing.\")\n",
    "if not ACCESS_KEY:\n",
    "    logging.error(\"Environment variable VASTDB_ACCESS_KEY is not set.\")\n",
    "    sys.exit(\"Error: VASTDB_ACCESS_KEY environment variable missing.\")\n",
    "if not SECRET_KEY:\n",
    "    logging.error(\"Environment variable VASTDB_SECRET_KEY is not set.\")\n",
    "    sys.exit(\"Error: VASTDB_SECRET_KEY environment variable missing.\")\n",
    "\n",
    "logging.info(f\"Using VAST Endpoint: {VAST_ENDPOINT}\")\n",
    "logging.info(f\"Querying Bucket: {TARGET_BUCKET}\")\n",
    "logging.info(f\"Querying Schema: {TARGET_SCHEMA}\")\n",
    "logging.info(f\"Querying Table: {TARGET_TABLE_NAME}\")\n",
    "logging.info(f\"Analyzing Column: {TARGET_COLUMN}\")\n",
    "\n",
    "\n",
    "def get_stats_and_max_rows(session, bucket_name, schema_name, table_name, column_name):\n",
    "    \"\"\"\n",
    "    Connects to VAST, finds min/max of a column, and retrieves row(s) with the max value.\n",
    "    Returns: min_val, max_val, record_count, max_rows_df (Pandas DF or None)\n",
    "    \"\"\"\n",
    "    min_val = None\n",
    "    max_val = None\n",
    "    record_count = 0\n",
    "    max_rows_df = pd.DataFrame() # Initialize empty DataFrame\n",
    "\n",
    "    if not IBIS_AVAILABLE:\n",
    "        logging.error(\"Cannot fetch max rows because `ibis-framework` is not installed.\")\n",
    "        # Still attempt to get min/max if possible without ibis for filtering\n",
    "        # You might want to return early or handle this differently\n",
    "        # For now, we'll proceed to get min/max but won't query for max rows.\n",
    "        pass\n",
    "\n",
    "    logging.info(f\"Starting analysis for '{column_name}' in {schema_name}.{table_name}...\")\n",
    "\n",
    "    try:\n",
    "        # Use a single transaction for both steps if possible\n",
    "        with session.transaction() as tx:\n",
    "            try:\n",
    "                table = tx.bucket(bucket_name).schema(schema_name).table(table_name)\n",
    "                logging.info(f\"Accessed table {schema_name}.{table_name}\")\n",
    "\n",
    "                # --- Step 1: Find Min/Max Value ---\n",
    "                logging.info(f\"Executing select for column: {column_name} to find min/max\")\n",
    "                reader = table.select(columns=[column_name])\n",
    "\n",
    "                logging.info(\"Reading data into PyArrow table...\")\n",
    "                arrow_table = reader.read_all()\n",
    "                record_count = arrow_table.num_rows\n",
    "                logging.info(f\"Read {record_count} records for min/max calculation.\")\n",
    "\n",
    "                if record_count > 0:\n",
    "                    logging.info(\"Converting to Pandas DataFrame for min/max...\")\n",
    "                    df_col = arrow_table.to_pandas()\n",
    "\n",
    "                    if column_name in df_col.columns and pd.api.types.is_numeric_dtype(df_col[column_name]):\n",
    "                        # Drop NaNs before calculating min/max if they shouldn't be considered\n",
    "                        valid_series = df_col[column_name].dropna()\n",
    "                        if not valid_series.empty:\n",
    "                            min_val = valid_series.min()\n",
    "                            max_val = valid_series.max()\n",
    "                            logging.info(f\"Calculated MIN={min_val}, MAX={max_val}.\")\n",
    "                        else:\n",
    "                             logging.warning(f\"Column '{column_name}' contains only NaN/Null values after dropping.\")\n",
    "                    else:\n",
    "                         logging.warning(f\"Column '{column_name}' not found or is not numeric in the result.\")\n",
    "                    del df_col # Free memory\n",
    "                    del arrow_table # Free memory\n",
    "                else:\n",
    "                    logging.warning(\"Table appears to be empty or column has no data.\")\n",
    "                    return min_val, max_val, record_count, max_rows_df # Return early\n",
    "\n",
    "                # --- Step 2: Find Row(s) with Max Value ---\n",
    "                if max_val is not None and IBIS_AVAILABLE:\n",
    "                    logging.info(f\"Executing select for full rows where {column_name} == {max_val}\")\n",
    "                    try:\n",
    "                        # Create the predicate using ibis placeholder\n",
    "                        predicate = (_[column_name] == max_val)\n",
    "\n",
    "                        # Select all columns (*) where the predicate matches\n",
    "                        max_rows_reader = table.select(predicate=predicate)\n",
    "\n",
    "                        logging.info(\"Reading max value rows into PyArrow table...\")\n",
    "                        max_rows_arrow_table = max_rows_reader.read_all()\n",
    "\n",
    "                        if max_rows_arrow_table.num_rows > 0:\n",
    "                            logging.info(f\"Found {max_rows_arrow_table.num_rows} row(s) with the max value.\")\n",
    "                            logging.info(\"Converting max rows to Pandas DataFrame...\")\n",
    "                            max_rows_df = max_rows_arrow_table.to_pandas()\n",
    "                        else:\n",
    "                            # This might happen with floating point inaccuracies, though less likely with ==\n",
    "                            logging.warning(f\"Could not find rows matching the calculated max value {max_val}. Potentially a float precision issue?\")\n",
    "\n",
    "                    except vast_errors.InvalidArgument as e:\n",
    "                        logging.error(f\"Error during filtering query (maybe column name issue?): {e}\")\n",
    "                    except Exception as e:\n",
    "                         logging.error(f\"An unexpected error occurred fetching max rows: {e}\")\n",
    "\n",
    "                elif max_val is not None and not IBIS_AVAILABLE:\n",
    "                    logging.warning(\"Cannot query for max rows because `ibis-framework` is not installed.\")\n",
    "\n",
    "\n",
    "            except vast_errors.MissingSchema:\n",
    "                logging.error(f\"Schema '{schema_name}' not found in bucket '{bucket_name}'.\")\n",
    "            except vast_errors.MissingTable:\n",
    "                logging.error(f\"Table '{table_name}' not found in schema '{schema_name}'.\")\n",
    "            except vast_errors.InvalidArgument as e:\n",
    "                 if column_name in str(e):\n",
    "                     logging.error(f\"Column '{column_name}' likely not found in table '{table_name}'. Error: {e}\")\n",
    "                 else:\n",
    "                     logging.error(f\"Invalid argument during initial query: {e}\")\n",
    "            except vast_errors.HttpError as e:\n",
    "                logging.error(f\"VAST HTTP Error during query: {e.status} - {e.message}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"An unexpected error occurred during query processing: {e}\")\n",
    "\n",
    "    except vast_errors.ConnectionError as e:\n",
    "        logging.error(f\"VAST Connection Error: {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to establish VAST transaction context: {e}\")\n",
    "\n",
    "    return min_val, max_val, record_count, max_rows_df\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to connect to VAST and run the analysis.\n",
    "    \"\"\"\n",
    "    session = None\n",
    "    start_time = datetime.now()\n",
    "    logging.info(f\"Script started at {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    try:\n",
    "        session = vastdb.connect(\n",
    "            endpoint=VAST_ENDPOINT,\n",
    "            access=ACCESS_KEY,\n",
    "            secret=SECRET_KEY,\n",
    "            # verify_ssl=False # Add if needed\n",
    "        )\n",
    "        logging.info(\"Successfully connected VAST session.\")\n",
    "\n",
    "        min_prcp, max_prcp, count, max_rows_dataframe = get_stats_and_max_rows(\n",
    "            session,\n",
    "            TARGET_BUCKET,\n",
    "            TARGET_SCHEMA,\n",
    "            TARGET_TABLE_NAME,\n",
    "            TARGET_COLUMN\n",
    "        )\n",
    "\n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - start_time\n",
    "\n",
    "        # --- Enhanced Summary ---\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\" VAST Data Query Summary - {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\" Target:         {TARGET_BUCKET}.{TARGET_SCHEMA}.{TARGET_TABLE_NAME}\")\n",
    "        print(f\" Analyzed Column: {TARGET_COLUMN}\")\n",
    "        print(f\" Query Duration: {duration}\")\n",
    "        print(\"-\"*60)\n",
    "        print(f\" Statistics for '{TARGET_COLUMN}':\")\n",
    "        print(f\"   Total Records Read (for {TARGET_COLUMN}): {count}\")\n",
    "        if min_prcp is not None:\n",
    "            print(f\"   Minimum Value:  {min_prcp}\")\n",
    "        else:\n",
    "            print(\"   Minimum Value:  N/A (No valid data found)\")\n",
    "        if max_prcp is not None:\n",
    "            print(f\"   Maximum Value:  {max_prcp}\")\n",
    "        else:\n",
    "            print(\"   Maximum Value:  N/A (No valid data found)\")\n",
    "        print(\"-\"*60)\n",
    "\n",
    "        if not max_rows_dataframe.empty:\n",
    "            print(f\" Full Row(s) Containing Maximum '{TARGET_COLUMN}' Value ({max_prcp}):\")\n",
    "            # Configure pandas display options if needed (e.g., for many columns)\n",
    "            # pd.set_option('display.max_rows', None)\n",
    "            # pd.set_option('display.max_columns', None)\n",
    "            # pd.set_option('display.width', 1000)\n",
    "            print(max_rows_dataframe.to_string(index=False)) # `to_string` avoids truncation sometimes\n",
    "        elif max_prcp is not None and not IBIS_AVAILABLE:\n",
    "             print(\" Row(s) for maximum value could not be retrieved.\")\n",
    "             print(\" Please install `ibis-framework` (`pip install ibis-framework`).\")\n",
    "        elif max_prcp is not None:\n",
    "             print(f\" No rows found matching the calculated maximum value ({max_prcp}).\")\n",
    "        else:\n",
    "             print(\" No maximum value found, cannot display corresponding rows.\")\n",
    "\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred in main execution: {e}\")\n",
    "        print(f\"\\nAn error occurred: {e}\", file=sys.stderr)\n",
    "    # finally:\n",
    "        # Session object from vastdb.connect doesn't have explicit close in examples\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure necessary libraries are installed:\n",
    "    # pip install vastdb pandas pyarrow ibis-framework\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96196293-44a4-41e1-bf73-c63178b73842",
   "metadata": {},
   "source": [
    "## Python API Examples and API Documentation links\n",
    "### Python SDK Examples\n",
    "https://github.com/vast-data/vastdb_sdk \n",
    "\n",
    "### Python SDK Documentation \n",
    "https://vastdb-sdk.readthedocs.io/en/v1.1.0/ \n",
    "\n",
    "### VAST DB Field Engineering Documentation\n",
    "https://vast-data.github.io/data-platform-field-docs/intro.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc41949-4ae8-4f56-849f-c8075c02798d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
