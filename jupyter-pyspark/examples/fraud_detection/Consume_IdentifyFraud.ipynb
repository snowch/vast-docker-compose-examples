{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a523f92d-6eb5-43a6-92bc-80b33ecabbe1",
   "metadata": {},
   "source": [
    "Fraud_Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cba218b-786c-4787-ad23-569d0044fe1f",
   "metadata": {},
   "source": [
    "# Trade Settlement (Spark Streaming app that consumes stock settlement data from Kafka and stores them into the  VAST Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8311525d-91b5-440b-986a-cf708b915c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "DOCKER_HOST_OR_IP=10.143.11.241\n",
      "---\n",
      "VASTDB_ENDPOINT=http://172.200.204.2:80\n",
      "VASTDB_ACCESS_KEY==****QXN5\n",
      "VASTDB_SECRET_KEY=****oLGr\n",
      "VASTDB_FRAUD_DETECTION_BUCKET=csnow-db\n",
      "VASTDB_FRAUD_DETECTION_SCHEMA=fraud_detection\n",
      "# VASTDB_FRAUD_DETECTION_TABLE=fraud\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Load environment variables for Kafka and VastDB connectivity\n",
    "DOCKER_HOST_OR_IP = os.getenv(\"DOCKER_HOST_OR_IP\", \"localhost\")\n",
    "VASTDB_ENDPOINT = os.getenv(\"VASTDB_ENDPOINT\")\n",
    "VASTDB_ACCESS_KEY = os.getenv(\"VASTDB_ACCESS_KEY\")\n",
    "VASTDB_SECRET_KEY = os.getenv(\"VASTDB_SECRET_KEY\")\n",
    "\n",
    "VASTDB_FRAUD_DETECTION_BUCKET = os.getenv(\"VASTDB_FRAUD_DETECTION_BUCKET\")\n",
    "VASTDB_FRAUD_DETECTION_SCHEMA = os.getenv(\"VASTDB_FRAUD_DETECTION_SCHEMA\")\n",
    "VASTDB_FRAUD_DETECTION_TABLE = 'fraud'\n",
    "\n",
    "# Print configurations\n",
    "print(f\"\"\"\n",
    "---\n",
    "DOCKER_HOST_OR_IP={DOCKER_HOST_OR_IP}\n",
    "---\n",
    "VASTDB_ENDPOINT={VASTDB_ENDPOINT}\n",
    "VASTDB_ACCESS_KEY==****{VASTDB_ACCESS_KEY[-4:]}\n",
    "VASTDB_SECRET_KEY=****{VASTDB_SECRET_KEY[-4:]}\n",
    "VASTDB_FRAUD_DETECTION_BUCKET={VASTDB_FRAUD_DETECTION_BUCKET}\n",
    "VASTDB_FRAUD_DETECTION_SCHEMA={VASTDB_FRAUD_DETECTION_SCHEMA}\n",
    "# VASTDB_FRAUD_DETECTION_TABLE={VASTDB_FRAUD_DETECTION_TABLE}\n",
    "---\n",
    "\"\"\")\n",
    "\n",
    "# Kafka Configuration\n",
    "kafka_brokers = f'{DOCKER_HOST_OR_IP}:19092'\n",
    "topic = 'stock-settlement'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7269aa1b-8af2-4200-a15b-7d3f7ac5c43d",
   "metadata": {},
   "source": [
    "Create Vast DB schema if it doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "926f7c72-dabd-4a80-a039-084c0e4bf976",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet vastdb\n",
    "\n",
    "import vastdb\n",
    "\n",
    "session = vastdb.connect(endpoint=VASTDB_ENDPOINT, access=VASTDB_ACCESS_KEY, secret=VASTDB_SECRET_KEY)\n",
    "with session.transaction() as tx:\n",
    "    bucket = tx.bucket(VASTDB_FRAUD_DETECTION_BUCKET)\n",
    "    bucket.schema(VASTDB_FRAUD_DETECTION_SCHEMA, fail_if_missing=False) or bucket.create_schema(VASTDB_FRAUD_DETECTION_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9488ff35-56ab-4f59-8a1f-4de356a5701b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark successfully loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, count\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, BooleanType\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Spark Configuration\n",
    "conf = SparkConf()\n",
    "conf.setAll([\n",
    "    (\"spark.driver.host\", socket.gethostbyname(socket.gethostname())),\n",
    "    (\"spark.sql.execution.arrow.pyspark.enabled\", \"false\"),\n",
    "    # VASTDB\n",
    "    (\"spark.sql.catalog.ndb\", 'spark.sql.catalog.ndb.VastCatalog'),\n",
    "    (\"spark.ndb.endpoint\", VASTDB_ENDPOINT),\n",
    "    (\"spark.ndb.data_endpoints\", VASTDB_ENDPOINT),\n",
    "    (\"spark.ndb.access_key_id\", VASTDB_ACCESS_KEY),\n",
    "    (\"spark.ndb.secret_access_key\", VASTDB_SECRET_KEY),\n",
    "    (\"spark.driver.extraClassPath\", '/usr/local/spark/jars/spark3-vast-3.4.1-f93839bfa38a/*'),\n",
    "    (\"spark.executor.extraClassPath\", '/usr/local/spark/jars/spark3-vast-3.4.1-f93839bfa38a/*'),\n",
    "    (\"spark.sql.extensions\", 'ndb.NDBSparkSessionExtension'),\n",
    "    # Kafka\n",
    "    (\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.4.3,\" \n",
    "                            \"org.apache.logging.log4j:log4j-slf4j2-impl:2.19.0,\" \n",
    "                            \"org.apache.logging.log4j:log4j-api:2.19.0,\" \n",
    "                            \"org.apache.logging.log4j:log4j-core:2.19.0\"),\n",
    "    (\"spark.jars.excludes\", \"org.slf4j:slf4j-api,org.slf4j:slf4j-log4j12\"),\n",
    "    (\"spark.hadoop.fs.file.impl\", \"org.apache.hadoop.fs.RawLocalFileSystem\"),\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"KafkaStreamingToVastDB\") \\\n",
    "    .config(conf=conf) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"DEBUG\")\n",
    "\n",
    "print(\"Spark successfully loaded\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39e771cf-14c2-45e8-8222-6153e5ec3be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'`ndb`.`csnow-db`.`fraud_detection`.`fraud`'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destination_table_name = f\"`ndb`.`{VASTDB_FRAUD_DETECTION_BUCKET}`.`{VASTDB_FRAUD_DETECTION_SCHEMA}`.`{VASTDB_FRAUD_DETECTION_TABLE}`\"\n",
    "destination_table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5642f2ec-902d-4665-864a-606a3606a51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6 (check_row_count):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/conda/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_529/2422378681.py\", line 96, in check_row_count\n",
      "  File \"/usr/local/spark/python/pyspark/sql/dataframe.py\", line 1218, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "  File \"/usr/local/spark/python/pyspark/errors/exceptions/captured.py\", line 169, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o133.collectToPython.\n",
      ": org.apache.spark.SparkException: Job 1 cancelled because SparkContext was shut down\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1217)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1215)\n",
      "\tat scala.collection.mutable.HashSet$Node.foreach(HashSet.scala:435)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:361)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1215)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3016)\n",
      "\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2907)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2907)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2123)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2123)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2076)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:659)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat scala.util.Try$.apply(Try.scala:210)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:976)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2258)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2298)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1022)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:408)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1021)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:360)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:388)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:360)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4038)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4208)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4206)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4206)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4035)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/errors/exceptions/captured.py\", line 169, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "org does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 109\u001b[0m\n\u001b[1;32m    105\u001b[0m row_count_thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Wait for termination\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m \u001b[43mvastdb_query\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:201\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:171\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 171\u001b[0m     converted \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:127\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AnalysisException(origin\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_instance_of(gw, e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.sql.streaming.StreamingQueryException\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStreamingQueryException\u001b[49m\u001b[43m(\u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_instance_of(gw, e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.sql.execution.QueryExecutionException\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m QueryExecutionException(origin\u001b[38;5;241m=\u001b[39me)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:62\u001b[0m, in \u001b[0;36mCapturedException.__init__\u001b[0;34m(self, desc, stackTrace, cause, origin)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesc \u001b[38;5;241m=\u001b[39m desc \u001b[38;5;28;01mif\u001b[39;00m desc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m cast(Py4JJavaError, origin)\u001b[38;5;241m.\u001b[39mgetMessage()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstackTrace \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     stackTrace\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stackTrace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (\u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morg\u001b[49m\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mUtils\u001b[38;5;241m.\u001b[39mexceptionString(origin))\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;241m=\u001b[39m convert_exception(cause) \u001b[38;5;28;01mif\u001b[39;00m cause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin\u001b[38;5;241m.\u001b[39mgetCause() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1725\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1722\u001b[0m _, error_message \u001b[38;5;241m=\u001b[39m get_error_message(answer)\n\u001b[1;32m   1723\u001b[0m message \u001b[38;5;241m=\u001b[39m compute_exception_message(\n\u001b[1;32m   1724\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not exist in the JVM\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name), error_message)\n\u001b[0;32m-> 1725\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(message)\n",
      "\u001b[0;31mPy4JError\u001b[0m: org does not exist in the JVM"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "# Create checkpoint directory with absolute path\n",
    "checkpoint_dir = os.path.abspath(\"/tmp/spark_checkpoint\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Define schema for Kafka message\n",
    "schema = StructType([\n",
    "    StructField(\"partitionID\", LongType(), True),\n",
    "    StructField(\"offset\", LongType(), True),\n",
    "    StructField(\"timestamp\", LongType(), True),\n",
    "    StructField(\"compression\", StringType(), True),\n",
    "    StructField(\"isTransactional\", BooleanType(), True),\n",
    "    StructField(\"key\", StructType([\n",
    "        StructField(\"payload\", StringType(), True),\n",
    "        StructField(\"encoding\", StringType(), True)\n",
    "    ])),\n",
    "    StructField(\"value\", StructType([\n",
    "        StructField(\"payload\", StructType([\n",
    "            StructField(\"transaction_id\", StringType(), True),\n",
    "            StructField(\"settlement_date\", StringType(), True),\n",
    "            StructField(\"stock_symbol\", StringType(), True),\n",
    "            StructField(\"quantity\", LongType(), True),\n",
    "            StructField(\"price\", DoubleType(), True),\n",
    "            StructField(\"buyer\", StringType(), True),\n",
    "            StructField(\"seller\", StringType(), True),\n",
    "            StructField(\"trade_date\", StringType(), True),\n",
    "            StructField(\"status\", StringType(), True)\n",
    "        ])),\n",
    "        StructField(\"encoding\", StringType(), True)\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Read data from Kafka stream\n",
    "raw_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_brokers) \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"failOnDataLoss\", \"true\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse the Kafka message\n",
    "decoded_stream = raw_stream.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "    .select(from_json(col(\"json\"), StructType([\n",
    "        StructField(\"transaction_id\", StringType(), True),\n",
    "        StructField(\"settlement_date\", StringType(), True),\n",
    "        StructField(\"stock_symbol\", StringType(), True),\n",
    "        StructField(\"quantity\", LongType(), True),\n",
    "        StructField(\"price\", DoubleType(), True),\n",
    "        StructField(\"buyer\", StringType(), True),\n",
    "        StructField(\"seller\", StringType(), True),\n",
    "        StructField(\"trade_date\", StringType(), True),\n",
    "        StructField(\"status\", StringType(), True)\n",
    "    ])).alias(\"payload\"))\n",
    "\n",
    "# Prepare data to match VastDB table schema\n",
    "\n",
    "vastdb_stream = decoded_stream.select(\n",
    "    col(\"payload.transaction_id\").alias(\"transaction_id\"),\n",
    "    col(\"payload.settlement_date\").alias(\"settlement_date\"),\n",
    "    col(\"payload.stock_symbol\").alias(\"stock_symbol\"),\n",
    "    col(\"payload.quantity\").alias(\"quantity\"),\n",
    "    col(\"payload.price\").alias(\"price\"),\n",
    "    col(\"payload.buyer\").alias(\"buyer\"),\n",
    "    col(\"payload.seller\").alias(\"seller\"),\n",
    "    col(\"payload.trade_date\").alias(\"trade_date\"),\n",
    "    col(\"payload.status\").alias(\"status\")\n",
    ")\n",
    "\n",
    "total_message_count = 0\n",
    "\n",
    "def process_microbatch(parsed_df, epoch_id):\n",
    "    global total_message_count\n",
    "    batch_size = parsed_df.count()\n",
    "    total_message_count += batch_size\n",
    "    parsed_df.write.mode(\"append\").saveAsTable(destination_table_name)\n",
    "    print(f\"Batch {epoch_id} processed: {batch_size} records written to the database. Total records written: {total_message_count}\")\n",
    "\n",
    "\n",
    "vastdb_stream.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime=\"1 second\") \\\n",
    "    .start()\n",
    "\n",
    "\n",
    "vastdb_query = vastdb_stream.writeStream \\\n",
    "    .foreachBatch(process_microbatch) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime='1 second') \\\n",
    "    .option(\"maxFilesPerTrigger\", 5000) \\\n",
    "    .start()\n",
    "\n",
    "def check_row_count():\n",
    "    while True:\n",
    "        try:\n",
    "            vast_table_row_count = spark.sql(f\"SELECT count(*) FROM {destination_table_name}\").collect()[0][0]\n",
    "            print(f\"Kafka messages consumed (in-memory count): {total_message_count} | Vast Table row count: {vast_table_row_count}\", end=\"\\r\")\n",
    "        except pyspark.errors.exceptions.captured.AnalysisException as e:\n",
    "            print(f\"AnalysisException: {e}. Ensure all tables exist.\")\n",
    "        time.sleep(1)\n",
    "\n",
    "# Start thread for checking row count\n",
    "row_count_thread = threading.Thread(target=check_row_count)\n",
    "row_count_thread.daemon = True\n",
    "row_count_thread.start()\n",
    "\n",
    "\n",
    "# Wait for termination\n",
    "vastdb_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4875e87b-c076-4ec8-a7b3-02806cd05a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
