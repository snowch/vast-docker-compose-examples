{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04107d0c-80c5-4eb1-a900-e15620246512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "DOCKER_HOST_OR_IP=10.143.11.241\n",
      "---\n",
      "VASTDB_ENDPOINT=http://172.200.204.2:80\n",
      "VASTDB_ACCESS_KEY=QXN5\n",
      "VASTDB_SECRET_KEY=****oLGr\n",
      "VASTDB_TWITTER_INGEST_BUCKET=csnow-db\n",
      "VASTDB_TWITTER_INGEST_SCHEMA=social_media\n",
      "VASTDB_TWITTER_INGEST_TABLE=tweets\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Load environment variables for Kafka and VastDB connectivity\n",
    "DOCKER_HOST_OR_IP = os.getenv(\"DOCKER_HOST_OR_IP\", \"localhost\")\n",
    "VASTDB_ENDPOINT = os.getenv(\"VASTDB_ENDPOINT\")\n",
    "VASTDB_ACCESS_KEY = os.getenv(\"VASTDB_ACCESS_KEY\")\n",
    "VASTDB_SECRET_KEY = os.getenv(\"VASTDB_SECRET_KEY\")\n",
    "\n",
    "VASTDB_TWITTER_INGEST_BUCKET = os.getenv(\"VASTDB_TWITTER_INGEST_BUCKET\")\n",
    "VASTDB_TWITTER_INGEST_SCHEMA = os.getenv(\"VASTDB_TWITTER_INGEST_SCHEMA\")\n",
    "VASTDB_TWITTER_INGEST_TABLE = os.getenv(\"VASTDB_TWITTER_INGEST_TABLE\")\n",
    "\n",
    "print(f\"\"\"\n",
    "---\n",
    "DOCKER_HOST_OR_IP={DOCKER_HOST_OR_IP}\n",
    "---\n",
    "VASTDB_ENDPOINT={VASTDB_ENDPOINT}\n",
    "VASTDB_ACCESS_KEY={VASTDB_ACCESS_KEY[-4:]}\n",
    "VASTDB_SECRET_KEY=****{VASTDB_SECRET_KEY[-4:]}\n",
    "VASTDB_TWITTER_INGEST_BUCKET={VASTDB_TWITTER_INGEST_BUCKET}\n",
    "VASTDB_TWITTER_INGEST_SCHEMA={VASTDB_TWITTER_INGEST_SCHEMA}\n",
    "VASTDB_TWITTER_INGEST_TABLE={VASTDB_TWITTER_INGEST_TABLE}\n",
    "---\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31eaf224-4163-44e4-b5f7-e56d141c138c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark successfully loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ## Spark Configuration\n",
    "\n",
    "import socket\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setAll([\n",
    "    (\"spark.driver.host\", socket.gethostbyname(socket.gethostname())),\n",
    "    (\"spark.sql.execution.arrow.pyspark.enabled\", \"false\"),\n",
    "    # VASTDB\n",
    "    (\"spark.sql.catalog.ndb\", 'spark.sql.catalog.ndb.VastCatalog'),\n",
    "    (\"spark.ndb.endpoint\", VASTDB_ENDPOINT),\n",
    "    (\"spark.ndb.data_endpoints\", VASTDB_ENDPOINT),\n",
    "    (\"spark.ndb.access_key_id\", VASTDB_ACCESS_KEY),\n",
    "    (\"spark.ndb.secret_access_key\", VASTDB_SECRET_KEY),\n",
    "    (\"spark.driver.extraClassPath\", '/usr/local/spark/jars/spark3-vast-3.4.1-f93839bfa38a/*'),\n",
    "    (\"spark.executor.extraClassPath\", '/usr/local/spark/jars/spark3-vast-3.4.1-f93839bfa38a/*'),\n",
    "    (\"spark.sql.extensions\", 'ndb.NDBSparkSessionExtension'),\n",
    "    # Kafka\n",
    "    (\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.4.3,\"\n",
    "                           \"org.apache.logging.log4j:log4j-slf4j2-impl:2.19.0,\"\n",
    "                           \"org.apache.logging.log4j:log4j-api:2.19.0,\"\n",
    "                           \"org.apache.logging.log4j:log4j-core:2.19.0\"),\n",
    "    (\"spark.jars.excludes\", \"org.slf4j:slf4j-api,org.slf4j:slf4j-log4j12\"),\n",
    "    (\"spark.hadoop.fs.file.impl\", \"org.apache.hadoop.fs.RawLocalFileSystem\"),\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"KafkaStreamingToVastDB\") \\\n",
    "    .config(conf=conf) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"DEBUG\")\n",
    "\n",
    "print(\"Spark successfully loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "626e0a21-37ef-4905-9959-ec22311bc4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kafka messages consumed: 10225 | Vast Table row count: 2569653\n",
      "Shutdown initiated...\n",
      "\n",
      "Stopping monitoring...\n",
      "Stopping queries...\n",
      "Shutdown complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import threading\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StringType, StructType, StructField, LongType\n",
    "\n",
    "class StreamMonitor:\n",
    "    def __init__(self, spark):\n",
    "        self.spark = spark\n",
    "        self.active = True\n",
    "        self.thread = None\n",
    "        \n",
    "    def monitor_counts(self, query_name, catalog_table_name):\n",
    "        while self.active:\n",
    "            try:\n",
    "                # Wrap counts in a short timeout to allow interruption\n",
    "                df_count = self.spark.sql(f\"SELECT count(*) FROM {query_name}\")\n",
    "                kafka_count = df_count.take(1)[0][0]\n",
    "                \n",
    "                vast_df = self.spark.sql(f\"SELECT count(*) FROM {catalog_table_name}\")\n",
    "                vast_count = vast_df.take(1)[0][0]\n",
    "                \n",
    "                print(f\"Kafka messages consumed: {kafka_count} | Vast Table row count: {vast_count}\", end=\"\\r\")\n",
    "                time.sleep(1)\n",
    "            except Exception:\n",
    "                if self.active:  # Only print error if not shutting down\n",
    "                    print(\"\\nMonitoring interrupted\")\n",
    "                break\n",
    "    \n",
    "    def start(self, query_name, catalog_table_name):\n",
    "        self.thread = threading.Thread(\n",
    "            target=self.monitor_counts,\n",
    "            args=(query_name, catalog_table_name)\n",
    "        )\n",
    "        self.thread.daemon = True\n",
    "        self.thread.start()\n",
    "    \n",
    "    def stop(self):\n",
    "        self.active = False\n",
    "        if self.thread:\n",
    "            self.thread.join(timeout=2)\n",
    "\n",
    "# Configuration\n",
    "kafka_brokers = f'{DOCKER_HOST_OR_IP}:19092'\n",
    "topic = 'streaming-demo-2'\n",
    "checkpoint_dir = os.path.abspath(\"/tmp/spark_checkpoint\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Schema definition\n",
    "schema = StructType([\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"created_at\", LongType(), True),\n",
    "    StructField(\"id\", LongType(), True),\n",
    "    StructField(\"id_str\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define the catalog table name\n",
    "catalog_table_name = f\"`ndb`.`{VASTDB_TWITTER_INGEST_BUCKET}`.`{VASTDB_TWITTER_INGEST_SCHEMA}`.`{VASTDB_TWITTER_INGEST_TABLE}`\"\n",
    "query_name = f\"debug_table_{int(time.time())}\"\n",
    "\n",
    "def process_microbatch(parsed_df, epoch_id):\n",
    "    parsed_df.write.mode(\"append\").saveAsTable(catalog_table_name)\n",
    "\n",
    "# Set up streams\n",
    "raw_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_brokers) \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"failOnDataLoss\", \"true\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse streams\n",
    "decoded_stream = raw_stream.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "    .select(from_json(col(\"json\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "vastdb_stream = decoded_stream.select(\n",
    "    col(\"text\"),\n",
    "    col(\"created_at\"),\n",
    "    col(\"id\"),\n",
    "    col(\"id_str\")\n",
    ")\n",
    "\n",
    "# Initialize monitor\n",
    "monitor = StreamMonitor(spark)\n",
    "\n",
    "try:\n",
    "    # Start the streams\n",
    "    vastdb_query = vastdb_stream.writeStream \\\n",
    "        .foreachBatch(process_microbatch) \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .trigger(processingTime='1 second') \\\n",
    "        .start()\n",
    "        \n",
    "    memory_table_query = decoded_stream.writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"memory\") \\\n",
    "        .queryName(query_name) \\\n",
    "        .start()\n",
    "    \n",
    "    # Start monitoring\n",
    "    monitor.start(query_name, catalog_table_name)\n",
    "    \n",
    "    # Use a simple loop instead of awaitTermination\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "        if not vastdb_query.isActive or not memory_table_query.isActive:\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nShutdown initiated...\")\n",
    "finally:\n",
    "    # Clean up\n",
    "    print(\"\\nStopping monitoring...\")\n",
    "    monitor.stop()\n",
    "    \n",
    "    print(\"Stopping queries...\")\n",
    "    try:\n",
    "        if 'memory_table_query' in locals():\n",
    "            memory_table_query.stop()\n",
    "        if 'vastdb_query' in locals():\n",
    "            vastdb_query.stop()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during shutdown: {e}\")\n",
    "    \n",
    "    print(\"Shutdown complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd11deed-4f3b-4e89-b333-e852c941c4ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
