{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10fed33d-be68-4c1b-a045-30f853f1e6a5",
   "metadata": {},
   "source": [
    "# Bulk Load Yellow Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bb0d111-afe5-4d0b-a5f0-a6fa1a8967d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install --quiet vastdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31dab003-4d9f-47b0-8181-63797260867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from io import StringIO\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "from pyarrow import csv as pa_csv\n",
    "import requests\n",
    "\n",
    "# Custom imports for VASTDB\n",
    "import vastdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1ed0c37-64da-4137-abfa-cdeb705dd1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "VASTDB_ENDPOINT = os.getenv(\"VASTDB_ENDPOINT\")\n",
    "VASTDB_ACCESS_KEY = os.getenv(\"VASTDB_ACCESS_KEY\")\n",
    "VASTDB_SECRET_KEY = os.getenv(\"VASTDB_SECRET_KEY\")\n",
    "\n",
    "VASTDB_TWITTER_INGEST_BUCKET = os.getenv(\"VASTDB_TWITTER_INGEST_BUCKET\")\n",
    "VASTDB_TWITTER_INGEST_SCHEMA = os.getenv(\"VASTDB_TWITTER_INGEST_SCHEMA\")\n",
    "\n",
    "S3_ENDPOINT = os.getenv(\"S3A_ENDPOINT\")\n",
    "S3_ACCESS_KEY = os.getenv(\"S3A_ACCESS_KEY\")\n",
    "S3_SECRET_KEY = os.getenv(\"S3A_SECRET_KEY\")\n",
    "S3_BUCKET = os.getenv(\"S3A_BUCKET\")\n",
    "\n",
    "###### SET THIS ######\n",
    "VASTDB_TWITTER_INGEST_TABLE = 'YELLOW_TRIP_DATA'\n",
    "###### SET THIS ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f40282e3-33b5-4443-a757-adb9c627a873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "VASTDB_ENDPOINT=http://172.200.204.2:80\n",
      "VASTDB_ACCESS_KEY=QXN5\n",
      "VASTDB_SECRET_KEY=****oLGr\n",
      "VASTDB_TWITTER_INGEST_BUCKET=csnow-db\n",
      "VASTDB_TWITTER_INGEST_SCHEMA=social_media\n",
      "VASTDB_TWITTER_INGEST_TABLE=YELLOW_TRIP_DATA\n",
      "---\n",
      "S3_ENDPOINT=http://172.200.204.2:80\n",
      "S3_ACCESS_KEY=QXN5\n",
      "S3_SECRET_KEY=****oLGr\n",
      "S3_BUCKET=csnow-bucket\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "---\n",
    "VASTDB_ENDPOINT={VASTDB_ENDPOINT}\n",
    "VASTDB_ACCESS_KEY={VASTDB_ACCESS_KEY[-4:]}\n",
    "VASTDB_SECRET_KEY=****{VASTDB_SECRET_KEY[-4:]}\n",
    "VASTDB_TWITTER_INGEST_BUCKET={VASTDB_TWITTER_INGEST_BUCKET}\n",
    "VASTDB_TWITTER_INGEST_SCHEMA={VASTDB_TWITTER_INGEST_SCHEMA}\n",
    "VASTDB_TWITTER_INGEST_TABLE={VASTDB_TWITTER_INGEST_TABLE}\n",
    "---\n",
    "S3_ENDPOINT={S3_ENDPOINT}\n",
    "S3_ACCESS_KEY={S3_ACCESS_KEY[-4:]}\n",
    "S3_SECRET_KEY=****{VASTDB_SECRET_KEY[-4:]}\n",
    "S3_BUCKET={S3_BUCKET}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "954ed1e1-b4ae-4522-b6b8-5bd1e5f67283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet(file_path):\n",
    "    \"\"\"Reads Parquet data from a file.\"\"\"\n",
    "    try:\n",
    "        return pq.read_table(file_path)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error reading Parquet file: {e}\") from e\n",
    "\n",
    "def connect_to_vastdb(endpoint, access_key, secret_key):\n",
    "    \"\"\"Connects to VastDB.\"\"\"\n",
    "    try:\n",
    "        session = vastdb.connect(endpoint=endpoint, access=access_key, secret=secret_key)\n",
    "        print(\"Connected to VastDB\")\n",
    "        return session\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to connect to VastDB: {e}\") from e\n",
    "\n",
    "def write_to_vastdb(session, bucket_name, schema_name, table_name, pa_table):\n",
    "    \"\"\"Writes data to VastDB.\"\"\"\n",
    "    with session.transaction() as tx:\n",
    "        bucket = tx.bucket(bucket_name)\n",
    "        schema = bucket.schema(schema_name, fail_if_missing=False) or bucket.create_schema(schema_name)\n",
    "\n",
    "        table = schema.table(table_name, fail_if_missing=False) or schema.create_table(table_name, pa_table.schema)\n",
    "\n",
    "        columns_to_add = get_columns_to_add(table.arrow_schema, pa_table.schema)\n",
    "        for column in columns_to_add:\n",
    "            table.add_column(column)\n",
    "\n",
    "        table.insert(pa_table)\n",
    "        print(f\"parquet file written to VastdB\")\n",
    "\n",
    "def import_to_vastdb(session, bucket_name, schema_name, table_name, files_to_import):\n",
    "    with session.transaction() as tx:\n",
    "        bucket = tx.bucket(bucket_name)\n",
    "        schema = bucket.schema(schema_name, fail_if_missing=False) or bucket.create_schema(schema_name)\n",
    "        table = schema.table(table_name, fail_if_missing=False)\n",
    "\n",
    "        if table:\n",
    "            table.import_files(files_to_import=files_to_import)\n",
    "        else:\n",
    "            table = vastdb.util.create_table_from_files(\n",
    "                schema=schema, \n",
    "                table_name=table_name,\n",
    "                parquet_files=files_to_import\n",
    "            )\n",
    "\n",
    "def get_columns_to_add(existing_schema, desired_schema):\n",
    "    \"\"\"Identifies columns to add to an existing schema.\"\"\"\n",
    "    existing_fields = set(existing_schema.names)\n",
    "    desired_fields = set(desired_schema.names)\n",
    "    return [pa.schema([pa.field(name, desired_schema.field(name).type)]) for name in desired_fields - existing_fields]\n",
    "\n",
    "\n",
    "def query_vastdb(session, bucket_name, schema_name, table_name):\n",
    "    \"\"\"Writes data to VastDB.\"\"\"\n",
    "    with session.transaction() as tx:\n",
    "        bucket = tx.bucket(bucket_name)\n",
    "        schema = bucket.schema(schema_name, fail_if_missing=False) or bucket.create_schema(schema_name)\n",
    "        table = schema.table(table_name, fail_if_missing=False) or schema.create_table(table_name, pa_table.schema)\n",
    "\n",
    "        return table.select().read_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01f14638-9819-4632-a87e-8a7ba5a9b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to download files\n",
    "def download_file(url):\n",
    "    file_name = os.path.basename(urlparse(url).path)\n",
    "    \n",
    "    # Check if file exists, skip if so\n",
    "    if not os.path.exists(file_name):\n",
    "        # print(f\"Downloading {file_name}...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            with open(file_name, \"wb\") as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            print(f\"Downloaded {file_name}\")\n",
    "        else:\n",
    "            print(f\"Failed to download {file_name}. Status code: {response.status_code}\")\n",
    "    else:\n",
    "        print(f\"{file_name} already exists. Skipping download.\")\n",
    "\n",
    "# Define the function to process and transform the data\n",
    "def process_parquet(file_path):\n",
    "    # Load the Parquet file\n",
    "    pa_table = pq.read_table(file_path)\n",
    "    \n",
    "    # Convert TIMESTAMP[US] to a standard timestamp (nanoseconds)\n",
    "    columns_to_convert = ['tpep_pickup_datetime', 'tpep_dropoff_datetime']\n",
    "    for column in columns_to_convert:\n",
    "        if column in pa_table.column_names:\n",
    "            # Convert the column to a timestamp with nanosecond precision\n",
    "            pa_table = pa_table.set_column(\n",
    "                pa_table.column_names.index(column),\n",
    "                column,\n",
    "                pc.cast(pa_table[column], pa.timestamp('ns'))\n",
    "            )\n",
    "    \n",
    "    # Handle NULL columns (e.g., 'airport_fee')\n",
    "    null_column = 'airport_fee'\n",
    "    if null_column in pa_table.column_names:\n",
    "        # Replace NULL values with 0.0 (float type)\n",
    "        pa_table = pa_table.set_column(\n",
    "            pa_table.column_names.index(null_column),\n",
    "            null_column,\n",
    "            pc.if_else(pc.is_null(pa_table[null_column]), pa.scalar(0.0, pa.float64()), pa_table[null_column])\n",
    "        )\n",
    "    \n",
    "    # Verify the conversion\n",
    "    # print(f\"Processed schema for {file_path}:\")\n",
    "    # print(pa_table.schema)\n",
    "    \n",
    "    return pa_table\n",
    "\n",
    "# Define a function to upload the file to S3\n",
    "def upload_to_s3(file_path, bucket_name, s3_key):\n",
    "    try:\n",
    "        # print(f\"Uploading {file_path} to S3 bucket {bucket_name}...\")\n",
    "        s3_client.upload_file(file_path, bucket_name, s3_key)\n",
    "        print(f\"File uploaded to s3://{bucket_name}/{s3_key}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {file_path} was not found.\")\n",
    "    except NoCredentialsError:\n",
    "        print(\"Credentials not available.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading file to S3: {e}\")\n",
    "\n",
    "# Define a function to delete the file after processing\n",
    "def delete_file(file_path):\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "        print(f\"Deleted {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d960a6e-c4ea-45c0-a0ae-04e5a58cd439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to VastDB\n"
     ]
    }
   ],
   "source": [
    "session = connect_to_vastdb(VASTDB_ENDPOINT, VASTDB_ACCESS_KEY, VASTDB_SECRET_KEY)\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client(\n",
    "    's3', \n",
    "    region_name='vast',\n",
    "    endpoint_url=S3_ENDPOINT,\n",
    "    aws_access_key_id=S3_ACCESS_KEY,\n",
    "    aws_secret_access_key=S3_SECRET_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb66cf8-0074-454f-980b-f757bc44954c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded yellow_tripdata_2019-01.parquet\n",
      "File uploaded to s3://csnow-bucket/yellow_tripdata/yellow_tripdata_2019-01.parquet\n",
      "parquet file written to VastdB\n",
      "Deleted yellow_tripdata_2019-01.parquet\n",
      "Downloaded yellow_tripdata_2019-02.parquet\n",
      "File uploaded to s3://csnow-bucket/yellow_tripdata/yellow_tripdata_2019-02.parquet\n",
      "parquet file written to VastdB\n",
      "Deleted yellow_tripdata_2019-02.parquet\n",
      "Downloaded yellow_tripdata_2019-03.parquet\n",
      "File uploaded to s3://csnow-bucket/yellow_tripdata/yellow_tripdata_2019-03.parquet\n",
      "parquet file written to VastdB\n",
      "Deleted yellow_tripdata_2019-03.parquet\n",
      "Downloaded yellow_tripdata_2019-04.parquet\n",
      "File uploaded to s3://csnow-bucket/yellow_tripdata/yellow_tripdata_2019-04.parquet\n"
     ]
    }
   ],
   "source": [
    "# Download and process all files from 2019-01 to 2024-08\n",
    "for year in range(2019, 2025):\n",
    "    for month in range(1, 13):\n",
    "        if year == 2024 and month > 8:\n",
    "            break  # Only process until August 2024\n",
    "        \n",
    "        # Format month to always have two digits\n",
    "        month_str = f\"{month:02d}\"\n",
    "        file_url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year}-{month_str}.parquet\"\n",
    "        \n",
    "        # Download the file\n",
    "        download_file(file_url)\n",
    "        \n",
    "        # Process the downloaded file\n",
    "        file_name = f\"yellow_tripdata_{year}-{month_str}.parquet\"\n",
    "        pa_table = process_parquet(file_name)\n",
    "\n",
    "        # Upload the file to S3\n",
    "        s3_key = f\"yellow_tripdata/yellow_tripdata_{year}-{month_str}.parquet\"\n",
    "        upload_to_s3(file_name, S3_BUCKET, s3_key)\n",
    "        \n",
    "        # Write the processed data to VASTDB (custom logic)\n",
    "        write_to_vastdb(session=session,\n",
    "                        bucket_name=VASTDB_TWITTER_INGEST_BUCKET, \n",
    "                        schema_name=VASTDB_TWITTER_INGEST_SCHEMA, \n",
    "                        table_name=VASTDB_TWITTER_INGEST_TABLE, \n",
    "                        pa_table=pa_table)\n",
    "\n",
    "        # import_to_vastdb(\n",
    "        #     session=session,\n",
    "        #     bucket_name=VASTDB_TWITTER_INGEST_BUCKET, \n",
    "        #     schema_name=VASTDB_TWITTER_INGEST_SCHEMA, \n",
    "        #     table_name=VASTDB_TWITTER_INGEST_TABLE, \n",
    "        #     files_to_import=[f\"/{s3_key}\"]\n",
    "        # )\n",
    "        \n",
    "        # Delete the file after processing\n",
    "        delete_file(file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8088cb7-8aec-4907-abfd-9abc7dd4072c",
   "metadata": {},
   "source": [
    "## Check Parquet file for non-compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0434a133-5c19-4829-a50a-e7c6a22054ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install --upgrade --quiet git+https://github.com/snowch/vastdb_parq_schema_file.git --use-pep517\n",
    "# ! parquet_checker yellow_tripdata_2019-01.parquet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
