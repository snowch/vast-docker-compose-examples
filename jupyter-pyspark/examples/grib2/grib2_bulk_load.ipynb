{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03f2dbe4-d11c-4f13-8a2a-13c6e8cb7da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --quiet cfgrib xarray pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "685c672f-a5da-48f1-b4d6-d038981a5c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = 'http://172.200.202.2:80'\n",
    "access_key = 'O6UZCRYPS9OPWUD1DYF8'\n",
    "secret_key = 'HnF34+7JkYFusmbU++Cv0/YLUiEpWGBRZTjmUsuu'\n",
    "bucket_name='csnowdb'\n",
    "schema_name='grib2'\n",
    "table_name='grib2_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec21981-59f1-479e-8b8d-b32385e078a3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import requests\n",
    "import tempfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Data processing imports\n",
    "import cfgrib\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import vastdb\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class GRIB2DataIngester:\n",
    "    \"\"\"GRIB2 Data Discovery and Bulk Ingestion System for VastDB\"\"\"\n",
    "\n",
    "    def __init__(self, vastdb_config: Dict[str, str], temp_dir: Optional[str] = None):\n",
    "        self.vastdb_config = vastdb_config\n",
    "        self.temp_dir = Path(temp_dir) if temp_dir else Path(tempfile.mkdtemp())\n",
    "        self.temp_dir.mkdir(exist_ok=True)\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({'User-Agent': 'GRIB2-Bulk-Ingester/1.0'})\n",
    "        self.vastdb_session = None\n",
    "\n",
    "        # Statistics tracking\n",
    "        self.stats = {\n",
    "            'files_discovered': 0,\n",
    "            'files_downloaded': 0,\n",
    "            'files_processed': 0,\n",
    "            'files_ingested': 0,\n",
    "            'bytes_downloaded': 0,\n",
    "            'bytes_processed': 0,\n",
    "            'start_time': None,\n",
    "            'errors': []\n",
    "        }\n",
    "\n",
    "    def connect_to_vastdb(self):\n",
    "        \"\"\"Connect to VastDB\"\"\"\n",
    "        try:\n",
    "            self.vastdb_session = vastdb.connect(\n",
    "                endpoint=self.vastdb_config['endpoint'],\n",
    "                access=self.vastdb_config['access_key'],\n",
    "                secret=self.vastdb_config['secret_key']\n",
    "            )\n",
    "            logger.info(\"✓ Connected to VastDB\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"✗ Failed to connect to VastDB: {e}\")\n",
    "            return False\n",
    "\n",
    "    def discover_grib2_sources(self) -> List[Dict]:\n",
    "        \"\"\"Discover available GRIB2 data sources\"\"\"\n",
    "        sources = [\n",
    "            {\n",
    "                'name': 'GEM Global 66km',\n",
    "                'url': 'https://dd.weather.gc.ca/model_gem_global/66km/grib2/lat_lon',\n",
    "                'description': 'Lower resolution global model (more stable)',\n",
    "                'estimated_file_size_mb': 0.8\n",
    "            },\n",
    "            {\n",
    "                'name': 'GEM Global 15km',\n",
    "                'url': 'https://dd.weather.gc.ca/model_gem_global/15km/grib2/lat_lon',\n",
    "                'description': 'High resolution global model',\n",
    "                'estimated_file_size_mb': 2.5\n",
    "            },\n",
    "            {\n",
    "                'name': 'HRDPS Continental 2.5km',\n",
    "                'url': 'https://dd.weather.gc.ca/model_hrdps/continental/2.5km',\n",
    "                'description': 'Very high resolution continental model',\n",
    "                'estimated_file_size_mb': 8.0\n",
    "            },\n",
    "            {\n",
    "                'name': 'Sample GRIB2 Files',\n",
    "                'url': 'https://dd.weather.gc.ca/model_gem_global/25km/grib2/lat_lon',\n",
    "                'description': 'Alternative global model data',\n",
    "                'estimated_file_size_mb': 1.5\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        logger.info(\"🔍 Discovering available GRIB2 data sources...\")\n",
    "        available_sources = []\n",
    "\n",
    "        for source in sources:\n",
    "            try:\n",
    "                logger.info(f\"Checking {source['name']}...\")\n",
    "\n",
    "                # More lenient check - just see if we can access the URL\n",
    "                response = self.session.get(source['url'], timeout=15)\n",
    "                if response.status_code == 200:\n",
    "                    # Quick check for actual files\n",
    "                    file_count = self._quick_file_count(source['url'])\n",
    "\n",
    "                    # Be more generous - even 1 file is enough\n",
    "                    if file_count >= 1:\n",
    "                        source['available'] = True\n",
    "                        source['estimated_files'] = max(file_count, 10)  # Minimum assumption\n",
    "                        source['estimated_total_size_gb'] = (source['estimated_files'] * source['estimated_file_size_mb']) / 1024\n",
    "                        available_sources.append(source)\n",
    "                        logger.info(f\"✓ {source['name']}: ~{source['estimated_files']} files (~{source['estimated_total_size_gb']:.1f} GB)\")\n",
    "                    else:\n",
    "                        # Even if no files found, add with minimal estimate\n",
    "                        source['available'] = True\n",
    "                        source['estimated_files'] = 5\n",
    "                        source['estimated_total_size_gb'] = (5 * source['estimated_file_size_mb']) / 1024\n",
    "                        available_sources.append(source)\n",
    "                        logger.info(f\"⚠️ {source['name']}: Directory accessible, assuming ~5 files\")\n",
    "                else:\n",
    "                    logger.info(f\"✗ {source['name']}: HTTP {response.status_code}\")\n",
    "            except Exception as e:\n",
    "                logger.info(f\"✗ {source['name']}: Error - {e}\")\n",
    "\n",
    "        # Fallback: If no sources found, create a test source with known working file\n",
    "        if not available_sources:\n",
    "            logger.info(\"No sources found via directory browsing, trying direct file access...\")\n",
    "            fallback_source = {\n",
    "                'name': 'Direct GRIB2 File',\n",
    "                'url': 'https://dd.weather.gc.ca/model_gem_global/15km/grib2/lat_lon',\n",
    "                'description': 'Direct file access method',\n",
    "                'estimated_file_size_mb': 2.5,\n",
    "                'available': True,\n",
    "                'estimated_files': 1,\n",
    "                'estimated_total_size_gb': 0.0025,\n",
    "                'direct_files': [\n",
    "                    'https://dd.weather.gc.ca/model_gem_global/15km/grib2/lat_lon/00/000/CMC_glb_ABSV_ISBL_200_latlon.15x.15_2024121100_P000.grib2'\n",
    "                ]\n",
    "            }\n",
    "\n",
    "            # Test if we can access the known file\n",
    "            try:\n",
    "                test_response = self.session.head(fallback_source['direct_files'][0], timeout=10)\n",
    "                if test_response.status_code == 200:\n",
    "                    available_sources.append(fallback_source)\n",
    "                    logger.info(\"✓ Found working direct file access\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return available_sources\n",
    "\n",
    "    def _quick_file_count(self, base_url: str, max_dirs: int = 50) -> int:\n",
    "        \"\"\"Quick estimation of available files\"\"\"\n",
    "        try:\n",
    "            # Check a few directories to estimate total files\n",
    "            dirs = self._get_directories(base_url)\n",
    "            total_files = 0\n",
    "\n",
    "            for i, dir_name in enumerate(dirs[:max_dirs]):\n",
    "                dir_url = f\"{base_url}/{dir_name}/\"\n",
    "                subdirs = self._get_directories(dir_url)\n",
    "\n",
    "                for j, subdir in enumerate(subdirs[:10]):  # Check first 10 subdirs\n",
    "                    subdir_url = f\"{dir_url}{subdir}/\"\n",
    "                    files = self._get_grib2_files(subdir_url, limit=100)\n",
    "                    total_files += len(files)\n",
    "\n",
    "                    if total_files > 500:  # Cap estimation\n",
    "                        return min(total_files * (len(dirs) * len(subdirs)) // ((i+1) * (j+1)), 10000)\n",
    "\n",
    "            # Extrapolate\n",
    "            if total_files > 0 and len(dirs) > max_dirs:\n",
    "                total_files = total_files * len(dirs) // max_dirs\n",
    "\n",
    "            #return min(total_files, 10000)  # Cap at reasonable number\n",
    "            return total_files\n",
    "\n",
    "        except Exception:\n",
    "            return 0\n",
    "\n",
    "    def _get_directories(self, url: str) -> List[str]:\n",
    "        \"\"\"Get directory listings\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            dirs = []\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if href.endswith('/') and not href.startswith('..') and href != '/':\n",
    "                    dirs.append(href.rstrip('/'))\n",
    "            return dirs\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    def _get_grib2_files(self, url: str, limit: int = 10000) -> List[str]:\n",
    "        \"\"\"Get GRIB2 file listings\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            files = []\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if href.endswith('.grib2'):\n",
    "                    files.append(href)\n",
    "                    if len(files) >= limit:\n",
    "                        break\n",
    "            return files\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    def build_download_plan(self, sources: List[Dict], target_size_gb: float) -> Dict:\n",
    "        \"\"\"Build a download plan to reach target data size\"\"\"\n",
    "        logger.info(f\"📋 Building download plan for {target_size_gb} GB of data...\")\n",
    "\n",
    "        plan = {\n",
    "            'target_size_gb': target_size_gb,\n",
    "            'sources': [],\n",
    "            'total_estimated_size_gb': 0,\n",
    "            'total_estimated_files': 0\n",
    "        }\n",
    "\n",
    "        # Sort sources by file size (prefer smaller files for more diversity)\n",
    "        sources.sort(key=lambda x: x['estimated_file_size_mb'])\n",
    "\n",
    "        remaining_gb = target_size_gb\n",
    "\n",
    "        for source in sources:\n",
    "            if remaining_gb <= 0:\n",
    "                break\n",
    "\n",
    "            # Calculate how many files we need from this source\n",
    "            files_needed = min(\n",
    "                int((remaining_gb * 1024) / source['estimated_file_size_mb']),\n",
    "                source['estimated_files'],\n",
    "                # 2000  # Cap per source\n",
    "            )\n",
    "\n",
    "            if files_needed > 0:\n",
    "                size_from_source = (files_needed * source['estimated_file_size_mb']) / 1024\n",
    "\n",
    "                plan['sources'].append({\n",
    "                    'name': source['name'],\n",
    "                    'url': source['url'],\n",
    "                    'files_to_download': files_needed,\n",
    "                    'estimated_size_gb': size_from_source,\n",
    "                    'estimated_file_size_mb': source['estimated_file_size_mb'],\n",
    "                    'direct_files': source.get('direct_files', None)\n",
    "                })\n",
    "\n",
    "                plan['total_estimated_files'] += files_needed\n",
    "                plan['total_estimated_size_gb'] += size_from_source\n",
    "                remaining_gb -= size_from_source\n",
    "\n",
    "        logger.info(f\"📊 Plan: {plan['total_estimated_files']} files, {plan['total_estimated_size_gb']:.1f} GB from {len(plan['sources'])} sources\")\n",
    "        return plan\n",
    "\n",
    "    def discover_files_from_source(self, source_url: str, max_files: int, direct_files: List[str] = None) -> List[str]:\n",
    "        \"\"\"Discover actual file URLs from a source\"\"\"\n",
    "        logger.info(f\"🔍 Discovering files from {source_url}...\")\n",
    "\n",
    "        # If we have direct files (fallback mode), use those\n",
    "        if direct_files:\n",
    "            logger.info(f\"Using direct file list: {len(direct_files)} files\")\n",
    "            return direct_files[:max_files]\n",
    "\n",
    "        file_urls = []\n",
    "        dirs = self._get_directories(source_url)\n",
    "\n",
    "        # If no directories found, try a more aggressive approach\n",
    "        if not dirs:\n",
    "            logger.info(\"No directories found, trying current date directories...\")\n",
    "            # Try recent dates\n",
    "            today = datetime.now()\n",
    "            for days_back in range(30):  # Try last 30 days\n",
    "                test_date = today - timedelta(days=days_back)\n",
    "                for hour in ['00', '06', '12', '18']:\n",
    "                    test_dir = f\"{test_date.strftime('%Y%m%d')}{hour}\"\n",
    "                    if test_dir not in dirs:\n",
    "                        dirs.append(hour)  # Just try the hour directories\n",
    "\n",
    "        for dir_name in dirs:\n",
    "            if len(file_urls) >= max_files:\n",
    "                break\n",
    "\n",
    "            dir_url = f\"{source_url}/{dir_name}/\"\n",
    "            subdirs = self._get_directories(dir_url)\n",
    "\n",
    "            # If no subdirs, try common forecast hours\n",
    "            if not subdirs:\n",
    "                subdirs = ['000', '003', '006', '012', '024', '048']\n",
    "\n",
    "            for subdir in subdirs:\n",
    "                if len(file_urls) >= max_files:\n",
    "                    break\n",
    "\n",
    "                subdir_url = f\"{dir_url}{subdir}/\"\n",
    "                #files = self._get_grib2_files(subdir_url, limit=max_files - len(file_urls))\n",
    "                files = self._get_grib2_files(subdir_url, limit=10000)\n",
    "\n",
    "                for file in files:\n",
    "                    file_urls.append(f\"{subdir_url}{file}\")\n",
    "                    if len(file_urls) >= max_files:\n",
    "                        break\n",
    "\n",
    "        logger.info(f\"📁 Discovered {len(file_urls)} file URLs\")\n",
    "        self.stats['files_discovered'] += len(file_urls)\n",
    "        return file_urls\n",
    "\n",
    "    def download_file(self, url: str, batch_id: str = None) -> Tuple[Optional[str], Optional[Dict]]:\n",
    "        \"\"\"Download a single GRIB2 file and return metadata\"\"\"\n",
    "        filename = Path(url).name\n",
    "        local_path = self.temp_dir / filename\n",
    "        download_start = datetime.utcnow()\n",
    "\n",
    "        try:\n",
    "            response = self.session.get(url, stream=True, timeout=120)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            with open(local_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "\n",
    "            download_end = datetime.utcnow()\n",
    "            file_size = local_path.stat().st_size\n",
    "            download_duration = (download_end - download_start).total_seconds()\n",
    "\n",
    "            self.stats['bytes_downloaded'] += file_size\n",
    "            self.stats['files_downloaded'] += 1\n",
    "\n",
    "            # Prepare download metadata\n",
    "            metadata = {\n",
    "                'source_url': url,\n",
    "                'filename': filename,\n",
    "                'file_size_bytes': file_size,\n",
    "                'download_timestamp': download_start,\n",
    "                'download_duration_seconds': download_duration,\n",
    "                'batch_id': batch_id or f\"batch_{int(datetime.utcnow().timestamp())}\"\n",
    "            }\n",
    "\n",
    "            logger.debug(f\"Downloaded {filename} ({file_size / 1024 / 1024:.1f} MB)\")\n",
    "            return str(local_path), metadata\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to download {url}: {e}\")\n",
    "            self.stats['errors'].append(f\"Download failed: {url} - {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def process_grib2_to_parquet(self, grib2_path: str, source_url: str = None, source_name: str = None) -> Tuple[Optional[str], Optional[Dict]]:\n",
    "        \"\"\"Process GRIB2 file to VastDB-compatible Parquet and return metadata\"\"\"\n",
    "        processing_start = datetime.utcnow()\n",
    "\n",
    "        try:\n",
    "            # Read GRIB2 file with explicit decode_timedelta to silence warning\n",
    "            ds = xr.open_dataset(grib2_path, engine=\"cfgrib\",\n",
    "                               backend_kwargs={\n",
    "                                   'decode_timedelta': False,\n",
    "                                   'decode_times': True\n",
    "                               })\n",
    "\n",
    "            # Extract filename hour information\n",
    "            filename = Path(grib2_path).name\n",
    "            extracted_hour = self._extract_hour_from_filename(filename)\n",
    "            logger.debug(f\"Processing {filename}, extracted hour: {extracted_hour}\")\n",
    "\n",
    "            # Log original coordinates for debugging\n",
    "            logger.debug(f\"Original dataset coordinates: {list(ds.coords.keys())}\")\n",
    "            for coord_name in ['time', 'valid_time', 'step']:\n",
    "                if coord_name in ds.coords:\n",
    "                    coord_value = ds.coords[coord_name]\n",
    "                    logger.debug(f\"Original {coord_name}: {coord_value.values}\")\n",
    "\n",
    "            # Convert to DataFrame\n",
    "            df = ds.to_dataframe().reset_index()\n",
    "            logger.debug(f\"DataFrame shape: {df.shape}\")\n",
    "            logger.debug(f\"DataFrame columns: {list(df.columns)}\")\n",
    "\n",
    "            # Now fix datetime columns in the DataFrame directly\n",
    "            datetime_columns = []\n",
    "\n",
    "            # Handle 'time' column\n",
    "            if 'time' in df.columns and extracted_hour is not None:\n",
    "                logger.debug(f\"Processing 'time' column, dtype: {df['time'].dtype}\")\n",
    "\n",
    "                if df['time'].dtype.kind == 'M':  # datetime64 type\n",
    "                    # Check a sample of times\n",
    "                    sample_times = df['time'].dropna().head(3)\n",
    "                    logger.debug(f\"Sample original times: {sample_times.tolist()}\")\n",
    "\n",
    "                    # Convert to pandas datetime for easier manipulation\n",
    "                    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "                    # Check if all times are at midnight (missing hour component)\n",
    "                    unique_hours = df['time'].dt.hour.unique()\n",
    "                    unique_minutes = df['time'].dt.minute.unique()\n",
    "                    unique_seconds = df['time'].dt.second.unique()\n",
    "\n",
    "                    logger.debug(f\"Time unique hours: {unique_hours}, minutes: {unique_minutes}, seconds: {unique_seconds}\")\n",
    "\n",
    "                    is_midnight_only = (\n",
    "                        len(unique_hours) == 1 and unique_hours[0] == 0 and\n",
    "                        len(unique_minutes) == 1 and unique_minutes[0] == 0 and\n",
    "                        len(unique_seconds) == 1 and unique_seconds[0] == 0\n",
    "                    )\n",
    "\n",
    "                    if is_midnight_only:\n",
    "                        logger.debug(f\"Reconstructing 'time' column with hour {extracted_hour:02d}\")\n",
    "                        df['time'] = df['time'] + pd.Timedelta(hours=extracted_hour)\n",
    "                        logger.debug(f\"Sample reconstructed times: {df['time'].head(3).tolist()}\")\n",
    "\n",
    "                    datetime_columns.append('time')\n",
    "\n",
    "            # Handle 'valid_time' column\n",
    "            if 'valid_time' in df.columns and extracted_hour is not None:\n",
    "                logger.debug(f\"Processing 'valid_time' column, dtype: {df['valid_time'].dtype}\")\n",
    "\n",
    "                if df['valid_time'].dtype.kind == 'M':  # datetime64 type\n",
    "                    # Check a sample of valid times\n",
    "                    sample_valid_times = df['valid_time'].dropna().head(3)\n",
    "                    logger.debug(f\"Sample original valid_times: {sample_valid_times.tolist()}\")\n",
    "\n",
    "                    # Convert to pandas datetime for easier manipulation\n",
    "                    df['valid_time'] = pd.to_datetime(df['valid_time'])\n",
    "\n",
    "                    # Check if all valid_times are at midnight\n",
    "                    unique_hours = df['valid_time'].dt.hour.unique()\n",
    "                    unique_minutes = df['valid_time'].dt.minute.unique()\n",
    "                    unique_seconds = df['valid_time'].dt.second.unique()\n",
    "\n",
    "                    logger.debug(f\"Valid_time unique hours: {unique_hours}, minutes: {unique_minutes}, seconds: {unique_seconds}\")\n",
    "\n",
    "                    is_midnight_only = (\n",
    "                        len(unique_hours) == 1 and unique_hours[0] == 0 and\n",
    "                        len(unique_minutes) == 1 and unique_minutes[0] == 0 and\n",
    "                        len(unique_seconds) == 1 and unique_seconds[0] == 0\n",
    "                    )\n",
    "\n",
    "                    if is_midnight_only:\n",
    "                        # For valid_time, also consider the step (forecast lead time)\n",
    "                        step_hours = 0\n",
    "                        if 'step' in df.columns:\n",
    "                            # Step might be in hours (float) or timedelta format\n",
    "                            if df['step'].dtype.kind == 'f':  # float (already in hours)\n",
    "                                step_hours = df['step'].iloc[0] if len(df) > 0 else 0\n",
    "                                logger.debug(f\"Step is float, using step_hours: {step_hours}\")\n",
    "                            elif df['step'].dtype.kind == 'm':  # timedelta\n",
    "                                step_td = pd.to_timedelta(df['step'].iloc[0]) if len(df) > 0 else pd.Timedelta(0)\n",
    "                                step_hours = step_td.total_seconds() / 3600\n",
    "                                logger.debug(f\"Step is timedelta, converted to hours: {step_hours}\")\n",
    "                            else:\n",
    "                                logger.debug(f\"Step column has unexpected dtype: {df['step'].dtype}\")\n",
    "\n",
    "                        total_hours = extracted_hour + step_hours\n",
    "                        logger.debug(f\"Reconstructing 'valid_time' with total hours: {total_hours} (base: {extracted_hour}, step: {step_hours})\")\n",
    "                        df['valid_time'] = df['valid_time'] + pd.Timedelta(hours=total_hours)\n",
    "                        logger.debug(f\"Sample reconstructed valid_times: {df['valid_time'].head(3).tolist()}\")\n",
    "\n",
    "                    datetime_columns.append('valid_time')\n",
    "\n",
    "            # Handle other potential datetime columns\n",
    "            for col in df.columns:\n",
    "                if col not in ['time', 'valid_time'] and df[col].dtype.kind == 'M':\n",
    "                    datetime_columns.append(col)\n",
    "                    logger.debug(f\"Found additional datetime column: {col}\")\n",
    "\n",
    "            # Ensure all datetime columns are timezone-naive for VastDB compatibility\n",
    "            for col in datetime_columns:\n",
    "                if col in df.columns:\n",
    "                    if hasattr(df[col].dtype, 'tz') and df[col].dtype.tz is not None:\n",
    "                        df[col] = df[col].dt.tz_localize(None)\n",
    "                        logger.debug(f\"Removed timezone from column: {col}\")\n",
    "\n",
    "            logger.debug(f\"Final datetime columns processed: {datetime_columns}\")\n",
    "\n",
    "            # Extract GRIB2 variables and attributes for metadata\n",
    "            grib2_variables = list(ds.data_vars.keys()) if hasattr(ds, 'data_vars') else []\n",
    "            grib2_attrs = dict(ds.attrs) if hasattr(ds, 'attrs') else {}\n",
    "\n",
    "            # Add metadata columns\n",
    "            df['source_file'] = Path(grib2_path).name\n",
    "            df['ingestion_timestamp'] = datetime.utcnow()\n",
    "\n",
    "            # Log final sample of key datetime columns\n",
    "            for col in ['time', 'valid_time']:\n",
    "                if col in df.columns:\n",
    "                    sample_final = df[col].head(2).tolist()\n",
    "                    logger.debug(f\"Final {col} sample: {sample_final}\")\n",
    "\n",
    "            # Convert to PyArrow table with explicit datetime preservation\n",
    "            table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "\n",
    "            # Fix VastDB incompatible types while preserving datetime precision\n",
    "            table = self._fix_vastdb_compatibility(table, preserve_datetimes=True)\n",
    "\n",
    "            # Save to parquet with datetime preservation\n",
    "            parquet_path = grib2_path.replace('.grib2', '.parquet')\n",
    "            pq.write_table(table, parquet_path,\n",
    "                          write_statistics=True,\n",
    "                          compression='snappy')\n",
    "\n",
    "            file_size = Path(parquet_path).stat().st_size\n",
    "            self.stats['bytes_processed'] += file_size\n",
    "            self.stats['files_processed'] += 1\n",
    "\n",
    "            processing_end = datetime.utcnow()\n",
    "            processing_duration = (processing_end - processing_start).total_seconds()\n",
    "\n",
    "            # Prepare metadata\n",
    "            metadata = {\n",
    "                'filename': Path(grib2_path).name,\n",
    "                'source_url': source_url or '',\n",
    "                'source_name': source_name or '',\n",
    "                'file_size_bytes': Path(grib2_path).stat().st_size,\n",
    "                'processing_timestamp': processing_start,\n",
    "                'processing_duration_seconds': processing_duration,\n",
    "                'grib2_variables': str(grib2_variables),\n",
    "                'grib2_attributes': str(grib2_attrs),\n",
    "                'record_count': len(df),\n",
    "                'datetime_columns_handled': str(datetime_columns),\n",
    "                'extracted_hour': extracted_hour\n",
    "            }\n",
    "\n",
    "            logger.debug(f\"Processed {Path(grib2_path).name} to Parquet ({file_size / 1024 / 1024:.1f} MB)\")\n",
    "            return parquet_path, metadata\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process {grib2_path}: {e}\")\n",
    "            self.stats['errors'].append(f\"Processing failed: {grib2_path} - {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def _extract_hour_from_filename(self, filename: str) -> Optional[int]:\n",
    "        \"\"\"Extract hour information from GRIB2 filename patterns\"\"\"\n",
    "        try:\n",
    "            # Common GRIB2 filename patterns:\n",
    "            # CMC_glb_ABSV_ISBL_200_latlon.15x.15_2025060400_P000.grib2\n",
    "            # Pattern: YYYYMMDDHH where HH is the hour\n",
    "\n",
    "            logger.debug(f\"Attempting to extract hour from filename: {filename}\")\n",
    "\n",
    "            # Look for YYYYMMDDHH pattern (10 digits)\n",
    "            import re\n",
    "\n",
    "            # Pattern 1: YYYYMMDDHH at end before extension\n",
    "            match = re.search(r'(\\d{10})(?:_P\\d+)?\\.grib2?$', filename)\n",
    "            if match:\n",
    "                datetime_str = match.group(1)\n",
    "                hour = int(datetime_str[8:10])  # Extract HH from YYYYMMDDHH\n",
    "                logger.debug(f\"✓ Extracted hour {hour:02d} from YYYYMMDDHH pattern ({datetime_str}) in {filename}\")\n",
    "                return hour\n",
    "\n",
    "            # Pattern 2: Look for HH in various positions\n",
    "            # Sometimes files have patterns like t00z, t06z, t12z, t18z\n",
    "            match = re.search(r't(\\d{2})z', filename)\n",
    "            if match:\n",
    "                hour = int(match.group(1))\n",
    "                logger.debug(f\"✓ Extracted hour {hour:02d} from tHHz pattern in {filename}\")\n",
    "                return hour\n",
    "\n",
    "            # Pattern 3: Look for /HH/ in URL path patterns (from source URL)\n",
    "            match = re.search(r'/(\\d{2})/', filename)\n",
    "            if match:\n",
    "                hour = int(match.group(1))\n",
    "                if 0 <= hour <= 23:  # Validate it's a valid hour\n",
    "                    logger.info(f\"✓ Extracted hour {hour:02d} from /HH/ pattern in {filename}\")\n",
    "                    return hour\n",
    "\n",
    "            # Pattern 4: More specific CMC pattern - look for YYYYMMDDHH anywhere in filename\n",
    "            match = re.search(r'(\\d{4})(\\d{2})(\\d{2})(\\d{2})', filename)\n",
    "            if match:\n",
    "                year, month, day, hour = match.groups()\n",
    "                hour_int = int(hour)\n",
    "                if 0 <= hour_int <= 23:  # Validate hour\n",
    "                    logger.info(f\"✓ Extracted hour {hour_int:02d} from YYYYMMDDHH pattern {year}{month}{day}{hour} in {filename}\")\n",
    "                    return hour_int\n",
    "\n",
    "            logger.warning(f\"⚠️ Could not extract hour from filename: {filename}\")\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error extracting hour from filename {filename}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _fix_vastdb_compatibility(self, table: pa.Table, preserve_datetimes: bool = True) -> pa.Table:\n",
    "        \"\"\"Convert fields incompatible with VastDB while preserving datetime precision\"\"\"\n",
    "        schema = table.schema\n",
    "        new_fields = []\n",
    "\n",
    "        for field in schema:\n",
    "            if field.type == pa.duration(\"ns\"):\n",
    "                # Convert duration to string as VastDB doesn't support it\n",
    "                new_fields.append(pa.field(field.name, pa.string()))\n",
    "            elif preserve_datetimes and pa.types.is_temporal(field.type):\n",
    "                # Preserve temporal types but ensure they're VastDB compatible\n",
    "                if pa.types.is_timestamp(field.type):\n",
    "                    # Convert to microsecond precision if nanosecond (VastDB prefers microseconds)\n",
    "                    if field.type.unit == 'ns':\n",
    "                        new_fields.append(pa.field(field.name, pa.timestamp('us')))\n",
    "                    else:\n",
    "                        new_fields.append(field)\n",
    "                elif pa.types.is_date(field.type) or pa.types.is_time(field.type):\n",
    "                    # Keep date and time types as-is\n",
    "                    new_fields.append(field)\n",
    "                else:\n",
    "                    new_fields.append(field)\n",
    "            else:\n",
    "                new_fields.append(field)\n",
    "\n",
    "        new_schema = pa.schema(new_fields)\n",
    "\n",
    "        # Cast incompatible columns\n",
    "        arrays = []\n",
    "        for i, field in enumerate(schema):\n",
    "            original_field = schema.field(i)\n",
    "            new_field = new_schema.field(i)\n",
    "\n",
    "            if original_field.type == pa.duration(\"ns\"):\n",
    "                # Convert duration to string\n",
    "                arrays.append(table.column(i).cast(pa.string()))\n",
    "            elif (preserve_datetimes and\n",
    "                  pa.types.is_timestamp(original_field.type) and\n",
    "                  original_field.type.unit == 'ns' and\n",
    "                  new_field.type != original_field.type):\n",
    "                # Convert nanosecond timestamps to microsecond\n",
    "                arrays.append(table.column(i).cast(pa.timestamp('us')))\n",
    "            else:\n",
    "                arrays.append(table.column(i))\n",
    "\n",
    "        return pa.Table.from_arrays(arrays, schema=new_schema)\n",
    "\n",
    "    def ingest_to_vastdb(self, parquet_path: str, bucket_name: str, schema_name: str, table_name: str,\n",
    "                        file_metadata: Dict = None) -> bool:\n",
    "        \"\"\"Ingest Parquet file to VastDB and save metadata\"\"\"\n",
    "        try:\n",
    "            # Read parquet\n",
    "            table = pq.read_table(parquet_path)\n",
    "\n",
    "            with self.vastdb_session.transaction() as tx:\n",
    "                bucket = tx.bucket(bucket_name)\n",
    "                schema = bucket.schema(schema_name, fail_if_missing=False) or bucket.create_schema(schema_name)\n",
    "\n",
    "                # Create or get main data table\n",
    "                db_table = schema.table(table_name, fail_if_missing=False)\n",
    "                if not db_table:\n",
    "                    db_table = schema.create_table(table_name, table.schema)\n",
    "                else:\n",
    "                    # Add any new columns\n",
    "                    self._add_missing_columns(db_table, table.schema)\n",
    "\n",
    "                # Insert data\n",
    "                db_table.insert(table)\n",
    "\n",
    "                # Save metadata to ingestion log table\n",
    "                if file_metadata:\n",
    "                    self._save_ingestion_metadata(schema, file_metadata, table.num_rows)\n",
    "\n",
    "            self.stats['files_ingested'] += 1\n",
    "            logger.debug(f\"Ingested {Path(parquet_path).name} to VastDB\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to ingest {parquet_path}: {e}\")\n",
    "            self.stats['errors'].append(f\"Ingestion failed: {parquet_path} - {e}\")\n",
    "            return False\n",
    "\n",
    "    def _add_missing_columns(self, table, new_schema):\n",
    "        \"\"\"Add missing columns to existing table\"\"\"\n",
    "        existing_fields = set(table.arrow_schema.names)\n",
    "        new_fields = set(new_schema.names)\n",
    "\n",
    "        for field_name in new_fields - existing_fields:\n",
    "            field = new_schema.field(field_name)\n",
    "            table.add_column(pa.schema([field]))\n",
    "            logger.debug(f\"Added column {field_name} to table\")\n",
    "\n",
    "    def _save_ingestion_metadata(self, schema, metadata: Dict, record_count: int):\n",
    "        \"\"\"Save ingestion metadata to a separate tracking table\"\"\"\n",
    "        try:\n",
    "            # Define metadata table schema\n",
    "            metadata_schema = pa.schema([\n",
    "                pa.field('ingestion_id', pa.string()),\n",
    "                pa.field('source_url', pa.string()),\n",
    "                pa.field('filename', pa.string()),\n",
    "                pa.field('file_size_bytes', pa.int64()),\n",
    "                pa.field('download_timestamp', pa.timestamp('us')),\n",
    "                pa.field('processing_timestamp', pa.timestamp('us')),\n",
    "                pa.field('ingestion_timestamp', pa.timestamp('us')),\n",
    "                pa.field('record_count', pa.int64()),\n",
    "                pa.field('source_name', pa.string()),\n",
    "                pa.field('download_duration_seconds', pa.float64()),\n",
    "                pa.field('processing_duration_seconds', pa.float64()),\n",
    "                pa.field('grib2_variables', pa.string()),  # JSON string of variables\n",
    "                pa.field('ingestion_batch_id', pa.string()),\n",
    "                pa.field('status', pa.string())\n",
    "            ])\n",
    "\n",
    "            # Create or get metadata table\n",
    "            metadata_table_name = f\"{metadata.get('main_table_name', 'grib2_data')}_ingestion_log\"\n",
    "            metadata_table = schema.table(metadata_table_name, fail_if_missing=False)\n",
    "\n",
    "            if not metadata_table:\n",
    "                metadata_table = schema.create_table(metadata_table_name, metadata_schema)\n",
    "                logger.info(f\"Created ingestion metadata table: {metadata_table_name}\")\n",
    "\n",
    "            # Prepare metadata record\n",
    "            metadata_record = {\n",
    "                'ingestion_id': metadata.get('ingestion_id', f\"{metadata['filename']}_{int(datetime.utcnow().timestamp())}\"),\n",
    "                'source_url': metadata.get('source_url', ''),\n",
    "                'filename': metadata.get('filename', ''),\n",
    "                'file_size_bytes': metadata.get('file_size_bytes', 0),\n",
    "                'download_timestamp': metadata.get('download_timestamp'),\n",
    "                'processing_timestamp': metadata.get('processing_timestamp'),\n",
    "                'ingestion_timestamp': datetime.utcnow(),\n",
    "                'record_count': record_count,\n",
    "                'source_name': metadata.get('source_name', ''),\n",
    "                'download_duration_seconds': metadata.get('download_duration_seconds', 0.0),\n",
    "                'processing_duration_seconds': metadata.get('processing_duration_seconds', 0.0),\n",
    "                'grib2_variables': metadata.get('grib2_variables', '[]'),  # JSON string\n",
    "                'ingestion_batch_id': metadata.get('batch_id', ''),\n",
    "                'status': 'SUCCESS'\n",
    "            }\n",
    "\n",
    "            # Convert to PyArrow table and insert\n",
    "            metadata_df = pd.DataFrame([metadata_record])\n",
    "            metadata_arrow_table = pa.Table.from_pandas(metadata_df, schema=metadata_schema)\n",
    "            metadata_table.insert(metadata_arrow_table)\n",
    "\n",
    "            logger.debug(f\"Saved metadata for {metadata['filename']} to {metadata_table_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to save ingestion metadata: {e}\")\n",
    "            # Don't fail the whole ingestion if metadata save fails\n",
    "\n",
    "    def cleanup_files(self, paths: List[str]):\n",
    "        \"\"\"Clean up temporary files\"\"\"\n",
    "        for path in paths:\n",
    "            try:\n",
    "                if os.path.exists(path):\n",
    "                    os.remove(path)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to remove {path}: {e}\")\n",
    "\n",
    "    def run_ingestion(self, plan: Dict, bucket_name: str, schema_name: str, table_name: str,\n",
    "                     batch_size: int = 10, max_workers: int = 3):\n",
    "        \"\"\"Run the complete ingestion process\"\"\"\n",
    "\n",
    "        logger.info(f\"🚀 Starting ingestion to {bucket_name}.{schema_name}.{table_name}\")\n",
    "        self.stats['start_time'] = datetime.now()\n",
    "\n",
    "        # Connect to VastDB\n",
    "        if not self.connect_to_vastdb():\n",
    "            return False\n",
    "\n",
    "        total_processed = 0\n",
    "\n",
    "        try:\n",
    "            for source_plan in plan['sources']:\n",
    "                logger.info(f\"📂 Processing source: {source_plan['name']}\")\n",
    "\n",
    "                # Discover files from this source\n",
    "                direct_files = source_plan.get('direct_files', None)\n",
    "                file_urls = self.discover_files_from_source(\n",
    "                    source_plan['url'],\n",
    "                    source_plan['files_to_download'],\n",
    "                    direct_files\n",
    "                )\n",
    "\n",
    "                if not file_urls:\n",
    "                    logger.warning(f\"No files found for {source_plan['name']}\")\n",
    "                    continue\n",
    "\n",
    "                # Process in batches to avoid disk space issues\n",
    "                for i in range(0, len(file_urls), batch_size):\n",
    "                    batch_urls = file_urls[i:i + batch_size]\n",
    "                    batch_id = f\"{source_plan['name']}_batch_{i//batch_size + 1}_{int(datetime.now().timestamp())}\"\n",
    "                    logger.info(f\"🔄 Processing batch {i//batch_size + 1}/{(len(file_urls) + batch_size - 1)//batch_size} ({len(batch_urls)} files)\")\n",
    "\n",
    "                    # Download batch with metadata\n",
    "                    downloaded_files = []\n",
    "                    download_metadata = []\n",
    "                    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                        future_to_url = {executor.submit(self.download_file, url, batch_id): url for url in batch_urls}\n",
    "\n",
    "                        for future in as_completed(future_to_url):\n",
    "                            result_path, metadata = future.result()\n",
    "                            if result_path and metadata:\n",
    "                                downloaded_files.append(result_path)\n",
    "                                download_metadata.append(metadata)\n",
    "\n",
    "                    # Process and ingest batch\n",
    "                    for j, grib2_path in enumerate(downloaded_files):\n",
    "                        download_meta = download_metadata[j] if j < len(download_metadata) else {}\n",
    "\n",
    "                        # Convert to Parquet with metadata\n",
    "                        parquet_path, processing_meta = self.process_grib2_to_parquet(\n",
    "                            grib2_path,\n",
    "                            download_meta.get('source_url', ''),\n",
    "                            source_plan['name']\n",
    "                        )\n",
    "\n",
    "                        if parquet_path and processing_meta:\n",
    "                            # Combine download and processing metadata\n",
    "                            combined_metadata = {**download_meta, **processing_meta}\n",
    "                            combined_metadata['main_table_name'] = table_name\n",
    "                            combined_metadata['batch_id'] = batch_id\n",
    "\n",
    "                            # Ingest to VastDB with metadata\n",
    "                            if self.ingest_to_vastdb(parquet_path, bucket_name, schema_name, table_name, combined_metadata):\n",
    "                                total_processed += 1\n",
    "\n",
    "                            # Clean up files immediately\n",
    "                            self.cleanup_files([grib2_path, parquet_path])\n",
    "                        else:\n",
    "                            # Clean up failed GRIB2 file\n",
    "                            self.cleanup_files([grib2_path])\n",
    "\n",
    "                    # Progress update\n",
    "                    elapsed = datetime.now() - self.stats['start_time']\n",
    "                    gb_downloaded = self.stats['bytes_downloaded'] / 1024 / 1024 / 1024\n",
    "\n",
    "                    logger.info(f\"📊 Progress: {total_processed} files ingested, {gb_downloaded:.2f} GB downloaded, {elapsed}\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"🛑 Process interrupted by user\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Unexpected error: {e}\")\n",
    "\n",
    "        # Final statistics\n",
    "        self._print_final_stats()\n",
    "        return True\n",
    "\n",
    "    def _print_final_stats(self):\n",
    "        \"\"\"Print final ingestion statistics\"\"\"\n",
    "        elapsed = datetime.now() - self.stats['start_time']\n",
    "        gb_downloaded = self.stats['bytes_downloaded'] / 1024 / 1024 / 1024\n",
    "        gb_processed = self.stats['bytes_processed'] / 1024 / 1024 / 1024\n",
    "\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"📈 FINAL INGESTION STATISTICS\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(f\"Files discovered: {self.stats['files_discovered']}\")\n",
    "        logger.info(f\"Files downloaded: {self.stats['files_downloaded']}\")\n",
    "        logger.info(f\"Files processed: {self.stats['files_processed']}\")\n",
    "        logger.info(f\"Files ingested: {self.stats['files_ingested']}\")\n",
    "        logger.info(f\"Data downloaded: {gb_downloaded:.2f} GB\")\n",
    "        logger.info(f\"Data processed: {gb_processed:.2f} GB\")\n",
    "        logger.info(f\"Total time: {elapsed}\")\n",
    "        logger.info(f\"Download rate: {gb_downloaded / elapsed.total_seconds() * 60:.1f} MB/min\")\n",
    "        logger.info(f\"Errors: {len(self.stats['errors'])}\")\n",
    "\n",
    "        if self.stats['errors']:\n",
    "            logger.info(\"Recent errors:\")\n",
    "            for error in self.stats['errors'][-5:]:  # Show last 5 errors\n",
    "                logger.info(f\"  - {error}\")\n",
    "\n",
    "\n",
    "def get_user_configuration(vastdb_config: Dict = None, db_config: Dict = None) -> Tuple[Dict, Dict, float]:\n",
    "    \"\"\"Get configuration from user input, parameters, and environment variables\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🌦️  GRIB2 BULK DATA INGESTER FOR VASTDB\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Data size selection\n",
    "    print(\"\\n📊 How much data would you like to ingest?\")\n",
    "    size_options = {\n",
    "        '1': 1.0,      # 1 GB\n",
    "        '2': 10.0,     # 10 GB\n",
    "        '3': 100.0,    # 100 GB\n",
    "        '4': 1000.0,   # 1 TB\n",
    "        '5': 10000.0   # 10 TB\n",
    "    }\n",
    "\n",
    "    print(\"1. 1 GB   (Quick test)\")\n",
    "    print(\"2. 10 GB  (Small dataset)\")\n",
    "    print(\"3. 100 GB (Medium dataset)\")\n",
    "    print(\"4. 1 TB   (Large dataset)\")\n",
    "    print(\"5. 10 TB  (Very large dataset)\")\n",
    "    print(\"6. Custom amount\")\n",
    "\n",
    "    choice = input(\"\\nSelect option (1-6): \").strip()\n",
    "\n",
    "    if choice in size_options:\n",
    "        target_size_gb = size_options[choice]\n",
    "    elif choice == '6':\n",
    "        target_size_gb = float(input(\"Enter size in GB: \"))\n",
    "    else:\n",
    "        print(\"Invalid choice, defaulting to 1 GB\")\n",
    "        target_size_gb = 1.0\n",
    "\n",
    "    # VastDB configuration from passed config or environment variables\n",
    "    print(f\"\\n🗄️  Loading VastDB Configuration\")\n",
    "\n",
    "    if vastdb_config:\n",
    "        print(\"✓ Using provided VastDB configuration\")\n",
    "        final_vastdb_config = vastdb_config.copy()\n",
    "    else:\n",
    "        print(\"Loading from environment variables...\")\n",
    "        final_vastdb_config = {}\n",
    "\n",
    "        # Required environment variables for VastDB connection\n",
    "        vastdb_env_vars = {\n",
    "            'VASTDB_ENDPOINT': 'endpoint',\n",
    "            'VASTDB_ACCESS_KEY': 'access_key',\n",
    "            'VASTDB_SECRET_KEY': 'secret_key'\n",
    "        }\n",
    "\n",
    "        missing_vastdb_vars = []\n",
    "        for var_name, key in vastdb_env_vars.items():\n",
    "            value = os.getenv(var_name)\n",
    "            if not value:\n",
    "                missing_vastdb_vars.append(var_name)\n",
    "            else:\n",
    "                final_vastdb_config[key] = value\n",
    "\n",
    "        if missing_vastdb_vars:\n",
    "            print(f\"\\n❌ Missing required VastDB environment variables:\")\n",
    "            for var in missing_vastdb_vars:\n",
    "                print(f\"   - {var}\")\n",
    "            print(f\"\\nPlease set these environment variables or pass vastdb_config to main().\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    # Database configuration from passed config or environment variables\n",
    "    print(f\"\\n🗃️  Loading Database Configuration\")\n",
    "\n",
    "    if db_config:\n",
    "        print(\"✓ Using provided database configuration\")\n",
    "        final_db_config = db_config.copy()\n",
    "\n",
    "        # Validate required keys\n",
    "        required_db_keys = ['bucket_name', 'schema_name', 'table_name']\n",
    "        missing_db_keys = [key for key in required_db_keys if key not in final_db_config]\n",
    "\n",
    "        if missing_db_keys:\n",
    "            print(f\"\\n❌ Missing required database config keys: {missing_db_keys}\")\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        print(\"Loading from environment variables...\")\n",
    "        final_db_config = {}\n",
    "        db_env_vars = {\n",
    "            'VASTDB_BUCKET': 'bucket_name',\n",
    "            'VASTDB_SCHEMA': 'schema_name',\n",
    "            'VASTDB_TABLE': 'table_name'\n",
    "        }\n",
    "\n",
    "        missing_db_vars = []\n",
    "        for var_name, key in db_env_vars.items():\n",
    "            value = os.getenv(var_name)\n",
    "            if not value:\n",
    "                missing_db_vars.append(var_name)\n",
    "            else:\n",
    "                final_db_config[key] = value\n",
    "\n",
    "        if missing_db_vars:\n",
    "            print(f\"\\n❌ Missing required database environment variables:\")\n",
    "            for var in missing_db_vars:\n",
    "                print(f\"   - {var}\")\n",
    "            print(f\"\\nExample:\")\n",
    "            print(f\"export VASTDB_BUCKET='your-bucket-name'\")\n",
    "            print(f\"export VASTDB_SCHEMA='your-schema-name'\")\n",
    "            print(f\"export VASTDB_TABLE='your-table-name'\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    print(f\"✓ Endpoint: {final_vastdb_config['endpoint']}\")\n",
    "    print(f\"✓ Target: {final_db_config['bucket_name']}.{final_db_config['schema_name']}.{final_db_config['table_name']}\")\n",
    "    print(f\"✓ Data Volume: {target_size_gb} GB\")\n",
    "\n",
    "    return final_vastdb_config, final_db_config, target_size_gb\n",
    "\n",
    "\n",
    "def test_basic_functionality(vastdb_config: Dict = None, db_config: Dict = None):\n",
    "    \"\"\"Test basic functionality with a simple manual approach\"\"\"\n",
    "    print(\"\\n🧪 TESTING MODE - Manual GRIB2 Download\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Test with a known working GRIB2 file\n",
    "    test_url = \"https://dd.weather.gc.ca/model_gem_global/15km/grib2/lat_lon/00/000/CMC_glb_ABSV_ISBL_200_latlon.15x.15_2024121100_P000.grib2\"\n",
    "\n",
    "    try:\n",
    "        import tempfile\n",
    "        import requests\n",
    "\n",
    "        session = requests.Session()\n",
    "        session.headers.update({'User-Agent': 'GRIB2-Test/1.0'})\n",
    "\n",
    "        print(f\"Testing download from: {test_url}\")\n",
    "\n",
    "        # Test download\n",
    "        response = session.head(test_url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            print(\"✓ Test file is accessible\")\n",
    "\n",
    "            # Get VastDB config from parameter or environment\n",
    "            if vastdb_config:\n",
    "                print(\"✓ Using provided VastDB configuration\")\n",
    "                final_vastdb_config = vastdb_config.copy()\n",
    "            else:\n",
    "                print(\"Loading VastDB configuration from environment variables...\")\n",
    "                final_vastdb_config = {}\n",
    "\n",
    "                vastdb_env_vars = {\n",
    "                    'VASTDB_ENDPOINT': 'endpoint',\n",
    "                    'VASTDB_ACCESS_KEY': 'access_key',\n",
    "                    'VASTDB_SECRET_KEY': 'secret_key'\n",
    "                }\n",
    "\n",
    "                missing_vars = []\n",
    "                for var_name, key in vastdb_env_vars.items():\n",
    "                    value = os.getenv(var_name)\n",
    "                    if not value:\n",
    "                        missing_vars.append(var_name)\n",
    "                    else:\n",
    "                        final_vastdb_config[key] = value\n",
    "\n",
    "                if missing_vars:\n",
    "                    print(f\"❌ Missing environment variables: {missing_vars}\")\n",
    "                    return\n",
    "\n",
    "            # Get database config from parameter or environment\n",
    "            if db_config:\n",
    "                print(\"✓ Using provided database configuration\")\n",
    "                final_db_config = db_config.copy()\n",
    "\n",
    "                # Validate required keys\n",
    "                required_db_keys = ['bucket_name', 'schema_name', 'table_name']\n",
    "                missing_db_keys = [key for key in required_db_keys if key not in final_db_config]\n",
    "\n",
    "                if missing_db_keys:\n",
    "                    print(f\"❌ Missing required database config keys: {missing_db_keys}\")\n",
    "                    return\n",
    "            else:\n",
    "                print(\"Loading database configuration from environment variables...\")\n",
    "                final_db_config = {}\n",
    "                db_env_vars = {\n",
    "                    'VASTDB_BUCKET': 'bucket_name',\n",
    "                    'VASTDB_SCHEMA': 'schema_name',\n",
    "                    'VASTDB_TABLE': 'table_name'\n",
    "                }\n",
    "\n",
    "                missing_db_vars = []\n",
    "                for var_name, key in db_env_vars.items():\n",
    "                    value = os.getenv(var_name)\n",
    "                    if not value:\n",
    "                        missing_db_vars.append(var_name)\n",
    "                    else:\n",
    "                        final_db_config[key] = value\n",
    "\n",
    "                if missing_db_vars:\n",
    "                    print(f\"❌ Missing database environment variables: {missing_db_vars}\")\n",
    "                    return\n",
    "\n",
    "            # Create test ingester\n",
    "            ingester = GRIB2DataIngester(final_vastdb_config)\n",
    "\n",
    "            if ingester.connect_to_vastdb():\n",
    "                print(\"\\n✓ VastDB connection successful\")\n",
    "\n",
    "                # Download test file\n",
    "                temp_dir = Path(tempfile.mkdtemp())\n",
    "                test_file = temp_dir / \"test.grib2\"\n",
    "\n",
    "                print(f\"Downloading test file...\")\n",
    "                response = session.get(test_url, stream=True)\n",
    "                with open(test_file, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "\n",
    "                print(f\"✓ Downloaded {test_file.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "                # Process to parquet\n",
    "                parquet_file, processing_meta = ingester.process_grib2_to_parquet(str(test_file))\n",
    "                if parquet_file and processing_meta:\n",
    "                    print(\"✓ Converted to Parquet\")\n",
    "\n",
    "                    # Add test metadata\n",
    "                    test_metadata = {\n",
    "                        **processing_meta,\n",
    "                        'source_url': test_url,\n",
    "                        'main_table_name': final_db_config['table_name'],\n",
    "                        'batch_id': f\"test_{int(datetime.now().timestamp())}\",\n",
    "                        'download_timestamp': datetime.utcnow(),\n",
    "                        'download_duration_seconds': 5.0  # Approximate\n",
    "                    }\n",
    "\n",
    "                    # Ingest to VastDB\n",
    "                    success = ingester.ingest_to_vastdb(\n",
    "                        parquet_file,\n",
    "                        final_db_config['bucket_name'],\n",
    "                        final_db_config['schema_name'],\n",
    "                        final_db_config['table_name'],\n",
    "                        test_metadata\n",
    "                    )\n",
    "\n",
    "                    if success:\n",
    "                        print(\"✅ Test ingestion successful!\")\n",
    "                        print(f\"Data is now in {final_db_config['bucket_name']}.{final_db_config['schema_name']}.{final_db_config['table_name']}\")\n",
    "                        print(f\"Metadata saved to {final_db_config['bucket_name']}.{final_db_config['schema_name']}.{final_db_config['table_name']}_ingestion_log\")\n",
    "                    else:\n",
    "                        print(\"❌ Test ingestion failed\")\n",
    "\n",
    "                # Cleanup\n",
    "                import shutil\n",
    "                shutil.rmtree(temp_dir)\n",
    "            else:\n",
    "                print(\"❌ VastDB connection failed\")\n",
    "        else:\n",
    "            print(f\"❌ Test file not accessible: HTTP {response.status_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Test failed: {e}\")\n",
    "\n",
    "\n",
    "def main(vastdb_config: Dict = None, db_config: Dict = None, target_size_gb: float = None):\n",
    "    \"\"\"\n",
    "    Main application\n",
    "\n",
    "    Args:\n",
    "        vastdb_config: Dict with keys 'endpoint', 'access_key', 'secret_key'\n",
    "        db_config: Dict with keys 'bucket_name', 'schema_name', 'table_name'\n",
    "        target_size_gb: Target data size in GB (if None, will prompt user)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🌦️  GRIB2 BULK DATA INGESTER FOR VASTDB\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # If configs are provided, skip mode selection and go straight to ingestion\n",
    "    if vastdb_config and db_config:\n",
    "        print(\"✓ Configuration provided - running in programmatic mode\")\n",
    "        mode = '1'  # Full bulk ingestion\n",
    "    else:\n",
    "        print(\"Choose mode:\")\n",
    "        print(\"1. Full bulk ingestion (discover and download multiple files)\")\n",
    "        print(\"2. Test mode (single file test)\")\n",
    "\n",
    "        mode = input(\"Select mode (1-2): \").strip()\n",
    "\n",
    "    if mode == '2':\n",
    "        test_basic_functionality(vastdb_config, db_config)\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Get user configuration (or use provided configs)\n",
    "        if vastdb_config and db_config and target_size_gb is not None:\n",
    "            final_vastdb_config = vastdb_config.copy()\n",
    "            final_db_config = db_config.copy()\n",
    "            final_target_size_gb = target_size_gb\n",
    "\n",
    "            print(f\"✓ Using provided configuration:\")\n",
    "            print(f\"  - Endpoint: {final_vastdb_config['endpoint']}\")\n",
    "            print(f\"  - Target: {final_db_config['bucket_name']}.{final_db_config['schema_name']}.{final_db_config['table_name']}\")\n",
    "            print(f\"  - Data Volume: {final_target_size_gb} GB\")\n",
    "        else:\n",
    "            final_vastdb_config, final_db_config, final_target_size_gb = get_user_configuration(vastdb_config, db_config)\n",
    "\n",
    "        # Initialize ingester\n",
    "        ingester = GRIB2DataIngester(final_vastdb_config)\n",
    "\n",
    "        # Discover available sources\n",
    "        print(f\"\\n🔍 Discovering GRIB2 data sources...\")\n",
    "        available_sources = ingester.discover_grib2_sources()\n",
    "\n",
    "        if not available_sources:\n",
    "            print(\"❌ No available GRIB2 sources found!\")\n",
    "            print(\"💡 Try test mode (option 2) to test with a single known file\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\n✅ Found {len(available_sources)} available sources\")\n",
    "\n",
    "        # Build download plan\n",
    "        plan = ingester.build_download_plan(available_sources, final_target_size_gb)\n",
    "\n",
    "        if plan['total_estimated_files'] == 0:\n",
    "            print(\"❌ Could not build a download plan!\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\n📋 Download Plan:\")\n",
    "        print(f\"Target: {final_target_size_gb} GB\")\n",
    "        print(f\"Estimated: {plan['total_estimated_files']} files, {plan['total_estimated_size_gb']:.1f} GB\")\n",
    "\n",
    "        for source in plan['sources']:\n",
    "            print(f\"  - {source['name']}: {source['files_to_download']} files ({source['estimated_size_gb']:.1f} GB)\")\n",
    "\n",
    "        # Confirm with user (unless running programmatically)\n",
    "        if not (vastdb_config and db_config and target_size_gb is not None):\n",
    "            confirm = input(f\"\\n🚀 Start ingestion to {final_db_config['bucket_name']}.{final_db_config['schema_name']}.{final_db_config['table_name']}? (y/n): \")\n",
    "            if confirm.lower() != 'y':\n",
    "                print(\"Cancelled by user\")\n",
    "                return\n",
    "        else:\n",
    "            print(f\"\\n🚀 Starting automatic ingestion to {final_db_config['bucket_name']}.{final_db_config['schema_name']}.{final_db_config['table_name']}\")\n",
    "\n",
    "        # Run ingestion\n",
    "        success = ingester.run_ingestion(\n",
    "            plan,\n",
    "            final_db_config['bucket_name'],\n",
    "            final_db_config['schema_name'],\n",
    "            final_db_config['table_name'],\n",
    "            batch_size=5,      # Process 5 files at a time\n",
    "            max_workers=2      # Conservative parallelism\n",
    "        )\n",
    "\n",
    "        if success:\n",
    "            print(\"\\n✅ Ingestion completed!\")\n",
    "            print(f\"Data is now available in {final_db_config['bucket_name']}.{final_db_config['schema_name']}.{final_db_config['table_name']}\")\n",
    "        else:\n",
    "            print(\"\\n❌ Ingestion failed!\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n🛑 Process interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Unexpected error: {e}\")\n",
    "    finally:\n",
    "        print(\"\\n👋 Goodbye!\")\n",
    "\n",
    "\n",
    "# Example usage functions for easy integration\n",
    "def run_with_config(endpoint: str, access_key: str, secret_key: str,\n",
    "                   bucket_name: str, schema_name: str, table_name: str,\n",
    "                   target_size_gb: float = 1.0):\n",
    "    \"\"\"\n",
    "    Convenience function to run ingestion with explicit parameters\n",
    "\n",
    "    Example:\n",
    "        run_with_config(\n",
    "            endpoint='https://my-vastdb.com',\n",
    "            access_key='your_access_key',\n",
    "            secret_key='your_secret_key',\n",
    "            bucket_name='weather_data',\n",
    "            schema_name='grib2',\n",
    "            table_name='meteorological_data',\n",
    "            target_size_gb=10.0\n",
    "        )\n",
    "    \"\"\"\n",
    "    vastdb_config = {\n",
    "        'endpoint': endpoint,\n",
    "        'access_key': access_key,\n",
    "        'secret_key': secret_key\n",
    "    }\n",
    "\n",
    "    db_config = {\n",
    "        'bucket_name': bucket_name,\n",
    "        'schema_name': schema_name,\n",
    "        'table_name': table_name\n",
    "    }\n",
    "\n",
    "    main(vastdb_config, db_config, target_size_gb)\n",
    "\n",
    "\n",
    "def run_test_mode(endpoint: str, access_key: str, secret_key: str,\n",
    "                 bucket_name: str, schema_name: str, table_name: str):\n",
    "    \"\"\"\n",
    "    Convenience function to run test mode with explicit parameters\n",
    "\n",
    "    Example:\n",
    "        run_test_mode(\n",
    "            endpoint='https://my-vastdb.com',\n",
    "            access_key='your_access_key',\n",
    "            secret_key='your_secret_key',\n",
    "            bucket_name='weather_data',\n",
    "            schema_name='grib2',\n",
    "            table_name='test_data'\n",
    "        )\n",
    "    \"\"\"\n",
    "    vastdb_config = {\n",
    "        'endpoint': endpoint,\n",
    "        'access_key': access_key,\n",
    "        'secret_key': secret_key\n",
    "    }\n",
    "\n",
    "    db_config = {\n",
    "        'bucket_name': bucket_name,\n",
    "        'schema_name': schema_name,\n",
    "        'table_name': table_name\n",
    "    }\n",
    "\n",
    "    test_basic_functionality(vastdb_config, db_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840aed23-b96b-46a1-be7c-b1adaa852dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_with_config(\n",
    "    endpoint=endpoint,\n",
    "    access_key=access_key,\n",
    "    secret_key=secret_key,\n",
    "    bucket_name='csnowdb',\n",
    "    schema_name='grib2',\n",
    "    table_name='grib2_data',\n",
    "    target_size_gb=1000000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
