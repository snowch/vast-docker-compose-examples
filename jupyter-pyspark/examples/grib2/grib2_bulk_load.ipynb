{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f2dbe4-d11c-4f13-8a2a-13c6e8cb7da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --quiet cfgrib xarray pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c672f-a5da-48f1-b4d6-d038981a5c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = 'http://172.200.202.2:80'\n",
    "access_key = 'O6UZCRYPS9OPWUD1DYF8'\n",
    "secret_key = 'HnF34+7JkYFusmbU++Cv0/YLUiEpWGBRZTjmUsuu'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "165a8252-120a-475e-abdc-cb2e2d7fe46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import requests\n",
    "import tempfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Data processing imports\n",
    "import cfgrib\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import vastdb\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class GRIB2DataIngester:\n",
    "    \"\"\"GRIB2 Data Discovery and Bulk Ingestion System for VastDB\"\"\"\n",
    "    \n",
    "    def __init__(self, vastdb_config: Dict[str, str], temp_dir: Optional[str] = None):\n",
    "        self.vastdb_config = vastdb_config\n",
    "        self.temp_dir = Path(temp_dir) if temp_dir else Path(tempfile.mkdtemp())\n",
    "        self.temp_dir.mkdir(exist_ok=True)\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({'User-Agent': 'GRIB2-Bulk-Ingester/1.0'})\n",
    "        self.vastdb_session = None\n",
    "        \n",
    "        # Statistics tracking\n",
    "        self.stats = {\n",
    "            'files_discovered': 0,\n",
    "            'files_downloaded': 0,\n",
    "            'files_processed': 0,\n",
    "            'files_ingested': 0,\n",
    "            'bytes_downloaded': 0,\n",
    "            'bytes_processed': 0,\n",
    "            'start_time': None,\n",
    "            'errors': []\n",
    "        }\n",
    "    \n",
    "    def connect_to_vastdb(self):\n",
    "        \"\"\"Connect to VastDB\"\"\"\n",
    "        try:\n",
    "            self.vastdb_session = vastdb.connect(\n",
    "                endpoint=self.vastdb_config['endpoint'],\n",
    "                access=self.vastdb_config['access_key'],\n",
    "                secret=self.vastdb_config['secret_key']\n",
    "            )\n",
    "            logger.info(\"✓ Connected to VastDB\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"✗ Failed to connect to VastDB: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def discover_grib2_sources(self) -> List[Dict]:\n",
    "        \"\"\"Discover available GRIB2 data sources\"\"\"\n",
    "        sources = [\n",
    "            {\n",
    "                'name': 'GEM Global 66km', \n",
    "                'url': 'https://dd.weather.gc.ca/model_gem_global/66km/grib2/lat_lon',\n",
    "                'description': 'Lower resolution global model (more stable)',\n",
    "                'estimated_file_size_mb': 0.8\n",
    "            },\n",
    "            {\n",
    "                'name': 'GEM Global 15km',\n",
    "                'url': 'https://dd.weather.gc.ca/model_gem_global/15km/grib2/lat_lon',\n",
    "                'description': 'High resolution global model',\n",
    "                'estimated_file_size_mb': 2.5\n",
    "            },\n",
    "            {\n",
    "                'name': 'HRDPS Continental 2.5km',\n",
    "                'url': 'https://dd.weather.gc.ca/model_hrdps/continental/2.5km',\n",
    "                'description': 'Very high resolution continental model',\n",
    "                'estimated_file_size_mb': 8.0\n",
    "            },\n",
    "            {\n",
    "                'name': 'Sample GRIB2 Files',\n",
    "                'url': 'https://dd.weather.gc.ca/model_gem_global/25km/grib2/lat_lon',\n",
    "                'description': 'Alternative global model data',\n",
    "                'estimated_file_size_mb': 1.5\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        logger.info(\"🔍 Discovering available GRIB2 data sources...\")\n",
    "        available_sources = []\n",
    "        \n",
    "        for source in sources:\n",
    "            try:\n",
    "                logger.info(f\"Checking {source['name']}...\")\n",
    "                \n",
    "                # More lenient check - just see if we can access the URL\n",
    "                response = self.session.get(source['url'], timeout=15)\n",
    "                if response.status_code == 200:\n",
    "                    # Quick check for actual files\n",
    "                    file_count = self._quick_file_count(source['url'])\n",
    "                    \n",
    "                    # Be more generous - even 1 file is enough\n",
    "                    if file_count >= 1:\n",
    "                        source['available'] = True\n",
    "                        source['estimated_files'] = max(file_count, 10)  # Minimum assumption\n",
    "                        source['estimated_total_size_gb'] = (source['estimated_files'] * source['estimated_file_size_mb']) / 1024\n",
    "                        available_sources.append(source)\n",
    "                        logger.info(f\"✓ {source['name']}: ~{source['estimated_files']} files (~{source['estimated_total_size_gb']:.1f} GB)\")\n",
    "                    else:\n",
    "                        # Even if no files found, add with minimal estimate\n",
    "                        source['available'] = True\n",
    "                        source['estimated_files'] = 5\n",
    "                        source['estimated_total_size_gb'] = (5 * source['estimated_file_size_mb']) / 1024\n",
    "                        available_sources.append(source)\n",
    "                        logger.info(f\"⚠️ {source['name']}: Directory accessible, assuming ~5 files\")\n",
    "                else:\n",
    "                    logger.info(f\"✗ {source['name']}: HTTP {response.status_code}\")\n",
    "            except Exception as e:\n",
    "                logger.info(f\"✗ {source['name']}: Error - {e}\")\n",
    "        \n",
    "        # Fallback: If no sources found, create a test source with known working file\n",
    "        if not available_sources:\n",
    "            logger.info(\"No sources found via directory browsing, trying direct file access...\")\n",
    "            fallback_source = {\n",
    "                'name': 'Direct GRIB2 File',\n",
    "                'url': 'https://dd.weather.gc.ca/model_gem_global/15km/grib2/lat_lon',\n",
    "                'description': 'Direct file access method',\n",
    "                'estimated_file_size_mb': 2.5,\n",
    "                'available': True,\n",
    "                'estimated_files': 1,\n",
    "                'estimated_total_size_gb': 0.0025,\n",
    "                'direct_files': [\n",
    "                    'https://dd.weather.gc.ca/model_gem_global/15km/grib2/lat_lon/00/000/CMC_glb_ABSV_ISBL_200_latlon.15x.15_2024121100_P000.grib2'\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            # Test if we can access the known file\n",
    "            try:\n",
    "                test_response = self.session.head(fallback_source['direct_files'][0], timeout=10)\n",
    "                if test_response.status_code == 200:\n",
    "                    available_sources.append(fallback_source)\n",
    "                    logger.info(\"✓ Found working direct file access\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return available_sources\n",
    "    \n",
    "    def _quick_file_count(self, base_url: str, max_dirs: int = 3) -> int:\n",
    "        \"\"\"Quick estimation of available files\"\"\"\n",
    "        try:\n",
    "            # Check a few directories to estimate total files\n",
    "            dirs = self._get_directories(base_url)\n",
    "            total_files = 0\n",
    "            \n",
    "            for i, dir_name in enumerate(dirs[:max_dirs]):\n",
    "                dir_url = f\"{base_url}/{dir_name}/\"\n",
    "                subdirs = self._get_directories(dir_url)\n",
    "                \n",
    "                for j, subdir in enumerate(subdirs[:2]):  # Check first 2 subdirs\n",
    "                    subdir_url = f\"{dir_url}{subdir}/\"\n",
    "                    files = self._get_grib2_files(subdir_url, limit=100)\n",
    "                    total_files += len(files)\n",
    "                    \n",
    "                    if total_files > 500:  # Cap estimation\n",
    "                        return min(total_files * (len(dirs) * len(subdirs)) // ((i+1) * (j+1)), 10000)\n",
    "            \n",
    "            # Extrapolate\n",
    "            if total_files > 0 and len(dirs) > max_dirs:\n",
    "                total_files = total_files * len(dirs) // max_dirs\n",
    "            \n",
    "            return min(total_files, 10000)  # Cap at reasonable number\n",
    "            \n",
    "        except Exception:\n",
    "            return 0\n",
    "    \n",
    "    def _get_directories(self, url: str) -> List[str]:\n",
    "        \"\"\"Get directory listings\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            dirs = []\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if href.endswith('/') and not href.startswith('..') and href != '/':\n",
    "                    dirs.append(href.rstrip('/'))\n",
    "            return dirs\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    def _get_grib2_files(self, url: str, limit: int = 50) -> List[str]:\n",
    "        \"\"\"Get GRIB2 file listings\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            files = []\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if href.endswith('.grib2'):\n",
    "                    files.append(href)\n",
    "                    if len(files) >= limit:\n",
    "                        break\n",
    "            return files\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    def build_download_plan(self, sources: List[Dict], target_size_gb: float) -> Dict:\n",
    "        \"\"\"Build a download plan to reach target data size\"\"\"\n",
    "        logger.info(f\"📋 Building download plan for {target_size_gb} GB of data...\")\n",
    "        \n",
    "        plan = {\n",
    "            'target_size_gb': target_size_gb,\n",
    "            'sources': [],\n",
    "            'total_estimated_size_gb': 0,\n",
    "            'total_estimated_files': 0\n",
    "        }\n",
    "        \n",
    "        # Sort sources by file size (prefer smaller files for more diversity)\n",
    "        sources.sort(key=lambda x: x['estimated_file_size_mb'])\n",
    "        \n",
    "        remaining_gb = target_size_gb\n",
    "        \n",
    "        for source in sources:\n",
    "            if remaining_gb <= 0:\n",
    "                break\n",
    "            \n",
    "            # Calculate how many files we need from this source\n",
    "            files_needed = min(\n",
    "                int((remaining_gb * 1024) / source['estimated_file_size_mb']),\n",
    "                source['estimated_files'],\n",
    "                2000  # Cap per source\n",
    "            )\n",
    "            \n",
    "            if files_needed > 0:\n",
    "                size_from_source = (files_needed * source['estimated_file_size_mb']) / 1024\n",
    "                \n",
    "                plan['sources'].append({\n",
    "                    'name': source['name'],\n",
    "                    'url': source['url'],\n",
    "                    'files_to_download': files_needed,\n",
    "                    'estimated_size_gb': size_from_source,\n",
    "                    'estimated_file_size_mb': source['estimated_file_size_mb'],\n",
    "                    'direct_files': source.get('direct_files', None)\n",
    "                })\n",
    "                \n",
    "                plan['total_estimated_files'] += files_needed\n",
    "                plan['total_estimated_size_gb'] += size_from_source\n",
    "                remaining_gb -= size_from_source\n",
    "        \n",
    "        logger.info(f\"📊 Plan: {plan['total_estimated_files']} files, {plan['total_estimated_size_gb']:.1f} GB from {len(plan['sources'])} sources\")\n",
    "        return plan\n",
    "    \n",
    "    def discover_files_from_source(self, source_url: str, max_files: int, direct_files: List[str] = None) -> List[str]:\n",
    "        \"\"\"Discover actual file URLs from a source\"\"\"\n",
    "        logger.info(f\"🔍 Discovering files from {source_url}...\")\n",
    "        \n",
    "        # If we have direct files (fallback mode), use those\n",
    "        if direct_files:\n",
    "            logger.info(f\"Using direct file list: {len(direct_files)} files\")\n",
    "            return direct_files[:max_files]\n",
    "        \n",
    "        file_urls = []\n",
    "        dirs = self._get_directories(source_url)\n",
    "        \n",
    "        # If no directories found, try a more aggressive approach\n",
    "        if not dirs:\n",
    "            logger.info(\"No directories found, trying current date directories...\")\n",
    "            # Try recent dates\n",
    "            today = datetime.now()\n",
    "            for days_back in range(7):  # Try last 7 days\n",
    "                test_date = today - timedelta(days=days_back)\n",
    "                for hour in ['00', '06', '12', '18']:\n",
    "                    test_dir = f\"{test_date.strftime('%Y%m%d')}{hour}\"\n",
    "                    if test_dir not in dirs:\n",
    "                        dirs.append(hour)  # Just try the hour directories\n",
    "        \n",
    "        for dir_name in dirs:\n",
    "            if len(file_urls) >= max_files:\n",
    "                break\n",
    "                \n",
    "            dir_url = f\"{source_url}/{dir_name}/\"\n",
    "            subdirs = self._get_directories(dir_url)\n",
    "            \n",
    "            # If no subdirs, try common forecast hours\n",
    "            if not subdirs:\n",
    "                subdirs = ['000', '003', '006', '012', '024', '048']\n",
    "            \n",
    "            for subdir in subdirs:\n",
    "                if len(file_urls) >= max_files:\n",
    "                    break\n",
    "                    \n",
    "                subdir_url = f\"{dir_url}{subdir}/\"\n",
    "                files = self._get_grib2_files(subdir_url, limit=max_files - len(file_urls))\n",
    "                \n",
    "                for file in files:\n",
    "                    file_urls.append(f\"{subdir_url}{file}\")\n",
    "                    if len(file_urls) >= max_files:\n",
    "                        break\n",
    "        \n",
    "        logger.info(f\"📁 Discovered {len(file_urls)} file URLs\")\n",
    "        self.stats['files_discovered'] += len(file_urls)\n",
    "        return file_urls\n",
    "    \n",
    "    def download_file(self, url: str, batch_id: str = None) -> Tuple[Optional[str], Optional[Dict]]:\n",
    "        \"\"\"Download a single GRIB2 file and return metadata\"\"\"\n",
    "        filename = Path(url).name\n",
    "        local_path = self.temp_dir / filename\n",
    "        download_start = datetime.utcnow()\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(url, stream=True, timeout=120)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            with open(local_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            \n",
    "            download_end = datetime.utcnow()\n",
    "            file_size = local_path.stat().st_size\n",
    "            download_duration = (download_end - download_start).total_seconds()\n",
    "            \n",
    "            self.stats['bytes_downloaded'] += file_size\n",
    "            self.stats['files_downloaded'] += 1\n",
    "            \n",
    "            # Prepare download metadata\n",
    "            metadata = {\n",
    "                'source_url': url,\n",
    "                'filename': filename,\n",
    "                'file_size_bytes': file_size,\n",
    "                'download_timestamp': download_start,\n",
    "                'download_duration_seconds': download_duration,\n",
    "                'batch_id': batch_id or f\"batch_{int(datetime.utcnow().timestamp())}\"\n",
    "            }\n",
    "            \n",
    "            logger.debug(f\"Downloaded {filename} ({file_size / 1024 / 1024:.1f} MB)\")\n",
    "            return str(local_path), metadata\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to download {url}: {e}\")\n",
    "            self.stats['errors'].append(f\"Download failed: {url} - {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def cleanup_files(self, paths: List[str]):\n",
    "        \"\"\"Clean up temporary files\"\"\"\n",
    "        for path in paths:\n",
    "            try:\n",
    "                if os.path.exists(path):\n",
    "                    os.remove(path)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to remove {path}: {e}\")\n",
    "    \n",
    "    def process_grib2_to_parquet(self, grib2_path: str) -> Optional[str]:\n",
    "        \"\"\"Process GRIB2 file to VastDB-compatible Parquet\"\"\"\n",
    "        try:\n",
    "            # Read GRIB2 file with explicit decode_timedelta to silence warning\n",
    "            ds = xr.open_dataset(grib2_path, engine=\"cfgrib\", \n",
    "                               backend_kwargs={'decode_timedelta': False})\n",
    "            df = ds.to_dataframe().reset_index()\n",
    "            \n",
    "            # Add metadata\n",
    "            df['source_file'] = Path(grib2_path).name\n",
    "            df['ingestion_timestamp'] = datetime.utcnow()\n",
    "            \n",
    "            # Convert to PyArrow table\n",
    "            table = pa.Table.from_pandas(df)\n",
    "            \n",
    "            # Fix VastDB incompatible types\n",
    "            table = self._fix_vastdb_compatibility(table)\n",
    "            \n",
    "            # Save to parquet\n",
    "            parquet_path = grib2_path.replace('.grib2', '.parquet')\n",
    "            pq.write_table(table, parquet_path)\n",
    "            \n",
    "            file_size = Path(parquet_path).stat().st_size\n",
    "            self.stats['bytes_processed'] += file_size\n",
    "            self.stats['files_processed'] += 1\n",
    "            \n",
    "            logger.debug(f\"Processed {Path(grib2_path).name} to Parquet ({file_size / 1024 / 1024:.1f} MB)\")\n",
    "            return parquet_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process {grib2_path}: {e}\")\n",
    "            self.stats['errors'].append(f\"Processing failed: {grib2_path} - {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _fix_vastdb_compatibility(self, table: pa.Table) -> pa.Table:\n",
    "        \"\"\"Convert fields incompatible with VastDB\"\"\"\n",
    "        schema = table.schema\n",
    "        new_fields = []\n",
    "        \n",
    "        for field in schema:\n",
    "            if field.type == pa.duration(\"ns\"):\n",
    "                new_fields.append(pa.field(field.name, pa.string()))\n",
    "            else:\n",
    "                new_fields.append(field)\n",
    "        \n",
    "        new_schema = pa.schema(new_fields)\n",
    "        \n",
    "        # Cast incompatible columns\n",
    "        arrays = []\n",
    "        for i, field in enumerate(schema):\n",
    "            if field.type == pa.duration(\"ns\"):\n",
    "                arrays.append(table.column(i).cast(pa.string()))\n",
    "            else:\n",
    "                arrays.append(table.column(i))\n",
    "        \n",
    "        return pa.Table.from_arrays(arrays, schema=new_schema)\n",
    "    \n",
    "    def ingest_to_vastdb(self, parquet_path: str, bucket_name: str, schema_name: str, table_name: str, \n",
    "                        file_metadata: Dict = None) -> bool:\n",
    "        \"\"\"Ingest Parquet file to VastDB and save metadata\"\"\"\n",
    "        try:\n",
    "            # Read parquet\n",
    "            table = pq.read_table(parquet_path)\n",
    "            \n",
    "            with self.vastdb_session.transaction() as tx:\n",
    "                bucket = tx.bucket(bucket_name)\n",
    "                schema = bucket.schema(schema_name, fail_if_missing=False) or bucket.create_schema(schema_name)\n",
    "                \n",
    "                # Create or get main data table\n",
    "                db_table = schema.table(table_name, fail_if_missing=False)\n",
    "                if not db_table:\n",
    "                    db_table = schema.create_table(table_name, table.schema)\n",
    "                else:\n",
    "                    # Add any new columns\n",
    "                    self._add_missing_columns(db_table, table.schema)\n",
    "                \n",
    "                # Insert data\n",
    "                db_table.insert(table)\n",
    "                \n",
    "                # Save metadata to ingestion log table\n",
    "                if file_metadata:\n",
    "                    self._save_ingestion_metadata(schema, file_metadata, table.num_rows)\n",
    "                \n",
    "            self.stats['files_ingested'] += 1\n",
    "            logger.debug(f\"Ingested {Path(parquet_path).name} to VastDB\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to ingest {parquet_path}: {e}\")\n",
    "            self.stats['errors'].append(f\"Ingestion failed: {parquet_path} - {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _add_missing_columns(self, table, new_schema):\n",
    "        \"\"\"Add missing columns to existing table\"\"\"\n",
    "        existing_fields = set(table.arrow_schema.names)\n",
    "        new_fields = set(new_schema.names)\n",
    "        \n",
    "        for field_name in new_fields - existing_fields:\n",
    "            field = new_schema.field(field_name)\n",
    "            table.add_column(pa.schema([field]))\n",
    "            logger.debug(f\"Added column {field_name} to table\")\n",
    "    \n",
    "    def _save_ingestion_metadata(self, schema, metadata: Dict, record_count: int):\n",
    "        \"\"\"Save ingestion metadata to a separate tracking table\"\"\"\n",
    "        try:\n",
    "            # Define metadata table schema\n",
    "            metadata_schema = pa.schema([\n",
    "                pa.field('ingestion_id', pa.string()),\n",
    "                pa.field('source_url', pa.string()),\n",
    "                pa.field('filename', pa.string()),\n",
    "                pa.field('file_size_bytes', pa.int64()),\n",
    "                pa.field('download_timestamp', pa.timestamp('us')),\n",
    "                pa.field('processing_timestamp', pa.timestamp('us')),\n",
    "                pa.field('ingestion_timestamp', pa.timestamp('us')),\n",
    "                pa.field('record_count', pa.int64()),\n",
    "                pa.field('source_name', pa.string()),\n",
    "                pa.field('download_duration_seconds', pa.float64()),\n",
    "                pa.field('processing_duration_seconds', pa.float64()),\n",
    "                pa.field('grib2_variables', pa.string()),  # JSON string of variables\n",
    "                pa.field('ingestion_batch_id', pa.string()),\n",
    "                pa.field('status', pa.string())\n",
    "            ])\n",
    "            \n",
    "            # Create or get metadata table\n",
    "            metadata_table_name = f\"{metadata.get('main_table_name', 'grib2_data')}_ingestion_log\"\n",
    "            metadata_table = schema.table(metadata_table_name, fail_if_missing=False)\n",
    "            \n",
    "            if not metadata_table:\n",
    "                metadata_table = schema.create_table(metadata_table_name, metadata_schema)\n",
    "                logger.info(f\"Created ingestion metadata table: {metadata_table_name}\")\n",
    "            \n",
    "            # Prepare metadata record\n",
    "            metadata_record = {\n",
    "                'ingestion_id': metadata.get('ingestion_id', f\"{metadata['filename']}_{int(datetime.utcnow().timestamp())}\"),\n",
    "                'source_url': metadata.get('source_url', ''),\n",
    "                'filename': metadata.get('filename', ''),\n",
    "                'file_size_bytes': metadata.get('file_size_bytes', 0),\n",
    "                'download_timestamp': metadata.get('download_timestamp'),\n",
    "                'processing_timestamp': metadata.get('processing_timestamp'),\n",
    "                'ingestion_timestamp': datetime.utcnow(),\n",
    "                'record_count': record_count,\n",
    "                'source_name': metadata.get('source_name', ''),\n",
    "                'download_duration_seconds': metadata.get('download_duration_seconds', 0.0),\n",
    "                'processing_duration_seconds': metadata.get('processing_duration_seconds', 0.0),\n",
    "                'grib2_variables': metadata.get('grib2_variables', '[]'),  # JSON string\n",
    "                'ingestion_batch_id': metadata.get('batch_id', ''),\n",
    "                'status': 'SUCCESS'\n",
    "            }\n",
    "            \n",
    "            # Convert to PyArrow table and insert\n",
    "            metadata_df = pd.DataFrame([metadata_record])\n",
    "            metadata_arrow_table = pa.Table.from_pandas(metadata_df, schema=metadata_schema)\n",
    "            metadata_table.insert(metadata_arrow_table)\n",
    "            \n",
    "            logger.debug(f\"Saved metadata for {metadata['filename']} to {metadata_table_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to save ingestion metadata: {e}\")\n",
    "            # Don't fail the whole ingestion if metadata save fails\n",
    "    \n",
    "    def process_grib2_to_parquet(self, grib2_path: str, source_url: str = None, source_name: str = None) -> Tuple[Optional[str], Optional[Dict]]:\n",
    "        \"\"\"Process GRIB2 file to VastDB-compatible Parquet and return metadata\"\"\"\n",
    "        processing_start = datetime.utcnow()\n",
    "        \n",
    "        try:\n",
    "            # Read GRIB2 file with explicit decode_timedelta to silence warning\n",
    "            ds = xr.open_dataset(grib2_path, engine=\"cfgrib\", \n",
    "                               backend_kwargs={'decode_timedelta': False})\n",
    "            df = ds.to_dataframe().reset_index()\n",
    "            \n",
    "            # Extract GRIB2 variables for metadata\n",
    "            grib2_variables = list(ds.data_vars.keys()) if hasattr(ds, 'data_vars') else []\n",
    "            \n",
    "            # Add metadata\n",
    "            df['source_file'] = Path(grib2_path).name\n",
    "            df['ingestion_timestamp'] = datetime.utcnow()\n",
    "            \n",
    "            # Convert to PyArrow table\n",
    "            table = pa.Table.from_pandas(df)\n",
    "            \n",
    "            # Fix VastDB incompatible types\n",
    "            table = self._fix_vastdb_compatibility(table)\n",
    "            \n",
    "            # Save to parquet\n",
    "            parquet_path = grib2_path.replace('.grib2', '.parquet')\n",
    "            pq.write_table(table, parquet_path)\n",
    "            \n",
    "            file_size = Path(parquet_path).stat().st_size\n",
    "            self.stats['bytes_processed'] += file_size\n",
    "            self.stats['files_processed'] += 1\n",
    "            \n",
    "            processing_end = datetime.utcnow()\n",
    "            processing_duration = (processing_end - processing_start).total_seconds()\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = {\n",
    "                'filename': Path(grib2_path).name,\n",
    "                'source_url': source_url or '',\n",
    "                'source_name': source_name or '',\n",
    "                'file_size_bytes': Path(grib2_path).stat().st_size,\n",
    "                'processing_timestamp': processing_start,\n",
    "                'processing_duration_seconds': processing_duration,\n",
    "                'grib2_variables': str(grib2_variables),  # Convert to string for storage\n",
    "                'record_count': len(df)\n",
    "            }\n",
    "            \n",
    "            logger.debug(f\"Processed {Path(grib2_path).name} to Parquet ({file_size / 1024 / 1024:.1f} MB)\")\n",
    "            return parquet_path, metadata\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process {grib2_path}: {e}\")\n",
    "            self.stats['errors'].append(f\"Processing failed: {grib2_path} - {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def run_ingestion(self, plan: Dict, bucket_name: str, schema_name: str, table_name: str, \n",
    "                     batch_size: int = 10, max_workers: int = 3):\n",
    "        \"\"\"Run the complete ingestion process\"\"\"\n",
    "        \n",
    "        logger.info(f\"🚀 Starting ingestion to {bucket_name}.{schema_name}.{table_name}\")\n",
    "        self.stats['start_time'] = datetime.now()\n",
    "        \n",
    "        # Connect to VastDB\n",
    "        if not self.connect_to_vastdb():\n",
    "            return False\n",
    "        \n",
    "        total_processed = 0\n",
    "        \n",
    "        try:\n",
    "            for source_plan in plan['sources']:\n",
    "                logger.info(f\"📂 Processing source: {source_plan['name']}\")\n",
    "                \n",
    "                # Discover files from this source\n",
    "                direct_files = source_plan.get('direct_files', None)\n",
    "                file_urls = self.discover_files_from_source(\n",
    "                    source_plan['url'], \n",
    "                    source_plan['files_to_download'],\n",
    "                    direct_files\n",
    "                )\n",
    "                \n",
    "                if not file_urls:\n",
    "                    logger.warning(f\"No files found for {source_plan['name']}\")\n",
    "                    continue\n",
    "                \n",
    "                # Process in batches to avoid disk space issues\n",
    "                for i in range(0, len(file_urls), batch_size):\n",
    "                    batch_urls = file_urls[i:i + batch_size]\n",
    "                    batch_id = f\"{source_plan['name']}_batch_{i//batch_size + 1}_{int(datetime.now().timestamp())}\"\n",
    "                    logger.info(f\"🔄 Processing batch {i//batch_size + 1}/{(len(file_urls) + batch_size - 1)//batch_size} ({len(batch_urls)} files)\")\n",
    "                    \n",
    "                    # Download batch with metadata\n",
    "                    downloaded_files = []\n",
    "                    download_metadata = []\n",
    "                    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                        future_to_url = {executor.submit(self.download_file, url, batch_id): url for url in batch_urls}\n",
    "                        \n",
    "                        for future in as_completed(future_to_url):\n",
    "                            result_path, metadata = future.result()\n",
    "                            if result_path and metadata:\n",
    "                                downloaded_files.append(result_path)\n",
    "                                download_metadata.append(metadata)\n",
    "                    \n",
    "                    # Process and ingest batch\n",
    "                    for j, grib2_path in enumerate(downloaded_files):\n",
    "                        download_meta = download_metadata[j] if j < len(download_metadata) else {}\n",
    "                        \n",
    "                        # Convert to Parquet with metadata\n",
    "                        parquet_path, processing_meta = self.process_grib2_to_parquet(\n",
    "                            grib2_path, \n",
    "                            download_meta.get('source_url', ''),\n",
    "                            source_plan['name']\n",
    "                        )\n",
    "                        \n",
    "                        if parquet_path and processing_meta:\n",
    "                            # Combine download and processing metadata\n",
    "                            combined_metadata = {**download_meta, **processing_meta}\n",
    "                            combined_metadata['main_table_name'] = table_name\n",
    "                            combined_metadata['batch_id'] = batch_id\n",
    "                            \n",
    "                            # Ingest to VastDB with metadata\n",
    "                            if self.ingest_to_vastdb(parquet_path, bucket_name, schema_name, table_name, combined_metadata):\n",
    "                                total_processed += 1\n",
    "                            \n",
    "                            # Clean up files immediately\n",
    "                            self.cleanup_files([grib2_path, parquet_path])\n",
    "                        else:\n",
    "                            # Clean up failed GRIB2 file\n",
    "                            self.cleanup_files([grib2_path])\n",
    "                    \n",
    "                    # Progress update\n",
    "                    elapsed = datetime.now() - self.stats['start_time']\n",
    "                    gb_downloaded = self.stats['bytes_downloaded'] / 1024 / 1024 / 1024\n",
    "                    \n",
    "                    logger.info(f\"📊 Progress: {total_processed} files ingested, {gb_downloaded:.2f} GB downloaded, {elapsed}\")\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"🛑 Process interrupted by user\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Unexpected error: {e}\")\n",
    "        \n",
    "        # Final statistics\n",
    "        self._print_final_stats()\n",
    "        return True\n",
    "    \n",
    "    def _print_final_stats(self):\n",
    "        \"\"\"Print final ingestion statistics\"\"\"\n",
    "        elapsed = datetime.now() - self.stats['start_time']\n",
    "        gb_downloaded = self.stats['bytes_downloaded'] / 1024 / 1024 / 1024\n",
    "        gb_processed = self.stats['bytes_processed'] / 1024 / 1024 / 1024\n",
    "        \n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"📈 FINAL INGESTION STATISTICS\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(f\"Files discovered: {self.stats['files_discovered']}\")\n",
    "        logger.info(f\"Files downloaded: {self.stats['files_downloaded']}\")\n",
    "        logger.info(f\"Files processed: {self.stats['files_processed']}\")\n",
    "        logger.info(f\"Files ingested: {self.stats['files_ingested']}\")\n",
    "        logger.info(f\"Data downloaded: {gb_downloaded:.2f} GB\")\n",
    "        logger.info(f\"Data processed: {gb_processed:.2f} GB\")\n",
    "        logger.info(f\"Total time: {elapsed}\")\n",
    "        logger.info(f\"Download rate: {gb_downloaded / elapsed.total_seconds() * 60:.1f} MB/min\")\n",
    "        logger.info(f\"Errors: {len(self.stats['errors'])}\")\n",
    "        \n",
    "        if self.stats['errors']:\n",
    "            logger.info(\"Recent errors:\")\n",
    "            for error in self.stats['errors'][-5:]:  # Show last 5 errors\n",
    "                logger.info(f\"  - {error}\")\n",
    "\n",
    "\n",
    "def get_user_configuration(vastdb_config: Dict = None, db_config: Dict = None) -> Tuple[Dict, Dict, float]:\n",
    "    \"\"\"Get configuration from user input, parameters, and environment variables\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🌦️  GRIB2 BULK DATA INGESTER FOR VASTDB\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Data size selection\n",
    "    print(\"\\n📊 How much data would you like to ingest?\")\n",
    "    size_options = {\n",
    "        '1': 1.0,      # 1 GB\n",
    "        '2': 10.0,     # 10 GB  \n",
    "        '3': 100.0,    # 100 GB\n",
    "        '4': 1000.0,   # 1 TB\n",
    "        '5': 10000.0   # 10 TB\n",
    "    }\n",
    "    \n",
    "    print(\"1. 1 GB   (Quick test)\")\n",
    "    print(\"2. 10 GB  (Small dataset)\")\n",
    "    print(\"3. 100 GB (Medium dataset)\")\n",
    "    print(\"4. 1 TB   (Large dataset)\")\n",
    "    print(\"5. 10 TB  (Very large dataset)\")\n",
    "    print(\"6. Custom amount\")\n",
    "    \n",
    "    choice = input(\"\\nSelect option (1-6): \").strip()\n",
    "    \n",
    "    if choice in size_options:\n",
    "        target_size_gb = size_options[choice]\n",
    "    elif choice == '6':\n",
    "        target_size_gb = float(input(\"Enter size in GB: \"))\n",
    "    else:\n",
    "        print(\"Invalid choice, defaulting to 1 GB\")\n",
    "        target_size_gb = 1.0\n",
    "    \n",
    "    # VastDB configuration from passed config or environment variables\n",
    "    print(f\"\\n🗄️  Loading VastDB Configuration\")\n",
    "    \n",
    "    if vastdb_config:\n",
    "        print(\"✓ Using provided VastDB configuration\")\n",
    "        final_vastdb_config = vastdb_config.copy()\n",
    "    else:\n",
    "        print(\"Loading from environment variables...\")\n",
    "        final_vastdb_config = {}\n",
    "        \n",
    "        # Required environment variables for VastDB connection\n",
    "        vastdb_env_vars = {\n",
    "            'VASTDB_ENDPOINT': 'endpoint',\n",
    "            'VASTDB_ACCESS_KEY': 'access_key', \n",
    "            'VASTDB_SECRET_KEY': 'secret_key'\n",
    "        }\n",
    "        \n",
    "        missing_vastdb_vars = []\n",
    "        for var_name, key in vastdb_env_vars.items():\n",
    "            value = os.getenv(var_name)\n",
    "            if not value:\n",
    "                missing_vastdb_vars.append(var_name)\n",
    "            else:\n",
    "                final_vastdb_config[key] = value\n",
    "        \n",
    "        if missing_vastdb_vars:\n",
    "            print(f\"\\n❌ Missing required VastDB environment variables:\")\n",
    "            for var in missing_vastdb_vars:\n",
    "                print(f\"   - {var}\")\n",
    "            print(f\"\\nPlease set these environment variables or pass vastdb_config to main().\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    # Database configuration from passed config or environment variables\n",
    "    print(f\"\\n🗃️  Loading Database Configuration\")\n",
    "    \n",
    "    if db_config:\n",
    "        print(\"✓ Using provided database configuration\")\n",
    "        final_db_config = db_config.copy()\n",
    "        \n",
    "        # Validate required keys\n",
    "        required_db_keys = ['bucket_name', 'schema_name', 'table_name']\n",
    "        missing_db_keys = [key for key in required_db_keys if key not in final_db_config]\n",
    "        \n",
    "        if missing_db_keys:\n",
    "            print(f\"\\n❌ Missing required database config keys: {missing_db_keys}\")\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        print(\"Loading from environment variables...\")\n",
    "        final_db_config = {}\n",
    "        db_env_vars = {\n",
    "            'VASTDB_BUCKET': 'bucket_name',\n",
    "            'VASTDB_SCHEMA': 'schema_name',\n",
    "            'VASTDB_TABLE': 'table_name'\n",
    "        }\n",
    "        \n",
    "        missing_db_vars = []\n",
    "        for var_name, key in db_env_vars.items():\n",
    "            value = os.getenv(var_name)\n",
    "            if not value:\n",
    "                missing_db_vars.append(var_name)\n",
    "            else:\n",
    "                final_db_config[key] = value\n",
    "        \n",
    "        if missing_db_vars:\n",
    "            print(f\"\\n❌ Missing required database environment variables:\")\n",
    "            for var in missing_db_vars:\n",
    "                print(f\"   - {var}\")\n",
    "            print(f\"\\nExample:\")\n",
    "            print(f\"export VASTDB_BUCKET='your-bucket-name'\")\n",
    "            print(f\"export VASTDB_SCHEMA='your-schema-name'\")\n",
    "            print(f\"export VASTDB_TABLE='your-table-name'\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    print(f\"✓ Endpoint: {final_vastdb_config['endpoint']}\")\n",
    "    print(f\"✓ Target: {final_db_config['bucket_name']}.{final_db_config['schema_name']}.{final_db_config['table_name']}\")\n",
    "    print(f\"✓ Data Volume: {target_size_gb} GB\")\n",
    "    \n",
    "    return final_vastdb_config, final_db_config, target_size_gb\n",
    "\n",
    "\n",
    "def test_basic_functionality(vastdb_config: Dict = None, db_config: Dict = None):\n",
    "    \"\"\"Test basic functionality with a simple manual approach\"\"\"\n",
    "    print(\"\\n🧪 TESTING MODE - Manual GRIB2 Download\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Test with a known working GRIB2 file\n",
    "    test_url = \"https://dd.weather.gc.ca/model_gem_global/15km/grib2/lat_lon/00/000/CMC_glb_ABSV_ISBL_200_latlon.15x.15_2024121100_P000.grib2\"\n",
    "    \n",
    "    try:\n",
    "        import tempfile\n",
    "        import requests\n",
    "        \n",
    "        session = requests.Session()\n",
    "        session.headers.update({'User-Agent': 'GRIB2-Test/1.0'})\n",
    "        \n",
    "        print(f\"Testing download from: {test_url}\")\n",
    "        \n",
    "        # Test download\n",
    "        response = session.head(test_url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            print(\"✓ Test file is accessible\")\n",
    "            \n",
    "            # Get VastDB config from parameter or environment\n",
    "            if vastdb_config:\n",
    "                print(\"✓ Using provided VastDB configuration\")\n",
    "                final_vastdb_config = vastdb_config.copy()\n",
    "            else:\n",
    "                print(\"Loading VastDB configuration from environment variables...\")\n",
    "                final_vastdb_config = {}\n",
    "                \n",
    "                vastdb_env_vars = {\n",
    "                    'VASTDB_ENDPOINT': 'endpoint',\n",
    "                    'VASTDB_ACCESS_KEY': 'access_key', \n",
    "                    'VASTDB_SECRET_KEY': 'secret_key'\n",
    "                }\n",
    "                \n",
    "                missing_vars = []\n",
    "                for var_name, key in vastdb_env_vars.items():\n",
    "                    value = os.getenv(var_name)\n",
    "                    if not value:\n",
    "                        missing_vars.append(var_name)\n",
    "                    else:\n",
    "                        final_vastdb_config[key] = value\n",
    "                \n",
    "                if missing_vars:\n",
    "                    print(f\"❌ Missing environment variables: {missing_vars}\")\n",
    "                    return\n",
    "            \n",
    "            # Get database config from parameter or environment\n",
    "            if db_config:\n",
    "                print(\"✓ Using provided database configuration\")\n",
    "                final_db_config = db_config.copy()\n",
    "                \n",
    "                # Validate required keys\n",
    "                required_db_keys = ['bucket_name', 'schema_name', 'table_name']\n",
    "                missing_db_keys = [key for key in required_db_keys if key not in final_db_config]\n",
    "                \n",
    "                if missing_db_keys:\n",
    "                    print(f\"❌ Missing required database config keys: {missing_db_keys}\")\n",
    "                    return\n",
    "            else:\n",
    "                print(\"Loading database configuration from environment variables...\")\n",
    "                final_db_config = {}\n",
    "                db_env_vars = {\n",
    "                    'VASTDB_BUCKET': 'bucket_name',\n",
    "                    'VASTDB_SCHEMA': 'schema_name',\n",
    "                    'VASTDB_TABLE': 'table_name'\n",
    "                }\n",
    "                \n",
    "                missing_db_vars = []\n",
    "                for var_name, key in db_env_vars.items():\n",
    "                    value = os.getenv(var_name)\n",
    "                    if not value:\n",
    "                        missing_db_vars.append(var_name)\n",
    "                    else:\n",
    "                        final_db_config[key] = value\n",
    "                \n",
    "                if missing_db_vars:\n",
    "                    print(f\"❌ Missing database environment variables: {missing_db_vars}\")\n",
    "                    return\n",
    "            \n",
    "            # Create test ingester\n",
    "            ingester = GRIB2DataIngester(final_vastdb_config)\n",
    "            \n",
    "            if ingester.connect_to_vastdb():\n",
    "                print(\"\\n✓ VastDB connection successful\")\n",
    "                \n",
    "                # Download test file\n",
    "                temp_dir = Path(tempfile.mkdtemp())\n",
    "                test_file = temp_dir / \"test.grib2\"\n",
    "                \n",
    "                print(f\"Downloading test file...\")\n",
    "                response = session.get(test_url, stream=True)\n",
    "                with open(test_file, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                \n",
    "                print(f\"✓ Downloaded {test_file.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "                \n",
    "                # Process to parquet\n",
    "                parquet_file = ingester.process_grib2_to_parquet(str(test_file))\n",
    "                if parquet_file:\n",
    "                    print(\"✓ Converted to Parquet\")\n",
    "                    \n",
    "                    # Ingest to VastDB\n",
    "                    success = ingester.ingest_to_vastdb(\n",
    "                        parquet_file,\n",
    "                        final_db_config['bucket_name'],\n",
    "                        final_db_config['schema_name'], \n",
    "                        final_db_config['table_name']\n",
    "                    )\n",
    "                    \n",
    "                    if success:\n",
    "                        print(\"✅ Test ingestion successful!\")\n",
    "                        print(f\"Data is now in {final_db_config['bucket_name']}.{final_db_config['schema_name']}.{final_db_config['table_name']}\")\n",
    "                    else:\n",
    "                        print(\"❌ Test ingestion failed\")\n",
    "                \n",
    "                # Cleanup\n",
    "                import shutil\n",
    "                shutil.rmtree(temp_dir)\n",
    "            else:\n",
    "                print(\"❌ VastDB connection failed\")\n",
    "        else:\n",
    "            print(f\"❌ Test file not accessible: HTTP {response.status_code}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Test failed: {e}\")\n",
    "\n",
    "\n",
    "def main(vastdb_config: Dict = None, db_config: Dict = None, target_size_gb: float = None):\n",
    "    \"\"\"\n",
    "    Main application\n",
    "    \n",
    "    Args:\n",
    "        vastdb_config: Dict with keys 'endpoint', 'access_key', 'secret_key'\n",
    "        db_config: Dict with keys 'bucket_name', 'schema_name', 'table_name'\n",
    "        target_size_gb: Target data size in GB (if None, will prompt user)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🌦️  GRIB2 BULK DATA INGESTER FOR VASTDB\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # If configs are provided, skip mode selection and go straight to ingestion\n",
    "    if vastdb_config and db_config:\n",
    "        print(\"✓ Configuration provided - running in programmatic mode\")\n",
    "        mode = '1'  # Full bulk ingestion\n",
    "    else:\n",
    "        print(\"Choose mode:\")\n",
    "        print(\"1. Full bulk ingestion (discover and download multiple files)\")\n",
    "        print(\"2. Test mode (single file test)\")\n",
    "        \n",
    "        mode = input(\"Select mode (1-2): \").strip()\n",
    "    \n",
    "    if mode == '2':\n",
    "        test_basic_functionality(vastdb_config, db_config)\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Get user configuration (or use provided configs)\n",
    "        if vastdb_config and db_config and target_size_gb is not None:\n",
    "            final_vastdb_config = vastdb_config.copy()\n",
    "            final_db_config = db_config.copy()\n",
    "            final_target_size_gb = target_size_gb\n",
    "            \n",
    "            print(f\"✓ Using provided configuration:\")\n",
    "            print(f\"  - Endpoint: {final_vastdb_config['endpoint']}\")\n",
    "            print(f\"  - Target: {final_db_config['bucket_name']}.{final_db_config['schema_name']}.{final_db_config['table_name']}\")\n",
    "            print(f\"  - Data Volume: {final_target_size_gb} GB\")\n",
    "        else:\n",
    "            final_vastdb_config, final_db_config, final_target_size_gb = get_user_configuration(vastdb_config, db_config)\n",
    "        \n",
    "        # Initialize ingester\n",
    "        ingester = GRIB2DataIngester(final_vastdb_config)\n",
    "        \n",
    "        # Discover available sources\n",
    "        print(f\"\\n🔍 Discovering GRIB2 data sources...\")\n",
    "        available_sources = ingester.discover_grib2_sources()\n",
    "        \n",
    "        if not available_sources:\n",
    "            print(\"❌ No available GRIB2 sources found!\")\n",
    "            print(\"💡 Try test mode (option 2) to test with a single known file\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n✅ Found {len(available_sources)} available sources\")\n",
    "        \n",
    "        # Build download plan\n",
    "        plan = ingester.build_download_plan(available_sources, final_target_size_gb)\n",
    "        \n",
    "        if plan['total_estimated_files'] == 0:\n",
    "            print(\"❌ Could not build a download plan!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n📋 Download Plan:\")\n",
    "        print(f\"Target: {final_target_size_gb} GB\")\n",
    "        print(f\"Estimated: {plan['total_estimated_files']} files, {plan['total_estimated_size_gb']:.1f} GB\")\n",
    "        \n",
    "        for source in plan['sources']:\n",
    "            print(f\"  - {source['name']}: {source['files_to_download']} files ({source['estimated_size_gb']:.1f} GB)\")\n",
    "        \n",
    "        # Confirm with user (unless running programmatically)\n",
    "        if not (vastdb_config and db_config and target_size_gb is not None):\n",
    "            confirm = input(f\"\\n🚀 Start ingestion to {final_db_config['bucket_name']}.{final_db_config['schema_name']}.{final_db_config['table_name']}? (y/n): \")\n",
    "            if confirm.lower() != 'y':\n",
    "                print(\"Cancelled by user\")\n",
    "                return\n",
    "        else:\n",
    "            print(f\"\\n🚀 Starting automatic ingestion to {final_db_config['bucket_name']}.{final_db_config['schema_name']}.{final_db_config['table_name']}\")\n",
    "        \n",
    "        # Run ingestion\n",
    "        success = ingester.run_ingestion(\n",
    "            plan, \n",
    "            final_db_config['bucket_name'],\n",
    "            final_db_config['schema_name'], \n",
    "            final_db_config['table_name'],\n",
    "            batch_size=5,      # Process 5 files at a time\n",
    "            max_workers=2      # Conservative parallelism\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            print(\"\\n✅ Ingestion completed!\")\n",
    "            print(f\"Data is now available in {final_db_config['bucket_name']}.{final_db_config['schema_name']}.{final_db_config['table_name']}\")\n",
    "        else:\n",
    "            print(\"\\n❌ Ingestion failed!\")\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n🛑 Process interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Unexpected error: {e}\")\n",
    "    finally:\n",
    "        print(\"\\n👋 Goodbye!\")\n",
    "\n",
    "\n",
    "# Example usage functions for easy integration\n",
    "def run_with_config(endpoint: str, access_key: str, secret_key: str, \n",
    "                   bucket_name: str, schema_name: str, table_name: str,\n",
    "                   target_size_gb: float = 1.0):\n",
    "    \"\"\"\n",
    "    Convenience function to run ingestion with explicit parameters\n",
    "    \n",
    "    Example:\n",
    "        run_with_config(\n",
    "            endpoint='https://my-vastdb.com',\n",
    "            access_key='your_access_key',\n",
    "            secret_key='your_secret_key',\n",
    "            bucket_name='weather_data',\n",
    "            schema_name='grib2',\n",
    "            table_name='meteorological_data',\n",
    "            target_size_gb=10.0\n",
    "        )\n",
    "    \"\"\"\n",
    "    vastdb_config = {\n",
    "        'endpoint': endpoint,\n",
    "        'access_key': access_key,\n",
    "        'secret_key': secret_key\n",
    "    }\n",
    "    \n",
    "    db_config = {\n",
    "        'bucket_name': bucket_name,\n",
    "        'schema_name': schema_name,\n",
    "        'table_name': table_name\n",
    "    }\n",
    "    \n",
    "    main(vastdb_config, db_config, target_size_gb)\n",
    "\n",
    "\n",
    "def run_test_mode(endpoint: str, access_key: str, secret_key: str,\n",
    "                 bucket_name: str, schema_name: str, table_name: str):\n",
    "    \"\"\"\n",
    "    Convenience function to run test mode with explicit parameters\n",
    "    \n",
    "    Example:\n",
    "        run_test_mode(\n",
    "            endpoint='https://my-vastdb.com',\n",
    "            access_key='your_access_key', \n",
    "            secret_key='your_secret_key',\n",
    "            bucket_name='weather_data',\n",
    "            schema_name='grib2',\n",
    "            table_name='test_data'\n",
    "        )\n",
    "    \"\"\"\n",
    "    vastdb_config = {\n",
    "        'endpoint': endpoint,\n",
    "        'access_key': access_key,\n",
    "        'secret_key': secret_key\n",
    "    }\n",
    "    \n",
    "    db_config = {\n",
    "        'bucket_name': bucket_name,\n",
    "        'schema_name': schema_name,\n",
    "        'table_name': table_name\n",
    "    }\n",
    "    \n",
    "    test_basic_functionality(vastdb_config, db_config)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6897e803-6146-4f31-8bc3-bcc59253d713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 10:28:25,233 - INFO - 🔍 Discovering available GRIB2 data sources...\n",
      "2025-06-04 10:28:25,233 - INFO - Checking GEM Global 66km...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "🌦️  GRIB2 BULK DATA INGESTER FOR VASTDB\n",
      "============================================================\n",
      "✓ Configuration provided - running in programmatic mode\n",
      "✓ Using provided configuration:\n",
      "  - Endpoint: http://172.200.202.2:80\n",
      "  - Target: csnowdb.grib2.grib2_data\n",
      "  - Data Volume: 0.01 GB\n",
      "\n",
      "🔍 Discovering GRIB2 data sources...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 10:28:25,479 - INFO - ✗ GEM Global 66km: HTTP 404\n",
      "2025-06-04 10:28:25,480 - INFO - Checking GEM Global 15km...\n",
      "2025-06-04 10:28:27,270 - INFO - ✓ GEM Global 15km: ~100 files (~0.2 GB)\n",
      "2025-06-04 10:28:27,271 - INFO - Checking HRDPS Continental 2.5km...\n",
      "2025-06-04 10:28:29,810 - INFO - ✓ HRDPS Continental 2.5km: ~200 files (~1.6 GB)\n",
      "2025-06-04 10:28:29,811 - INFO - Checking Sample GRIB2 Files...\n",
      "2025-06-04 10:28:30,012 - INFO - ✗ Sample GRIB2 Files: HTTP 404\n",
      "2025-06-04 10:28:30,013 - INFO - 📋 Building download plan for 0.01 GB of data...\n",
      "2025-06-04 10:28:30,013 - INFO - 📊 Plan: 4 files, 0.0 GB from 1 sources\n",
      "2025-06-04 10:28:30,014 - INFO - 🚀 Starting ingestion to csnowdb.grib2.grib2_data\n",
      "2025-06-04 10:28:30,020 - INFO - VAST version: (5, 2, 0, 131)\n",
      "2025-06-04 10:28:30,021 - INFO - ✓ Connected to VastDB\n",
      "2025-06-04 10:28:30,021 - INFO - 📂 Processing source: GEM Global 15km\n",
      "2025-06-04 10:28:30,021 - INFO - 🔍 Discovering files from https://dd.weather.gc.ca/model_gem_global/15km/grib2/lat_lon...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Found 2 available sources\n",
      "\n",
      "📋 Download Plan:\n",
      "Target: 0.01 GB\n",
      "Estimated: 4 files, 0.0 GB\n",
      "  - GEM Global 15km: 4 files (0.0 GB)\n",
      "\n",
      "🚀 Starting automatic ingestion to csnowdb.grib2.grib2_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 10:28:32,695 - INFO - 📁 Discovered 4 file URLs\n",
      "2025-06-04 10:28:32,696 - INFO - 🔄 Processing batch 1/1 (4 files)\n",
      "2025-06-04 10:28:35,887 - INFO - Created table: grib2_data\n",
      "2025-06-04 10:28:39,309 - INFO - Created table: grib2_data_ingestion_log\n",
      "2025-06-04 10:28:39,321 - INFO - Created ingestion metadata table: grib2_data_ingestion_log\n",
      "2025-06-04 10:28:55,245 - INFO - 📊 Progress: 4 files ingested, 0.01 GB downloaded, 0:00:25.231317\n",
      "2025-06-04 10:28:55,246 - INFO - ============================================================\n",
      "2025-06-04 10:28:55,246 - INFO - 📈 FINAL INGESTION STATISTICS\n",
      "2025-06-04 10:28:55,246 - INFO - ============================================================\n",
      "2025-06-04 10:28:55,247 - INFO - Files discovered: 4\n",
      "2025-06-04 10:28:55,247 - INFO - Files downloaded: 4\n",
      "2025-06-04 10:28:55,247 - INFO - Files processed: 4\n",
      "2025-06-04 10:28:55,248 - INFO - Files ingested: 4\n",
      "2025-06-04 10:28:55,248 - INFO - Data downloaded: 0.01 GB\n",
      "2025-06-04 10:28:55,249 - INFO - Data processed: 0.02 GB\n",
      "2025-06-04 10:28:55,249 - INFO - Total time: 0:00:25.232018\n",
      "2025-06-04 10:28:55,249 - INFO - Download rate: 0.0 MB/min\n",
      "2025-06-04 10:28:55,250 - INFO - Errors: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Ingestion completed!\n",
      "Data is now available in csnowdb.grib2.grib2_data\n",
      "\n",
      "👋 Goodbye!\n"
     ]
    }
   ],
   "source": [
    "run_with_config(\n",
    "    endpoint=endpoint,\n",
    "    access_key=access_key,\n",
    "    secret_key=secret_key,\n",
    "    bucket_name='csnowdb',\n",
    "    schema_name='grib2',\n",
    "    table_name='grib2_data',\n",
    "    target_size_gb=0.01\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa8f769-ed0b-4cd3-a402-91d36b0b3157",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
