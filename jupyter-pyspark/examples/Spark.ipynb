{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10fed33d-be68-4c1b-a045-30f853f1e6a5",
   "metadata": {},
   "source": [
    "# Spark Connectivity Test to Vast DB and Vast S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3485e1-107b-481a-9fb5-a0825e6a06b0",
   "metadata": {},
   "source": [
    "## Load Endpoint Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7655dbdd-e541-429e-b653-ace158721613",
   "metadata": {},
   "source": [
    "These environment variables have been set when your docker container was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204f19da-1de4-4e54-95e5-be3409dfaea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "DOCKER_HOST_OR_IP=se-var-vastdb-ingest\n",
      "---\n",
      "VASTDB_ENDPOINT=http://172.200.201.1:80\n",
      "VASTDB_ACCESS_KEY=DYF8\n",
      "VASTDB_SECRET_KEY=****Usuu\n",
      "---\n",
      "S3_ENDPOINT=http://172.200.201.1:80\n",
      "S3_ACCESS_KEY=DYF8\n",
      "S3_SECRET_KEY=****Usuu\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DOCKER_HOST_OR_IP = os.getenv(\"DOCKER_HOST_OR_IP\")\n",
    "\n",
    "VASTDB_ENDPOINT = os.getenv(\"VASTDB_ENDPOINT\")\n",
    "VASTDB_ACCESS_KEY = os.getenv(\"VASTDB_ACCESS_KEY\")\n",
    "VASTDB_SECRET_KEY = os.getenv(\"VASTDB_SECRET_KEY\")\n",
    "\n",
    "S3_ENDPOINT = os.getenv(\"S3A_ENDPOINT\")\n",
    "S3_ACCESS_KEY = os.getenv(\"S3A_ACCESS_KEY\")\n",
    "S3_SECRET_KEY = os.getenv(\"S3A_SECRET_KEY\")\n",
    "\n",
    "print(f\"\"\"\n",
    "---\n",
    "DOCKER_HOST_OR_IP={DOCKER_HOST_OR_IP}\n",
    "---\n",
    "VASTDB_ENDPOINT={VASTDB_ENDPOINT}\n",
    "VASTDB_ACCESS_KEY={VASTDB_ACCESS_KEY[-4:]}\n",
    "VASTDB_SECRET_KEY=****{VASTDB_SECRET_KEY[-4:]}\n",
    "---\n",
    "S3_ENDPOINT={S3_ENDPOINT}\n",
    "S3_ACCESS_KEY={S3_ACCESS_KEY[-4:]}\n",
    "S3_SECRET_KEY=****{VASTDB_SECRET_KEY[-4:]}\n",
    "---\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf87ca-92d2-46a1-abda-aa59b9a3f4ab",
   "metadata": {},
   "source": [
    "## Specify other Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6596d81a-04e2-4007-9777-bf406c6ea190",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_APPLICATION_NAME='Spark Demo'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e356c0a-a13c-48e8-bf5f-0603a6953e31",
   "metadata": {},
   "source": [
    "## Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7385eee4-6b08-4be0-b6de-d95b64dac1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark successfully loaded\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setAll([\n",
    "    (\"spark.driver.host\", socket.gethostbyname(socket.gethostname())),\n",
    "     # VASTDB\n",
    "    (\"spark.sql.catalog.ndb\", 'spark.sql.catalog.ndb.VastCatalog'),\n",
    "    (\"spark.ndb.endpoint\", VASTDB_ENDPOINT),\n",
    "    (\"spark.ndb.data_endpoints\", VASTDB_ENDPOINT),\n",
    "    (\"spark.ndb.access_key_id\", VASTDB_ACCESS_KEY),\n",
    "    (\"spark.ndb.secret_access_key\", VASTDB_SECRET_KEY),\n",
    "    (\"spark.driver.extraClassPath\", '/usr/local/spark/jars/spark3-vast-3.4.1-f93839bfa38a/*'),\n",
    "    (\"spark.executor.extraClassPath\", '/usr/local/spark/jars/spark3-vast-3.4.1-f93839bfa38a/*'),\n",
    "    (\"spark.sql.extensions\", 'ndb.NDBSparkSessionExtension'),\n",
    "    # ICEBERG\n",
    "    (\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\"),\n",
    "    (\"spark.sql.catalog.iceberg.type\", \"hive\"),\n",
    "    (\"spark.sql.catalog.iceberg.uri\", f\"thrift://{DOCKER_HOST_OR_IP}:9083\"),\n",
    "    # S3A\n",
    "    (\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\"),\n",
    "    (\"fs.s3a.endpoint\", S3_ENDPOINT),\n",
    "    (\"fs.s3a.access.key\", S3_ACCESS_KEY),\n",
    "    (\"fs.s3a.secret.key\", S3_SECRET_KEY),\n",
    "    (\"fs.s3a.endpoint.region\", \"vast\"),\n",
    "    (\"fs.s3a.connection.ssl.enabled\", \"false\"),\n",
    "    # HIVE METASTORE\n",
    "    (\"hive.metastore.uris\", f\"thrift://{DOCKER_HOST_OR_IP}:9083\")\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(SPARK_APPLICATION_NAME) \\\n",
    "    .config(conf=conf) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"DEBUG\")\n",
    "\n",
    "print(\"Spark successfully loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "315482c2-5efe-486a-81b4-6c6ed5f592fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"None\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Demo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=Spark Demo>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560205d4-086a-4b62-88c3-12abc17f4870",
   "metadata": {},
   "source": [
    "## Connect to Vast DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b62420-0e4a-49ac-9fd9-23a06d7129e2",
   "metadata": {},
   "source": [
    "### Specify Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cb0485c-3101-498c-989c-a665310a6767",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'csnowdb'\n",
    "DATABASE_SCHEMA='twitter_import'\n",
    "TABLE_NAME='twitter_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d19a23a-9a00-4c6a-a058-dd03029fbc14",
   "metadata": {},
   "source": [
    "### Connect and run a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d383066-7f4b-4c7a-a88a-0da64a532f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DATABASE_FULLNAME='ndb.csnowdb.twitter_import'\n"
     ]
    }
   ],
   "source": [
    "DATABASE_FULLNAME = f\"ndb.{BUCKET_NAME}.{DATABASE_SCHEMA}\"\n",
    "\n",
    "spark.sql(f\"create schema if not exists {DATABASE_FULLNAME}\")\n",
    "\n",
    "# Set the database name so we don't have to fully qualify all object names\n",
    "# https://spark.apache.org/docs/latest/sql-ref-syntax-ddl-usedb.html\n",
    "# spark.sql(f\"use {DATABASE_FULLNAME}\")\n",
    "\n",
    "print(f\"Using {DATABASE_FULLNAME=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebb52e94-288a-4013-b682-f5777febb204",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "  *\n",
    "FROM \n",
    "  {DATABASE_FULLNAME}.{TABLE_NAME}\n",
    "WHERE\n",
    "  created_at in (\n",
    "    SELECT \n",
    "      MAX(created_at) latest_created_at\n",
    "    FROM \n",
    "      {DATABASE_FULLNAME}.{TABLE_NAME}\n",
    "  )\n",
    "ORDER BY id\n",
    "LIMIT 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8c3314d-6c21-4690-a9a4-0f054d1e684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|          created_at|                  id|              id_str|                text|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|2024/11/02 19:30:...|-9199571530558861734|-9199571530558861734|eager to see how ...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bfd2d2-f01e-4ee0-afe7-805f335005b7",
   "metadata": {},
   "source": [
    "## Connect to Vast S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f205e126-6049-4e66-8041-5b9da5f70874",
   "metadata": {},
   "source": [
    "### Specify Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47b0e94a-a1f5-489f-81ef-7bd88c3283c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|   namespace|\n",
      "+------------+\n",
      "|     default|\n",
      "|social_media|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES in iceberg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b90ada70-a394-4688-97f5-db794870dd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+\n",
      "|   namespace|   tableName|isTemporary|\n",
      "+------------+------------+-----------+\n",
      "|social_media|twitter_data|      false|\n",
      "+------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES in iceberg.social_media\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "869dcd04-0d1e-4fcd-b479-6a0c6fc839da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|          created_at|              string|   null|\n",
      "|                  id|              bigint|   null|\n",
      "|              id_str|              string|   null|\n",
      "|                text|              string|   null|\n",
      "|                    |                    |       |\n",
      "|  # Metadata Columns|                    |       |\n",
      "|            _spec_id|                 int|       |\n",
      "|          _partition|            struct<>|       |\n",
      "|               _file|              string|       |\n",
      "|                _pos|              bigint|       |\n",
      "|            _deleted|             boolean|       |\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|                Name|iceberg.social_me...|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Location|s3a://csnow-bucke...|       |\n",
      "|            Provider|             iceberg|       |\n",
      "|    Table Properties|[current-snapshot...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED iceberg.social_media.twitter_data\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a983014-77df-4968-bf95-a3f9010db1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|          created_at|                  id|              id_str|                text|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                   1|                   1|                   1|                Yay!|\n",
      "|2024/11/02 16:15:...|-1659833618039073746|-1659833618039073746|impressed with ho...|\n",
      "|2024/11/02 16:15:...| 6512778609215664072| 6512778609215664072|motivated by how ...|\n",
      "|2024/11/02 16:15:...| 8555625018826030287| 8555625018826030287|motivated by how ...|\n",
      "|2024/11/02 16:15:...| 8614357796825723058| 8614357796825723058|finally got how w...|\n",
      "|2024/11/02 16:15:...| 3520836456545401736| 3520836456545401736|totally in love w...|\n",
      "|2024/11/02 16:15:...| 4371851260384616541| 4371851260384616541|can't believe how...|\n",
      "|2024/11/02 16:15:...| -671018250379665149| -671018250379665149|impressed with ho...|\n",
      "|2024/11/02 16:15:...| 1433228438804513320| 1433228438804513320|motivated by how ...|\n",
      "|2024/11/02 16:15:...| 8758305974311999734| 8758305974311999734|blown away by how...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    * \n",
    "FROM \n",
    "    iceberg.social_media.twitter_data\n",
    "\"\"\")\n",
    "result_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006c9b99-3f45-4e1b-9a6e-61ecab06b3fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
