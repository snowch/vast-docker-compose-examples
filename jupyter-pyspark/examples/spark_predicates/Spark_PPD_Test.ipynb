{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10fed33d-be68-4c1b-a045-30f853f1e6a5",
   "metadata": {},
   "source": [
    "# Spark Connectivity Test to Vast DB and Vast S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3485e1-107b-481a-9fb5-a0825e6a06b0",
   "metadata": {},
   "source": [
    "## Load Endpoint Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7655dbdd-e541-429e-b653-ace158721613",
   "metadata": {},
   "source": [
    "These environment variables have been set when your docker container was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204f19da-1de4-4e54-95e5-be3409dfaea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "DOCKER_HOST_OR_IP=10.143.11.241\n",
      "---\n",
      "VASTDB_ENDPOINT=http://172.200.204.2:80\n",
      "VASTDB_ACCESS_KEY=QXN5\n",
      "VASTDB_SECRET_KEY=****oLGr\n",
      "VASTDB_TWITTER_INGEST_BUCKET=csnow-db\n",
      "VASTDB_TWITTER_INGEST_SCHEMA=social_media\n",
      "VASTDB_TWITTER_INGEST_TABLE=tweets\n",
      "---\n",
      "S3_ENDPOINT=http://172.200.204.2:80\n",
      "S3_ACCESS_KEY=QXN5\n",
      "S3_SECRET_KEY=****oLGr\n",
      "S3A_ICEBERG_URI=s3a://csnow-bucket/iceberg/\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DOCKER_HOST_OR_IP = os.getenv(\"DOCKER_HOST_OR_IP\")\n",
    "\n",
    "VASTDB_ENDPOINT = os.getenv(\"VASTDB_ENDPOINT\")\n",
    "VASTDB_ACCESS_KEY = os.getenv(\"VASTDB_ACCESS_KEY\")\n",
    "VASTDB_SECRET_KEY = os.getenv(\"VASTDB_SECRET_KEY\")\n",
    "\n",
    "VASTDB_TWITTER_INGEST_BUCKET = os.getenv(\"VASTDB_TWITTER_INGEST_BUCKET\")\n",
    "VASTDB_TWITTER_INGEST_SCHEMA = os.getenv(\"VASTDB_TWITTER_INGEST_SCHEMA\")\n",
    "VASTDB_TWITTER_INGEST_TABLE = os.getenv(\"VASTDB_TWITTER_INGEST_TABLE\")\n",
    "\n",
    "S3_ENDPOINT = os.getenv(\"S3A_ENDPOINT\")\n",
    "S3_ACCESS_KEY = os.getenv(\"S3A_ACCESS_KEY\")\n",
    "S3_SECRET_KEY = os.getenv(\"S3A_SECRET_KEY\")\n",
    "\n",
    "S3A_ICEBERG_URI = os.getenv(\"S3A_ICEBERG_URI\")\n",
    "\n",
    "print(f\"\"\"\n",
    "---\n",
    "DOCKER_HOST_OR_IP={DOCKER_HOST_OR_IP}\n",
    "---\n",
    "VASTDB_ENDPOINT={VASTDB_ENDPOINT}\n",
    "VASTDB_ACCESS_KEY={VASTDB_ACCESS_KEY[-4:]}\n",
    "VASTDB_SECRET_KEY=****{VASTDB_SECRET_KEY[-4:]}\n",
    "VASTDB_TWITTER_INGEST_BUCKET={VASTDB_TWITTER_INGEST_BUCKET}\n",
    "VASTDB_TWITTER_INGEST_SCHEMA={VASTDB_TWITTER_INGEST_SCHEMA}\n",
    "VASTDB_TWITTER_INGEST_TABLE={VASTDB_TWITTER_INGEST_TABLE}\n",
    "---\n",
    "S3_ENDPOINT={S3_ENDPOINT}\n",
    "S3_ACCESS_KEY={S3_ACCESS_KEY[-4:]}\n",
    "S3_SECRET_KEY=****{VASTDB_SECRET_KEY[-4:]}\n",
    "S3A_ICEBERG_URI={S3A_ICEBERG_URI}\n",
    "---\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf87ca-92d2-46a1-abda-aa59b9a3f4ab",
   "metadata": {},
   "source": [
    "## Specify other Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6596d81a-04e2-4007-9777-bf406c6ea190",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_APPLICATION_NAME='Spark Demo'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e356c0a-a13c-48e8-bf5f-0603a6953e31",
   "metadata": {},
   "source": [
    "## Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7385eee4-6b08-4be0-b6de-d95b64dac1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark successfully loaded\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "pd.set_option(\"max_colwidth\", 150)\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setAll([\n",
    "    (\"spark.driver.host\", socket.gethostbyname(socket.gethostname())),\n",
    "    (\"spark.sql.execution.arrow.pyspark.enabled\", \"false\"),\n",
    "     # VASTDB\n",
    "    (\"spark.sql.catalog.ndb\", 'spark.sql.catalog.ndb.VastCatalog'),\n",
    "    (\"spark.ndb.endpoint\", VASTDB_ENDPOINT),\n",
    "    (\"spark.ndb.data_endpoints\", VASTDB_ENDPOINT),\n",
    "    (\"spark.ndb.access_key_id\", VASTDB_ACCESS_KEY),\n",
    "    (\"spark.ndb.secret_access_key\", VASTDB_SECRET_KEY),\n",
    "    (\"spark.driver.extraClassPath\", '/usr/local/spark/jars/spark3-vast-3.4.1-f93839bfa38a/*'),\n",
    "    (\"spark.executor.extraClassPath\", '/usr/local/spark/jars/spark3-vast-3.4.1-f93839bfa38a/*'),\n",
    "    (\"spark.sql.extensions\", 'ndb.NDBSparkSessionExtension'),\n",
    "    # ICEBERG\n",
    "    (\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\"),\n",
    "    (\"spark.sql.catalog.iceberg.type\", \"hive\"),\n",
    "    (\"spark.sql.catalog.iceberg.uri\", f\"thrift://{DOCKER_HOST_OR_IP}:9083\"),\n",
    "    # S3A\n",
    "    (\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\"),\n",
    "    (\"fs.s3a.endpoint\", S3_ENDPOINT),\n",
    "    (\"fs.s3a.access.key\", S3_ACCESS_KEY),\n",
    "    (\"fs.s3a.secret.key\", S3_SECRET_KEY),\n",
    "    (\"fs.s3a.endpoint.region\", \"vast\"),\n",
    "    (\"fs.s3a.connection.ssl.enabled\", \"false\"),\n",
    "    # Hive\n",
    "    (\"hive.metastore.uris\", f\"thrift://{DOCKER_HOST_OR_IP}:9083\"),\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(SPARK_APPLICATION_NAME) \\\n",
    "    .config(conf=conf) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"DEBUG\")\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set logging for a specific class/package\n",
    "logging.getLogger(\"com.example.HelloWorldCatalog\").setLevel(logging.DEBUG)\n",
    "\n",
    "print(\"Spark successfully loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2d558bb-2071-440a-b7b4-f86eda0c8e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"None\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Demo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f21e5321510>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a34aeee-fb5b-4e2f-9e0b-b75d91c57bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicate Pushdown Analysis Matrix\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| Column          | Col Type   | Predicate Type       | Query                          | Pushed Down Predicates                             |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| id              | long       | equality             | id = 1                         | [id = 1]                                           |\n",
      "| fruit           | string     | equality             | fruit = 'apple'                | [fruit = 'apple']                                  |\n",
      "| price           | double     | greater than         | price > 15                     | [price > 15.0]                                     |\n",
      "| date            | date       | equality             | date = '2023-10-26'            | [date = 19656]                                     |\n",
      "| is_ripe         | boolean    | equality             | is_ripe = true                 | [is_ripe = true]                                   |\n",
      "| fruit           | string     | LIKE                 | fruit LIKE 'a%'                |                                                    |\n",
      "| price           | double     | between              | price >= 10 AND price <= 20    | [price >= 10.0], [price <= 20.0]                   |\n",
      "| date            | date       | year function        | year(date) = 2023              |                                                    |\n",
      "| is_ripe         | boolean    | NOT                  | NOT is_ripe                    | [is_ripe > true, is_ripe < true]                   |\n",
      "| fruit           | string     | substr match (LIKE)  | fruit LIKE '%app%'             |                                                    |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import date\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "import re\n",
    "\n",
    "def create_test_table(spark):\n",
    "    table_name = \"ndb.`csnow-db`.nyt.ppdtest\"\n",
    "    data = [(1, \"apple\", 10.5, date(2023, 10, 26), True),\n",
    "            (2, \"banana\", 20.0, date(2023, 10, 27), False),\n",
    "            (3, \"orange\", 15.7, date(2023, 10, 28), True)]\n",
    "    df = spark.createDataFrame(data, [\"id\", \"fruit\", \"price\", \"date\", \"is_ripe\"])\n",
    "    df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "    return table_name\n",
    "\n",
    "def get_explain_plan(df):\n",
    "    buffer = io.StringIO()\n",
    "    with redirect_stdout(buffer):\n",
    "        df.explain(True)\n",
    "    return buffer.getvalue()\n",
    "\n",
    "def extract_individual_predicates(text):\n",
    "    match = re.search(r\"pushed_down_predicates=\\[(.*?)\\](?:}| RuntimeFilters)\", text, re.DOTALL)\n",
    "    if match:\n",
    "        predicates_str = match.group(1)\n",
    "        if not predicates_str:\n",
    "            return []\n",
    "        predicates = []\n",
    "        current_predicate = \"\"\n",
    "        bracket_level = 0\n",
    "        in_quotes = False\n",
    "        for char in predicates_str:\n",
    "            if char == '[':\n",
    "                bracket_level += 1\n",
    "                current_predicate += char\n",
    "            elif char == ']':\n",
    "                bracket_level -= 1\n",
    "                current_predicate += char\n",
    "            elif char == \"'\" or char == '\"':\n",
    "                in_quotes = not in_quotes\n",
    "                current_predicate += char\n",
    "            elif char == ',' and bracket_level == 0 and not in_quotes:\n",
    "                predicates.append(current_predicate.strip())\n",
    "                current_predicate = \"\"\n",
    "            else:\n",
    "                current_predicate += char\n",
    "        predicates.append(current_predicate.strip())\n",
    "        if predicates:\n",
    "            return predicates[1:]\n",
    "        return []\n",
    "    return []\n",
    "\n",
    "def check_pushdown(explain_str, query):\n",
    "    if \"Filter\" in explain_str and \"VastScan\" in explain_str:\n",
    "        predicates_list = extract_individual_predicates(explain_str)\n",
    "        if predicates_list:\n",
    "            if \"year\" in query or \"LIKE\" in query:\n",
    "                return any(\"IS NOT NULL\" in pred for pred in predicates_list)\n",
    "            elif \"NOT is_ripe\" in query:\n",
    "                return any(\"is_ripe IS NOT NULL\" in pred for pred in predicates_list) and any(\"is_ripe > true, is_ripe < true\" in pred for pred in predicates_list)\n",
    "            elif \"LIKE '%app%'\" in query:\n",
    "                return any(\"like\" in pred.lower() and \"'%app%'\" in pred.lower() for pred in predicates_list)\n",
    "            else:\n",
    "                cleaned_query = query.replace(\"'\", \"\").replace(\" = \", \" = \").lower()\n",
    "                query_terms = cleaned_query.split(\"and\")\n",
    "                return all(any(term.strip().lower() in pred.lower() for pred in predicates_list) for term in query_terms)\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def analyze_predicate_pushdown(spark, table_name):\n",
    "    queries = [(\"id = 1\", \"id\", \"equality\"),\n",
    "               (\"fruit = 'apple'\", \"fruit\", \"equality\"),\n",
    "               (\"price > 15\", \"price\", \"greater than\"),\n",
    "               (\"date = '2023-10-26'\", \"date\", \"equality\"),\n",
    "               (\"is_ripe = true\", \"is_ripe\", \"equality\"),\n",
    "               (\"fruit LIKE 'a%'\", \"fruit\", \"LIKE\"),\n",
    "               (\"price >= 10 AND price <= 20\", \"price\", \"between\"),\n",
    "               (\"year(date) = 2023\", \"date\", \"year function\"),\n",
    "               (\"NOT is_ripe\", \"is_ripe\", \"NOT\"),\n",
    "               (\"fruit LIKE '%app%'\", \"fruit\", \"substr match (LIKE)\")]\n",
    "\n",
    "    results = []\n",
    "    df_schema = spark.table(\"ndb.`csnow-db`.nyt.ppdtest\").schema\n",
    "\n",
    "    for query, column, predicate_type in queries:\n",
    "        df = spark.sql(f\"SELECT * FROM {table_name} WHERE {query}\")\n",
    "        explain_str = get_explain_plan(df)\n",
    "        pushed_down = check_pushdown(explain_str, query)\n",
    "        predicates = extract_individual_predicates(explain_str)\n",
    "        try:\n",
    "            column_type = df_schema[column].dataType.typeName()\n",
    "        except KeyError:\n",
    "            column_type = \"N/A\"\n",
    "        results.append((column, column_type, predicate_type, query, predicates, pushed_down))\n",
    "\n",
    "    return results\n",
    "\n",
    "def print_matrix(results):\n",
    "    print(\"\\nPredicate Pushdown Analysis Matrix\")\n",
    "    print(\"-\" * 141)\n",
    "    print(f\"| {'Column':<15} | {'Col Type':<10} | {'Predicate Type':<20} | {'Query':<30} | {'Pushed Down Predicates':<50} |\")\n",
    "    print(\"-\" * 141)\n",
    "    for column, column_type, predicate_type, query, predicates, pushed_down in results:\n",
    "        predicates_str = \", \".join(map(str, predicates))\n",
    "        print(f\"| {column:<15} | {column_type:<10} | {predicate_type:<20} | {query:<30} | {predicates_str:<50} |\")\n",
    "    print(\"-\" * 141)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    table_name = create_test_table(spark)\n",
    "    results = analyze_predicate_pushdown(spark, table_name)\n",
    "    print_matrix(results)\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78851499-0d76-460c-8c47-f1e116a2e84e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
