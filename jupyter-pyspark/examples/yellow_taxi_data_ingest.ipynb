{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10fed33d-be68-4c1b-a045-30f853f1e6a5",
   "metadata": {},
   "source": [
    "# Import Yellow Taxi Data - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb93c071-3c28-4c32-b782-2512f38eec58",
   "metadata": {},
   "source": [
    "Import yellow taxi data into Vast S3 and Vast DB.\n",
    "\n",
    "The schema changes over time, so we need to evolve the schema:\n",
    "- currently only new fields are added during loading\n",
    "- datatype changes are handled before loading the parquet into VastDB\n",
    "- column renames are handled before loading the parquet into VastDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb0d111-afe5-4d0b-a5f0-a6fa1a8967d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install --quiet vastdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dab003-4d9f-47b0-8181-63797260867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from io import StringIO\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "from pyarrow import csv as pa_csv\n",
    "import requests\n",
    "\n",
    "# Custom imports for VASTDB\n",
    "import vastdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ed0c37-64da-4137-abfa-cdeb705dd1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "VASTDB_ENDPOINT = os.getenv(\"VASTDB_ENDPOINT\")\n",
    "VASTDB_ACCESS_KEY = os.getenv(\"VASTDB_ACCESS_KEY\")\n",
    "VASTDB_SECRET_KEY = os.getenv(\"VASTDB_SECRET_KEY\")\n",
    "\n",
    "VASTDB_TWITTER_INGEST_BUCKET = os.getenv(\"VASTDB_TWITTER_INGEST_BUCKET\")\n",
    "VASTDB_TWITTER_INGEST_SCHEMA = os.getenv(\"VASTDB_TWITTER_INGEST_SCHEMA\")\n",
    "\n",
    "S3_ENDPOINT = os.getenv(\"S3A_ENDPOINT\")\n",
    "S3_ACCESS_KEY = os.getenv(\"S3A_ACCESS_KEY\")\n",
    "S3_SECRET_KEY = os.getenv(\"S3A_SECRET_KEY\")\n",
    "S3_BUCKET = os.getenv(\"S3A_BUCKET\")\n",
    "\n",
    "###### SET THIS ######\n",
    "VASTDB_TWITTER_INGEST_TABLE = 'YELLOW_TRIP_DATA'\n",
    "###### SET THIS ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40282e3-33b5-4443-a757-adb9c627a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "---\n",
    "VASTDB_ENDPOINT={VASTDB_ENDPOINT}\n",
    "VASTDB_ACCESS_KEY={VASTDB_ACCESS_KEY[-4:]}\n",
    "VASTDB_SECRET_KEY=****{VASTDB_SECRET_KEY[-4:]}\n",
    "VASTDB_TWITTER_INGEST_BUCKET={VASTDB_TWITTER_INGEST_BUCKET}\n",
    "VASTDB_TWITTER_INGEST_SCHEMA={VASTDB_TWITTER_INGEST_SCHEMA}\n",
    "VASTDB_TWITTER_INGEST_TABLE={VASTDB_TWITTER_INGEST_TABLE}\n",
    "---\n",
    "S3_ENDPOINT={S3_ENDPOINT}\n",
    "S3_ACCESS_KEY={S3_ACCESS_KEY[-4:]}\n",
    "S3_SECRET_KEY=****{VASTDB_SECRET_KEY[-4:]}\n",
    "S3_BUCKET={S3_BUCKET}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954ed1e1-b4ae-4522-b6b8-5bd1e5f67283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet(file_path):\n",
    "    \"\"\"Reads Parquet data from a file.\"\"\"\n",
    "    try:\n",
    "        return pq.read_table(file_path)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error reading Parquet file: {e}\") from e\n",
    "\n",
    "def connect_to_vastdb(endpoint, access_key, secret_key):\n",
    "    \"\"\"Connects to VastDB.\"\"\"\n",
    "    try:\n",
    "        session = vastdb.connect(endpoint=endpoint, access=access_key, secret=secret_key)\n",
    "        print(\"Connected to VastDB\")\n",
    "        return session\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to connect to VastDB: {e}\") from e\n",
    "\n",
    "def write_to_vastdb(session, bucket_name, schema_name, table_name, pa_table):\n",
    "    \"\"\"Writes data to VastDB.\"\"\"\n",
    "    with session.transaction() as tx:\n",
    "        bucket = tx.bucket(bucket_name)\n",
    "        schema = bucket.schema(schema_name, fail_if_missing=False) or bucket.create_schema(schema_name)\n",
    "\n",
    "        table = schema.table(table_name, fail_if_missing=False) or schema.create_table(table_name, pa_table.schema)\n",
    "\n",
    "        columns_to_add = get_columns_to_add(table.arrow_schema, pa_table.schema)\n",
    "        for column in columns_to_add:\n",
    "            table.add_column(column)\n",
    "            \n",
    "        try:\n",
    "            # Attempt to insert data\n",
    "            table.insert(pa_table)\n",
    "            print(f\"Inserted parquet into {table}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during table.insert: {e}\")\n",
    "            \n",
    "            # Perform schema diff if insert fails\n",
    "            perform_schema_diff(table.arrow_schema, pa_table.schema)\n",
    "            raise  # Re-raise the exception for further handling\n",
    "\n",
    "def perform_schema_diff(existing_schema, new_schema):\n",
    "    \"\"\"Compares two schemas and logs the differences, clarifying which is existing and which is new.\"\"\"\n",
    "    existing_fields = {field.name.lower(): field for field in existing_schema}\n",
    "    new_fields = {field.name.lower(): field for field in new_schema}\n",
    "\n",
    "    print(\"\\nSchema Differences:\")\n",
    "    \n",
    "    # Check for missing fields in the existing schema\n",
    "    missing_in_existing = [field for name, field in new_fields.items() if name not in existing_fields]\n",
    "    if missing_in_existing:\n",
    "        print(\"Fields missing in the existing schema:\")\n",
    "        for field in missing_in_existing:\n",
    "            print(f\"  - {field.name} (new): {field.type}\")\n",
    "    else:\n",
    "        print(\"No fields are missing in the existing schema.\")\n",
    "    \n",
    "    # Check for extra fields in the existing schema\n",
    "    extra_in_existing = [field for name, field in existing_fields.items() if name not in new_fields]\n",
    "    if extra_in_existing:\n",
    "        print(\"Fields present in the existing schema but not in the new schema:\")\n",
    "        for field in extra_in_existing:\n",
    "            print(f\"  - {field.name} (existing): {field.type}\")\n",
    "    else:\n",
    "        print(\"No extra fields in the existing schema.\")\n",
    "    \n",
    "    # Check for type mismatches\n",
    "    type_mismatches = [\n",
    "        (existing_fields[name].name, existing_fields[name].type, new_fields[name].type)\n",
    "        for name in new_fields\n",
    "        if name in existing_fields and existing_fields[name].type != new_fields[name].type\n",
    "    ]\n",
    "    if type_mismatches:\n",
    "        print(\"Type mismatches:\")\n",
    "        for name, existing_type, new_type in type_mismatches:\n",
    "            print(f\"  - {name}: (existing) {existing_type} -> (new) {new_type}\")\n",
    "    else:\n",
    "        print(\"No type mismatches found.\")\n",
    "\n",
    "def import_to_vastdb(session, bucket_name, schema_name, table_name, files_to_import):\n",
    "    with session.transaction() as tx:\n",
    "        bucket = tx.bucket(bucket_name)\n",
    "        schema = bucket.schema(schema_name, fail_if_missing=False) or bucket.create_schema(schema_name)\n",
    "        table = schema.table(table_name, fail_if_missing=False)\n",
    "\n",
    "        if table:\n",
    "            table.import_files(files_to_import=files_to_import)\n",
    "        else:\n",
    "            table = vastdb.util.create_table_from_files(\n",
    "                schema=schema, \n",
    "                table_name=table_name,\n",
    "                parquet_files=files_to_import\n",
    "            )\n",
    "\n",
    "def get_columns_to_add(existing_schema, desired_schema):\n",
    "    \"\"\"Identifies columns to add to an existing schema.\"\"\"\n",
    "    existing_fields = set(existing_schema.names)\n",
    "    desired_fields = set(desired_schema.names)\n",
    "    return [pa.schema([pa.field(name, desired_schema.field(name).type)]) for name in desired_fields - existing_fields]\n",
    "\n",
    "\n",
    "def query_vastdb(session, bucket_name, schema_name, table_name):\n",
    "    \"\"\"Writes data to VastDB.\"\"\"\n",
    "    with session.transaction() as tx:\n",
    "        bucket = tx.bucket(bucket_name)\n",
    "        schema = bucket.schema(schema_name, fail_if_missing=False) or bucket.create_schema(schema_name)\n",
    "        table = schema.table(table_name, fail_if_missing=False) or schema.create_table(table_name, pa_table.schema)\n",
    "\n",
    "        return table.select().read_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f14638-9819-4632-a87e-8a7ba5a9b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to download files\n",
    "def download_file(url):\n",
    "    file_name = os.path.basename(urlparse(url).path)\n",
    "    \n",
    "    # Check if file exists, skip if so\n",
    "    if not os.path.exists(file_name):\n",
    "        # print(f\"Downloading {file_name}...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            with open(file_name, \"wb\") as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            print(f\"Downloaded {file_name}\")\n",
    "        else:\n",
    "            print(f\"Failed to download {file_name}. Status code: {response.status_code}\")\n",
    "    else:\n",
    "        print(f\"{file_name} already exists. Skipping download.\")\n",
    "\n",
    "\n",
    "# Define a function to upload the file to S3\n",
    "def upload_to_s3(file_path, bucket_name, s3_key):\n",
    "    try:\n",
    "        # print(f\"Uploading {file_path} to S3 bucket {bucket_name}...\")\n",
    "        s3_client.upload_file(file_path, bucket_name, s3_key)\n",
    "        print(f\"File uploaded to s3://{bucket_name}/{s3_key}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {file_path} was not found.\")\n",
    "    except NoCredentialsError:\n",
    "        print(\"Credentials not available.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading file to S3: {e}\")\n",
    "\n",
    "# Define a function to delete the file after processing\n",
    "def delete_file(file_path):\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "        print(f\"Deleted {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a619b-9ae3-45a9-83e6-a6f2d9748f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "def process_parquet(file_path):\n",
    "    \"\"\"Process and transform the Parquet file data.\"\"\"\n",
    "    # Load the Parquet file\n",
    "    pa_table = pq.read_table(file_path)\n",
    "    \n",
    "    # Convert TIMESTAMP[US] to a standard timestamp (nanoseconds)\n",
    "    columns_to_convert = ['tpep_pickup_datetime', 'tpep_dropoff_datetime']\n",
    "    for column in columns_to_convert:\n",
    "        if column in pa_table.column_names:\n",
    "            # Convert the column to a timestamp with nanosecond precision\n",
    "            pa_table = pa_table.set_column(\n",
    "                pa_table.column_names.index(column),\n",
    "                column,\n",
    "                pc.cast(pa_table[column], pa.timestamp('ns'))\n",
    "            )\n",
    "    \n",
    "    # Handle NULL columns (e.g., 'airport_fee') and rename to 'Airport_fee'\n",
    "    null_column = 'airport_fee'\n",
    "    renamed_column = 'Airport_fee'\n",
    "    if null_column in pa_table.column_names:\n",
    "        # Replace NULL values with 0.0 and rename the column\n",
    "        pa_table = pa_table.set_column(\n",
    "            pa_table.column_names.index(null_column),\n",
    "            renamed_column,\n",
    "            pc.if_else(pc.is_null(pa_table[null_column]), pa.scalar(0.0, pa.float64()), pa_table[null_column])\n",
    "        )\n",
    "    \n",
    "    # Handle casting of passenger_count to int64\n",
    "    passenger_count_column = 'passenger_count'\n",
    "    if passenger_count_column in pa_table.column_names:\n",
    "        # Cast 'passenger_count' to int64\n",
    "        pa_table = pa_table.set_column(\n",
    "            pa_table.column_names.index(passenger_count_column),\n",
    "            passenger_count_column,\n",
    "            pc.cast(pa_table[passenger_count_column], pa.int64())\n",
    "        )\n",
    "    \n",
    "    # Handle casting of RatecodeID to int32\n",
    "    ratecode_column = 'RatecodeID'\n",
    "    if ratecode_column in pa_table.column_names:\n",
    "        # Cast 'RatecodeID' to int32\n",
    "        pa_table = pa_table.set_column(\n",
    "            pa_table.column_names.index(ratecode_column),\n",
    "            ratecode_column,\n",
    "            pc.cast(pa_table[ratecode_column], pa.int32())\n",
    "        )\n",
    "    \n",
    "    # Handle LARGE_STRING columns (e.g., 'store_and_fwd_flag')\n",
    "    string_columns_to_cast = ['store_and_fwd_flag']\n",
    "    for column in string_columns_to_cast:\n",
    "        if column in pa_table.column_names:\n",
    "            # Convert the column to a STRING type\n",
    "            pa_table = pa_table.set_column(\n",
    "                pa_table.column_names.index(column),\n",
    "                column,\n",
    "                pc.cast(pa_table[column], pa.string())\n",
    "            )\n",
    "    \n",
    "    # Return the processed table\n",
    "    return pa_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d960a6e-c4ea-45c0-a0ae-04e5a58cd439",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = connect_to_vastdb(VASTDB_ENDPOINT, VASTDB_ACCESS_KEY, VASTDB_SECRET_KEY)\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client(\n",
    "    's3', \n",
    "    region_name='vast',\n",
    "    endpoint_url=S3_ENDPOINT,\n",
    "    aws_access_key_id=S3_ACCESS_KEY,\n",
    "    aws_secret_access_key=S3_SECRET_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb66cf8-0074-454f-980b-f757bc44954c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download and process all files from 2019-01 to 2024-08\n",
    "for year in range(2019, 2025):\n",
    "    for month in range(1, 13):\n",
    "        if year == 2024 and month > 8:\n",
    "            break  # Only process until August 2024\n",
    "        \n",
    "        # Format month to always have two digits\n",
    "        month_str = f\"{month:02d}\"\n",
    "        file_url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year}-{month_str}.parquet\"\n",
    "        \n",
    "        # Download the file\n",
    "        download_file(file_url)\n",
    "        \n",
    "        # Process the downloaded file\n",
    "        file_name = f\"yellow_tripdata_{year}-{month_str}.parquet\"\n",
    "        pa_table = process_parquet(file_name)\n",
    "\n",
    "        # Upload the file to S3\n",
    "        s3_key = f\"yellow_tripdata/yellow_tripdata_{year}-{month_str}.parquet\"\n",
    "        upload_to_s3(file_name, S3_BUCKET, s3_key)\n",
    "        \n",
    "        # Write the processed data to VASTDB (custom logic)\n",
    "        write_to_vastdb(session=session,\n",
    "                        bucket_name=VASTDB_TWITTER_INGEST_BUCKET, \n",
    "                        schema_name=VASTDB_TWITTER_INGEST_SCHEMA, \n",
    "                        table_name=VASTDB_TWITTER_INGEST_TABLE, \n",
    "                        pa_table=pa_table)\n",
    "\n",
    "        # import_to_vastdb(\n",
    "        #     session=session,\n",
    "        #     bucket_name=VASTDB_TWITTER_INGEST_BUCKET, \n",
    "        #     schema_name=VASTDB_TWITTER_INGEST_SCHEMA, \n",
    "        #     table_name=VASTDB_TWITTER_INGEST_TABLE, \n",
    "        #     files_to_import=[f\"/{s3_key}\"]\n",
    "        # )\n",
    "        \n",
    "        # Delete the file after processing\n",
    "        delete_file(file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8088cb7-8aec-4907-abfd-9abc7dd4072c",
   "metadata": {},
   "source": [
    "## Check Parquet file for non-compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0434a133-5c19-4829-a50a-e7c6a22054ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install --upgrade --quiet git+https://github.com/snowch/vastdb_parq_schema_file.git --use-pep517"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2727ffbb-2b53-4c7d-a96e-0aa09990b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This can run a long time because the second check verifies the datasize in each row\n",
    "# ! parquet_checker yellow_tripdata_2019-01.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f039f2-fe90-4b90-9f65-0bd8489e621c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
