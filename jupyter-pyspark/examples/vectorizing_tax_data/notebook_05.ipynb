{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "916c8cd5-422c-4601-bf15-83ae576ed57a",
   "metadata": {},
   "source": [
    "# Setting Up Vector DB & Indexing Profile Embeddings\n",
    "\n",
    "**Purpose**:\n",
    "  1. Initialize a connection to the chosen vector database.\n",
    "     *MVP Example*: Using ChromaDB for its ease of setup for local development.\n",
    "  2. Create a dedicated collection (or index) within the database to store the\n",
    "     taxpayer profile embeddings.\n",
    "  3. Load the taxpayer profile embeddings and their corresponding Taxpayer IDs\n",
    "     generated in [Notebook 04](./notebook_04.ipynb).\n",
    "  4. Ingest (add/index) these embeddings into the vector database collection,\n",
    "     linking each vector to its Taxpayer ID.\n",
    "  5. Perform basic verification to confirm that the embeddings have been indexed.\n",
    "\n",
    "**Why a Vector Database?**\n",
    "  Vector databases are specialized for storing, indexing, and efficiently querying\n",
    "  high-dimensional vector embeddings based on similarity. This allows us to quickly\n",
    "  find taxpayers with similar profiles (represented by nearby vectors) without\n",
    "  comparing a query vector to every single stored vector.\n",
    "\n",
    "**Prerequisites**:\n",
    "  - Successful completion of [Notebook 04](./notebook_04.ipynb).\n",
    "  - Existence of the embeddings file ('embeddings.npy').\n",
    "  - Existence of the corresponding Taxpayer IDs file ('embedding_ids.csv').\n",
    "  - Vector Database client library installed (e.g., `pip install chromadb`).\n",
    "\n",
    "**Outputs**:\n",
    "  - An initialized vector database client.\n",
    "  - A populated collection within the vector database.\n",
    "  - Confirmation that the embeddings are indexed.\n",
    "\n",
    "**Next Step**:\n",
    "  [Notebook 06](./notebook_06.ipynb) will use the indexed embeddings to perform similarity searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f01b329-6906-4716-a074-515fbae5f7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00795846-9e3b-4607-baef-4a73f055ca01",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eae1f32-5fc7-4503-8b4c-26bf9b8cc8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook 05: Setting Up Vector DB & Indexing Profile Embeddings\n",
      "--------------------------------------------------\n",
      "Using ChromaDB as the vector database example.\n",
      "Loading embeddings from: ./data/processed/embeddings.npy\n",
      "Loading IDs from: ./data/processed/embedding_ids.csv\n",
      "ChromaDB persistence directory: ./vector_db/chroma_persist\n",
      "Collection Name: taxpayer_profiles\n",
      "Distance Metric: cosine\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import chromadb # MVP Example Client\n",
    "from chromadb.utils import embedding_functions # If needed, but we have precomputed embeddings\n",
    "\n",
    "# --- Configuration ---\n",
    "PROCESSED_DATA_DIR = './data/processed' # Directory containing N04 output\n",
    "VECTOR_DB_DIR = './vector_db' # Directory to persist ChromaDB data\n",
    "\n",
    "EMBEDDINGS_INPUT_FILE = os.path.join(PROCESSED_DATA_DIR, 'embeddings.npy')\n",
    "IDS_INPUT_FILE = os.path.join(PROCESSED_DATA_DIR, 'embedding_ids.csv')\n",
    "\n",
    "# ChromaDB specific config\n",
    "CHROMA_PERSIST_DIR = os.path.join(VECTOR_DB_DIR, 'chroma_persist')\n",
    "COLLECTION_NAME = \"taxpayer_profiles\"\n",
    "# Distance metric: 'cosine' or 'l2' (Euclidean) are common.\n",
    "# Cosine is good for orientation, L2 for magnitude differences.\n",
    "# Since features were scaled with StandardScaler (not normalized to unit length),\n",
    "# L2 might be slightly more conventional, but Cosine is also widely used. Let's use Cosine.\n",
    "DISTANCE_METRIC = \"cosine\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(CHROMA_PERSIST_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Notebook 05: Setting Up Vector DB & Indexing Profile Embeddings\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Using ChromaDB as the vector database example.\")\n",
    "print(f\"Loading embeddings from: {EMBEDDINGS_INPUT_FILE}\")\n",
    "print(f\"Loading IDs from: {IDS_INPUT_FILE}\")\n",
    "print(f\"ChromaDB persistence directory: {CHROMA_PERSIST_DIR}\")\n",
    "print(f\"Collection Name: {COLLECTION_NAME}\")\n",
    "print(f\"Distance Metric: {DISTANCE_METRIC}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f3eb32-a8b4-437e-84c0-1fe895c56467",
   "metadata": {},
   "source": [
    "## Load Embeddings and IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64feaf33-6767-47e9-973b-820dbdf31263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded embeddings array with shape: (4900, 28)\n",
      "Successfully loaded 4900 Taxpayer IDs.\n",
      "Validation: Number of embeddings matches number of IDs.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    embeddings = np.load(EMBEDDINGS_INPUT_FILE)\n",
    "    print(f\"Successfully loaded embeddings array with shape: {embeddings.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Embeddings file not found at {EMBEDDINGS_INPUT_FILE}.\")\n",
    "    print(\"Please ensure Notebook 04 was run successfully and saved the file.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading embeddings file: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    ids_df = pd.read_csv(IDS_INPUT_FILE)\n",
    "    # Ensure IDs are strings, as required by ChromaDB\n",
    "    id_list = ids_df['Taxpayer ID'].astype(str).tolist()\n",
    "    print(f\"Successfully loaded {len(id_list)} Taxpayer IDs.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: IDs file not found at {IDS_INPUT_FILE}.\")\n",
    "    print(\"Please ensure Notebook 04 was run successfully and saved the file.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading IDs file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Validation\n",
    "if embeddings.shape[0] != len(id_list):\n",
    "    print(f\"ERROR: Mismatch between number of embeddings ({embeddings.shape[0]}) and number of IDs ({len(id_list)})!\")\n",
    "    raise ValueError(\"Mismatch in length between embeddings and IDs.\")\n",
    "else:\n",
    "    print(\"Validation: Number of embeddings matches number of IDs.\")\n",
    "\n",
    "# Convert embeddings to list of lists for ChromaDB ingestion if needed (depends on client version)\n",
    "# Current versions often handle numpy arrays directly, but tolist() is safe.\n",
    "embeddings_list = embeddings.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7275400f-d26c-480b-a926-7ce4a2eb1703",
   "metadata": {},
   "source": [
    "## Initialize Vector Database Client (ChromaDB Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "361b68d1-dd63-4ffd-affc-49b9de5fadab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB Persistent Client initialized. Data will be stored in: ./vector_db/chroma_persist\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Using PersistentClient to save data to disk\n",
    "    # Use chromadb.Client() for a purely in-memory instance (data lost on restart)\n",
    "    client = chromadb.PersistentClient(path=CHROMA_PERSIST_DIR)\n",
    "    print(f\"ChromaDB Persistent Client initialized. Data will be stored in: {CHROMA_PERSIST_DIR}\")\n",
    "    # You might want to reset the DB for repeatable runs during development:\n",
    "    # client.reset() # Uncomment carefully - this deletes all collections!\n",
    "    # print(\"ChromaDB client reset (collections deleted).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR initializing ChromaDB client: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff5c8ee-84a4-4b92-ae30-a0ead0dc7951",
   "metadata": {},
   "source": [
    "## Create or Get Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a54a487-7f2f-4639-9b8a-fa8ade02517d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully got or created collection: 'taxpayer_profiles' with distance metric 'cosine'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Using get_or_create_collection is idempotent: creates if not exists, gets if it does.\n",
    "    # Specify the distance metric in the metadata.\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=COLLECTION_NAME,\n",
    "        metadata={\"hnsw:space\": DISTANCE_METRIC} # HNSW is a common index type used by Chroma\n",
    "    )\n",
    "    print(f\"Successfully got or created collection: '{COLLECTION_NAME}' with distance metric '{DISTANCE_METRIC}'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR getting or creating ChromaDB collection '{COLLECTION_NAME}': {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffc1f5a-dfbb-40bf-a688-837d61c60c1e",
   "metadata": {},
   "source": [
    "## Ingest / Index Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9de2d905-89ca-4b65-b444-25f2b65fc91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to add 4900 embeddings to the 'taxpayer_profiles' collection.\n",
      "Adding batch 1/1 (4900 items)...\n",
      "Finished adding all batches.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Preparing to add {len(id_list)} embeddings to the '{COLLECTION_NAME}' collection.\")\n",
    "\n",
    "# Note: For very large datasets (millions+), consider batching the `add` operation\n",
    "# for better performance and memory management. For this MVP scale (~5k), one batch is fine.\n",
    "BATCH_SIZE = 5000 # Example batch size\n",
    "num_batches = (len(id_list) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "try:\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * BATCH_SIZE\n",
    "        end_idx = min((i + 1) * BATCH_SIZE, len(id_list))\n",
    "\n",
    "        batch_ids = id_list[start_idx:end_idx]\n",
    "        batch_embeddings = embeddings_list[start_idx:end_idx]\n",
    "\n",
    "        print(f\"Adding batch {i+1}/{num_batches} ({len(batch_ids)} items)...\")\n",
    "\n",
    "        collection.add(\n",
    "            embeddings=batch_embeddings,\n",
    "            ids=batch_ids\n",
    "            # metadatas=[{\"id\": tid} for tid in batch_ids] # Optional: Add metadata if needed\n",
    "        )\n",
    "    print(\"Finished adding all batches.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR adding embeddings to collection '{COLLECTION_NAME}': {e}\")\n",
    "    # Consider potential issues: duplicate IDs, incorrect embedding dimensions, DB connection errors.\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd3a1bc-f498-4ba2-93a8-3a42f53e85ff",
   "metadata": {},
   "source": [
    "## Verify Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a3926e-e377-4e40-8ed9-b3f267a7b00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification: Collection 'taxpayer_profiles' now contains 4900 items.\n",
      "Verification successful: Item count matches expected count.\n",
      "Successfully retrieved sample item with ID: TXP_0A78B11C9A.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Get the total count of items in the collection\n",
    "    count = collection.count()\n",
    "    print(f\"Verification: Collection '{COLLECTION_NAME}' now contains {count} items.\")\n",
    "\n",
    "    # Assert that the count matches the number of items we intended to add\n",
    "    expected_count = len(id_list)\n",
    "    assert count == expected_count, f\"Count mismatch! Expected {expected_count}, found {count}.\"\n",
    "    print(\"Verification successful: Item count matches expected count.\")\n",
    "\n",
    "    # Optional: Retrieve a sample item to ensure it was added correctly\n",
    "    if count > 0:\n",
    "        sample_id = id_list[0]\n",
    "        retrieved_item = collection.get(ids=[sample_id], include=['embeddings']) # Can also include 'metadatas', 'documents'\n",
    "        if retrieved_item and retrieved_item['ids'] and retrieved_item['ids'][0] == sample_id:\n",
    "             print(f\"Successfully retrieved sample item with ID: {sample_id}.\")\n",
    "             # print(f\"  Retrieved Embedding (first 5 dims): {retrieved_item['embeddings'][0][:5]}...\") # Optional check\n",
    "        else:\n",
    "             print(f\"Warning: Could not retrieve sample item with ID: {sample_id}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during verification: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c6899d-4069-4766-87cf-c9452008ab55",
   "metadata": {},
   "source": [
    "## Notes on Other Vector Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84e527ad-68e1-47dd-ab86-b729b16f461f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "\n",
      "This notebook used ChromaDB as an example due to its simplicity for local MVPs.\n",
      "If using other vector databases, the specific steps in sections 2-5 would change:\n",
      "\n",
      "* **Milvus:**\n",
      "    * Requires connection setup (`pymilvus.connections.connect`, `utility.has_collection`).\n",
      "    * Need to define a schema for the collection (specifying fields like ID, embedding vector, dimension, index type like HNSW or IVF_FLAT, metric type).\n",
      "    * Create the collection using the schema (`Collection(...)`).\n",
      "    * Insert data typically as lists of entities or Pandas DataFrames matching the schema (`collection.insert(...)`).\n",
      "    * Explicitly create an index on the vector field (`collection.create_index(...)`) and load the collection (`collection.load()`) before querying.\n",
      "\n",
      "* **Pinecone:**\n",
      "    * Initialize connection using API key and environment (`pinecone.init(...)`).\n",
      "    * Create an index (`pinecone.create_index(...)`) specifying name, dimension, metric, and pod configuration.\n",
      "    * Connect to the index (`pinecone.Index(...)`).\n",
      "    * Upsert (add or update) data in batches, providing tuples of (ID, vector, optional_metadata) (`index.upsert(vectors=...)`).\n",
      "\n",
      "* **Others (Weaviate, Qdrant, pgvector, Cloud Services):**\n",
      "    * Each has its own specific client library, connection methods, schema/collection/index definition process, data insertion API (`.add`, `.upsert`, SQL `INSERT`, etc.), and configuration details (indexing parameters, distance metrics).\n",
      "\n",
      "Refer to the official documentation of your chosen vector database for the exact API calls and procedures. The core concepts (connect, create structure, index data, verify) remain similar.\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 50)\n",
    "print(\"\"\"\n",
    "This notebook used ChromaDB as an example due to its simplicity for local MVPs.\n",
    "If using other vector databases, the specific steps in sections 2-5 would change:\n",
    "\n",
    "* **Milvus:**\n",
    "    * Requires connection setup (`pymilvus.connections.connect`, `utility.has_collection`).\n",
    "    * Need to define a schema for the collection (specifying fields like ID, embedding vector, dimension, index type like HNSW or IVF_FLAT, metric type).\n",
    "    * Create the collection using the schema (`Collection(...)`).\n",
    "    * Insert data typically as lists of entities or Pandas DataFrames matching the schema (`collection.insert(...)`).\n",
    "    * Explicitly create an index on the vector field (`collection.create_index(...)`) and load the collection (`collection.load()`) before querying.\n",
    "\n",
    "* **Pinecone:**\n",
    "    * Initialize connection using API key and environment (`pinecone.init(...)`).\n",
    "    * Create an index (`pinecone.create_index(...)`) specifying name, dimension, metric, and pod configuration.\n",
    "    * Connect to the index (`pinecone.Index(...)`).\n",
    "    * Upsert (add or update) data in batches, providing tuples of (ID, vector, optional_metadata) (`index.upsert(vectors=...)`).\n",
    "\n",
    "* **Others (Weaviate, Qdrant, pgvector, Cloud Services):**\n",
    "    * Each has its own specific client library, connection methods, schema/collection/index definition process, data insertion API (`.add`, `.upsert`, SQL `INSERT`, etc.), and configuration details (indexing parameters, distance metrics).\n",
    "\n",
    "Refer to the official documentation of your chosen vector database for the exact API calls and procedures. The core concepts (connect, create structure, index data, verify) remain similar.\n",
    "\"\"\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b6ee04-d814-44f1-8c09-f2f570ff8774",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43206afa-7b6c-43c8-ae08-14057bce7f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook 05 finished.\n",
      "Successfully set up the vector database using ChromaDB (persisted at ./vector_db/chroma_persist).\n",
      "  - Created or connected to the 'taxpayer_profiles' collection.\n",
      "  - Indexed 4900 taxpayer profile embeddings.\n",
      "  - Verified the item count in the collection.\n",
      "\n",
      "The vector database is now populated and ready for similarity queries.\n"
     ]
    }
   ],
   "source": [
    "print(\"Notebook 05 finished.\")\n",
    "print(f\"Successfully set up the vector database using ChromaDB (persisted at {CHROMA_PERSIST_DIR}).\")\n",
    "print(f\"  - Created or connected to the '{COLLECTION_NAME}' collection.\")\n",
    "print(f\"  - Indexed {count} taxpayer profile embeddings.\")\n",
    "print(\"  - Verified the item count in the collection.\")\n",
    "print(\"\\nThe vector database is now populated and ready for similarity queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f329b8c-15e5-44c9-bf04-8992c9ab4999",
   "metadata": {},
   "source": [
    "Proceed to [Notebook 06](./notebook_06.ipynb): Querying for Cross-Source Similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f644da50-4c42-4c32-b1cb-164dfdf68607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
