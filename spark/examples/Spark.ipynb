{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10fed33d-be68-4c1b-a045-30f853f1e6a5",
   "metadata": {},
   "source": [
    "# Spark Connectivity Test to Vast DB and Vast S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3485e1-107b-481a-9fb5-a0825e6a06b0",
   "metadata": {},
   "source": [
    "## Load Endpoint Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7655dbdd-e541-429e-b653-ace158721613",
   "metadata": {},
   "source": [
    "These environment variables have been set when your docker container was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204f19da-1de4-4e54-95e5-be3409dfaea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "VASTDB_ENDPOINT=http://172.200.201.1:80\n",
      "VASTDB_ACCESS_KEY=DYF8\n",
      "VASTDB_SECRET_KEY=****Usuu\n",
      "---\n",
      "S3_ENDPOINT=http://172.200.201.1:80\n",
      "S3_ACCESS_KEY=DYF8\n",
      "S3_SECRET_KEY=****Usuu\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "VASTDB_ENDPOINT = os.getenv(\"VASTDB_ENDPOINT\")\n",
    "VASTDB_ACCESS_KEY = os.getenv(\"VASTDB_ACCESS_KEY\")\n",
    "VASTDB_SECRET_KEY = os.getenv(\"VASTDB_SECRET_KEY\")\n",
    "\n",
    "S3_ENDPOINT = os.getenv(\"S3A_ENDPOINT\")\n",
    "S3_ACCESS_KEY = os.getenv(\"S3A_ACCESS_KEY\")\n",
    "S3_SECRET_KEY = os.getenv(\"S3A_SECRET_KEY\")\n",
    "\n",
    "print(f\"\"\"\n",
    "---\n",
    "VASTDB_ENDPOINT={VASTDB_ENDPOINT}\n",
    "VASTDB_ACCESS_KEY={VASTDB_ACCESS_KEY[-4:]}\n",
    "VASTDB_SECRET_KEY=****{VASTDB_SECRET_KEY[-4:]}\n",
    "---\n",
    "S3_ENDPOINT={S3_ENDPOINT}\n",
    "S3_ACCESS_KEY={S3_ACCESS_KEY[-4:]}\n",
    "S3_SECRET_KEY=****{VASTDB_SECRET_KEY[-4:]}\n",
    "---\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf87ca-92d2-46a1-abda-aa59b9a3f4ab",
   "metadata": {},
   "source": [
    "## Specify other Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6596d81a-04e2-4007-9777-bf406c6ea190",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_APPLICATION_NAME='Spark Demo'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e356c0a-a13c-48e8-bf5f-0603a6953e31",
   "metadata": {},
   "source": [
    "## Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7385eee4-6b08-4be0-b6de-d95b64dac1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark successfully loaded\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setAll([\n",
    "    (\"spark.driver.host\", socket.gethostbyname(socket.gethostname())),\n",
    "    (\"spark.ndb.endpoint\", VASTDB_ENDPOINT),\n",
    "    (\"spark.ndb.data_endpoints\", VASTDB_ENDPOINT),\n",
    "    (\"spark.ndb.access_key_id\", VASTDB_ACCESS_KEY),\n",
    "    (\"spark.ndb.secret_access_key\", VASTDB_SECRET_KEY),\n",
    "    (\"spark.driver.extraClassPath\", '/usr/local/spark/jars/spark3-vast-3.4.1-f93839bfa38a/*'),\n",
    "    (\"spark.executor.extraClassPath\", '/usr/local/spark/jars/spark3-vast-3.4.1-f93839bfa38a/*'),\n",
    "    # (\"spark.jars.packages\", \"com.amazonaws:aws-java-sdk:1.12.742,org.apache.hadoop:hadoop-aws:3.3.4,org.apache.hadoop:hadoop-client:3.3.4\"),\n",
    "    (\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\"),\n",
    "    (\"fs.s3a.endpoint\", S3_ENDPOINT),\n",
    "    (\"fs.s3a.access.key\", S3_ACCESS_KEY),\n",
    "    (\"fs.s3a.secret.key\", S3_SECRET_KEY),\n",
    "    (\"fs.s3a.endpoint.region\", \"vast\"),\n",
    "    (\"fs.s3a.connection.ssl.enabled\", \"false\"),\n",
    "    (\"spark.sql.catalog.ndb\", 'spark.sql.catalog.ndb.VastCatalog'),\n",
    "    (\"spark.sql.extensions\", 'ndb.NDBSparkSessionExtension')\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(SPARK_APPLICATION_NAME) \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"DEBUG\")\n",
    "\n",
    "print(\"Spark successfully loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560205d4-086a-4b62-88c3-12abc17f4870",
   "metadata": {},
   "source": [
    "## Connect to Vast DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b62420-0e4a-49ac-9fd9-23a06d7129e2",
   "metadata": {},
   "source": [
    "### Specify Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cb0485c-3101-498c-989c-a665310a6767",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'csnowdb'\n",
    "DATABASE_SCHEMA='twitter_import'\n",
    "TABLE_NAME='twitter_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d19a23a-9a00-4c6a-a058-dd03029fbc14",
   "metadata": {},
   "source": [
    "### Connect and run a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d383066-7f4b-4c7a-a88a-0da64a532f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DATABASE_FULLNAME='ndb.csnowdb.twitter_import'\n"
     ]
    }
   ],
   "source": [
    "DATABASE_FULLNAME = f\"ndb.{BUCKET_NAME}.{DATABASE_SCHEMA}\"\n",
    "\n",
    "spark.sql(f\"create schema if not exists {DATABASE_FULLNAME}\")\n",
    "\n",
    "# Set the database name so we don't have to fully qualify all object names\n",
    "# https://spark.apache.org/docs/latest/sql-ref-syntax-ddl-usedb.html\n",
    "# spark.sql(f\"use {DATABASE_FULLNAME}\")\n",
    "\n",
    "print(f\"Using {DATABASE_FULLNAME=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebb52e94-288a-4013-b682-f5777febb204",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "  *\n",
    "FROM \n",
    "  {DATABASE_FULLNAME}.{TABLE_NAME}\n",
    "WHERE\n",
    "  created_at in (\n",
    "    SELECT \n",
    "      MAX(created_at) latest_created_at\n",
    "    FROM \n",
    "      {DATABASE_FULLNAME}.{TABLE_NAME}\n",
    "  )\n",
    "ORDER BY id\n",
    "LIMIT 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8c3314d-6c21-4690-a9a4-0f054d1e684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|          created_at|                  id|              id_str|                text|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|2024/11/02 13:48:...|-9204228685057172416|-9204228685057172416|so excited about ...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bfd2d2-f01e-4ee0-afe7-805f335005b7",
   "metadata": {},
   "source": [
    "## Connect to Vast S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f205e126-6049-4e66-8041-5b9da5f70874",
   "metadata": {},
   "source": [
    "### Specify Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73efd079-896e-47e0-8c1e-36706c5bf0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_PATH = 's3a://csnow-bucket/csnow_spark'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a036ec-70d8-4282-b736-9f1c2481fa43",
   "metadata": {},
   "source": [
    "**WARNING**: the next cell does not return - it just hangs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5976780-9f28-47ad-bbae-be842b56ec2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name| Id|\n",
      "+-----+---+\n",
      "|Alice|  1|\n",
      "|  Bob|  2|\n",
      "|Cathy|  3|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Cathy\", 3)]\n",
    "cols = [\"Name\", \"Id\"]\n",
    "\n",
    "s3_df = spark.createDataFrame(data, cols)\n",
    "s3_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba687e50-9301-4739-8e14-d3ade7b0514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_df.write.mode('append').parquet(S3_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ed2a6d4-6df9-4952-92fe-9a534a136cec",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Hive support is required to CREATE Hive TABLE (AS SELECT).;\n'CreateTable `spark_catalog`.`default`.`spark_demo`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, Ignore\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;43mCREATE EXTERNAL TABLE IF NOT EXISTS spark_demo (\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m    Name STRING,\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43m    Id INT\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43m)\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43mSTORED AS PARQUET\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43mLOCATION \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mS3_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1439\u001b[0m     litArgs \u001b[38;5;241m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Hive support is required to CREATE Hive TABLE (AS SELECT).;\n'CreateTable `spark_catalog`.`default`.`spark_demo`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, Ignore\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS spark_demo (\n",
    "    Name STRING,\n",
    "    Id INT\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION '{S3_PATH}';\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d452ee-ad72-4337-9c39-93b53d5792c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "INSERT INTO TABLE spark_demo VALUES ('abc', 123);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0607f8-7581-42cc-883b-09151e098d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the table has been created and query it\n",
    "result_df = spark.sql(\"SELECT * FROM spark_demo\")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecc0cfb-aedc-49e0-b533-9bae31304f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
